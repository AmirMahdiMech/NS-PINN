{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUnTwcZXU_E"
      },
      "source": [
        "Solve NS for flow around a cylinder (x,y,t) = [-1 1] * [-1 1] * (0,5)\n",
        "\n",
        "$IC : u,v = 0$\n",
        "\n",
        "$BC : u(-1,y) = 1$; $v(-1,y) = 0$;  $p(1,y) = 0$; $u_{cylinder} = v_{cylinder} = 0$\n",
        "\n",
        "$u_t + (u u_x + vu_y) = −p_x + (1/Re)*(u_{xx} + u_{yy}),$\n",
        "\n",
        "$v_t + (u v_x + v v_y) = −p_y + (1/Re)*(v_{xx} + v_{yy}),$\n",
        "\n",
        "$u_x + v_y = 0$\n",
        "\n",
        "Recommended algorithm: https://github.com/jdtoscano94/Learning-Python-Physics-Informed-Machine-Learning-PINNs-DeepONets/blob/main/PINNs/5_BurgersEquation.ipynb\n",
        "\n",
        "Similar code with deepxde: https://github.com/lululxvi/deepxde/blob/master/examples/pinn_forward/Kovasznay_flow.py\n",
        "\n",
        "Similar code for flow around cylinder : https://github.com/Shengfeng233/PINN-for-NS-equation/blob/sparse_data/pinn_model.py#L391\n",
        "\n",
        "Equations:https://en.wikipedia.org/wiki/Non-dimensionalization_and_scaling_of_the_Navier%E2%80%93Stokes_equations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEHpS5fVXSE3"
      },
      "outputs": [],
      "source": [
        "# import scipy.io\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from functools import partial\n",
        "# from pyDOE import lhs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYOPlza--p6z",
        "outputId": "9ff7b307-4d97-4ff6-a2e4-19112cbe0d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is:  cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device is: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 100"
      ],
      "metadata": {
        "id": "q_eTDz2ibBqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Z4bxLR6GnU"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(seed)\n",
        "\n",
        "class PINN_Net(nn.Module):\n",
        "    def __init__(self, layer_mat):\n",
        "        torch.manual_seed(seed)\n",
        "        super(PINN_Net, self).__init__()\n",
        "        self.layer_num = len(layer_mat) - 1\n",
        "        self.base = nn.Sequential()\n",
        "        for i in range(0, self.layer_num - 1):\n",
        "            self.base.add_module(str(i) + \"linear\", nn.Linear(layer_mat[i], layer_mat[i + 1]))\n",
        "            # nn.init.kaiming_normal()\n",
        "            self.base.add_module(str(i) + \"Activation\", nn.Tanh())\n",
        "        self.base.add_module(str(self.layer_num - 1) + \"linear\",\n",
        "                             nn.Linear(layer_mat[self.layer_num - 1], layer_mat[self.layer_num]))\n",
        "        # self.lam1 = nn.Parameter(torch.randn(1, requires_grad=True))\n",
        "        # self.lam2 = nn.Parameter(torch.randn(1, requires_grad=True))\n",
        "        self.Initial_param()\n",
        "\n",
        "    def forward(self, x, y, t):\n",
        "        X = torch.cat([x, y, t], 1).requires_grad_(True)\n",
        "        predict = self.base(X)\n",
        "        return predict\n",
        "\n",
        "    def Initial_param(self):\n",
        "        for name, param in self.base.named_parameters():\n",
        "            if name.endswith('weight'):\n",
        "                nn.init.xavier_normal_(param)\n",
        "            elif name.endswith('bias'):\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "    def BC_loss_velocity(self, x, y, t, u, v):\n",
        "        predict_out = self.forward(x, y, t)\n",
        "        psi = predict_out[:, 0].reshape(-1, 1)\n",
        "        u_predict = torch.autograd.grad(psi.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v_predict = -torch.autograd.grad(psi.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        mse = torch.nn.MSELoss()\n",
        "        mse_predict = mse(u_predict, u) + mse(v_predict, v)\n",
        "        return mse_predict\n",
        "\n",
        "\n",
        "    def BC_loss_pressure(self, x, y, t, p):\n",
        "        # print('begin of BC_loss_without_p')\n",
        "        predict_out = self.forward(x, y, t)\n",
        "        p_predict = predict_out[:, 1].reshape(-1, 1)\n",
        "        mse = torch.nn.MSELoss()\n",
        "        mse_predict = mse(p_predict, p)\n",
        "        return mse_predict\n",
        "\n",
        "\n",
        "\n",
        "    def BC_loss_without_p(self, x, y, t, u, v):\n",
        "        # print('begin of BC_loss_without_p')\n",
        "        predict_out = self.forward(x, y, t)\n",
        "        psi = predict_out[:, 0].reshape(-1, 1)\n",
        "        # print('psi :', psi, psi.requires_grad)\n",
        "        u_predict = torch.autograd.grad(psi.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v_predict = -torch.autograd.grad(psi.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        mse = torch.nn.MSELoss()\n",
        "        mse_predict = mse(u_predict, u) + mse(v_predict, v)\n",
        "        return mse_predict\n",
        "\n",
        "\n",
        "    def PDE_loss(self, x, y, t, Re=1000):\n",
        "        # print('begin of PDE_loss')\n",
        "        predict_out = self.forward(x, y, t)\n",
        "        psi = predict_out[:, 0].reshape(-1, 1)\n",
        "        p = predict_out[:, 1].reshape(-1, 1)\n",
        "        u = torch.autograd.grad(psi.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v = -torch.autograd.grad(psi.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        u_t = torch.autograd.grad(u.sum(), t, create_graph=True, retain_graph=True)[0]\n",
        "        u_x = torch.autograd.grad(u.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        u_y = torch.autograd.grad(u.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v_t = torch.autograd.grad(v.sum(), t, create_graph=True, retain_graph=True)[0]\n",
        "        v_x = torch.autograd.grad(v.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        v_y = torch.autograd.grad(v.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        p_x = torch.autograd.grad(p.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        p_y = torch.autograd.grad(p.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v_xx = torch.autograd.grad(v_x.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "        v_yy = torch.autograd.grad(v_y.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        f_equation_x = u_t + (u * u_x + v * u_y) + p_x - 1.0/Re * (u_xx + u_yy)\n",
        "        f_equation_y = v_t + (u * v_x + v * v_y) + p_y - 1.0/Re * (v_xx + v_yy)\n",
        "        f_equation_continuity = u_x + v_y\n",
        "        mse = torch.nn.MSELoss()\n",
        "        batch_t_zeros = torch.from_numpy(np.zeros((x.shape[0], 1))).float().requires_grad_(True).to(device)\n",
        "        mse_equation = mse(f_equation_x, batch_t_zeros) + mse(f_equation_y, batch_t_zeros) + mse(f_equation_continuity, batch_t_zeros)\n",
        "\n",
        "        return mse_equation\n",
        "\n",
        "    def total_loss(self, x_bc, y_bc, t_bc, u_bc, v_bc, x_collo, y_collo, t_collo, x_rightbc, y_rightbc, t_rightbc, p_rightbc):\n",
        "         return self.BC_loss_velocity(x_bc, y_bc, t_bc, u_bc, v_bc) + self.PDE_loss(x_bc, y_bc, t_bc, Re) + self.PDE_loss(x_collo, y_collo, t_collo, Re)  +  self.BC_loss_pressure(x_rightbc, y_rightbc, t_rightbc, p_rightbc) + self.PDE_loss(x_rightbc, y_rightbc, t_rightbc, Re)\n",
        "        #return self.PDE_loss(x_collo, y_collo, t_collo, Re)  +  20*self.BC_loss_pressure(x_rightbc, y_rightbc, t_rightbc, p_rightbc) + self.PDE_loss(x_rightbc, y_rightbc, t_rightbc, Re)\n",
        "\n",
        "\n",
        "\n",
        "    def inference(self, x, y, t):\n",
        "        # predict_out = self.forward(x, y, t)\n",
        "        # psi = predict_out[:, 0].reshape(-1, 1)\n",
        "        # p = predict_out[:, 1].reshape(-1, 1)\n",
        "        # u = torch.autograd.grad(psi.sum(), y)[0]\n",
        "        # v = -torch.autograd.grad(psi.sum(), x)[0]\n",
        "\n",
        "        predict_out = self.forward(x, y, t)\n",
        "        psi = predict_out[:, 0].reshape(-1, 1)\n",
        "        u = torch.autograd.grad(psi.sum(), y, create_graph=True, retain_graph=True)[0]\n",
        "        v = -torch.autograd.grad(psi.sum(), x, create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        return u,v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hJhd5UC_vsV"
      },
      "outputs": [],
      "source": [
        "L = 2\n",
        "r = 0.3 / 2\n",
        "x_min, y_min, x_max, y_max = -1, -1, 1, 1\n",
        "t_min, t_max = 0, 5\n",
        "total_points = 100\n",
        "time_steps = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hmmSdjRC8HD"
      },
      "outputs": [],
      "source": [
        "# Defining boundaries and BCs\n",
        "x_rightbc = 1*torch.ones(total_points).requires_grad_(True)\n",
        "y_rightbc = torch.linspace(y_min,y_max,total_points).requires_grad_(True)\n",
        "t_rightbc = torch.linspace(t_min,t_max,time_steps).requires_grad_(True)\n",
        "p_rightbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "\n",
        "x_leftbc = -1*torch.ones(total_points).requires_grad_(True)\n",
        "y_leftbc = torch.linspace(y_min,y_max,total_points).requires_grad_(True)\n",
        "\n",
        "x_upperbc = torch.linspace(x_min,x_max,total_points).requires_grad_(True)\n",
        "y_upperbc = +1*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "x_lowerbc = torch.linspace(x_min,x_max,total_points).requires_grad_(True)\n",
        "y_lowerbc = -1*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "u_leftbc = torch.ones(total_points).requires_grad_(True)\n",
        "v_leftbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "u_upperbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "v_upperbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "u_lowerbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "v_lowerbc = 0*torch.ones(total_points).requires_grad_(True)\n",
        "\n",
        "t_bc = torch.linspace(t_min,t_max,time_steps).requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31RKDavTxL_",
        "outputId": "ae472da2-72d6-4fe5-9916-5f40c695b70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_rightbc, t_rightbc = torch.meshgrid(x_rightbc, t_bc)\n",
        "y_rightbc, t_rightbc = torch.meshgrid(y_rightbc, t_bc)\n",
        "p_rightbc, t_rightbc = torch.meshgrid(p_rightbc, t_bc)\n",
        "\n",
        "x_rightbc = torch.reshape(x_rightbc, (-1, 1)).to(device)\n",
        "y_rightbc = torch.reshape(y_rightbc, (-1, 1)).to(device)\n",
        "t_rightbc = torch.reshape(t_rightbc, (-1, 1)).to(device)\n",
        "p_rightbc = torch.reshape(p_rightbc, (-1, 1)).to(device)\n",
        "\n",
        "x_rightbc.shape, y_rightbc.shape, t_rightbc.shape, p_rightbc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnwTBr6RPMjw",
        "outputId": "37da2a47-6ddf-4552-a4ab-102982efe3ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x_leftbc, t_leftbc = torch.meshgrid(x_leftbc, t_bc)\n",
        "y_leftbc, t_leftbc = torch.meshgrid(y_leftbc, t_bc)\n",
        "u_leftbc, t_leftbc = torch.meshgrid(u_leftbc, t_bc)\n",
        "v_leftbc, t_leftbc = torch.meshgrid(v_leftbc, t_bc)\n",
        "\n",
        "x_leftbc = torch.reshape(x_leftbc, (-1, 1))\n",
        "y_leftbc = torch.reshape(y_leftbc, (-1, 1))\n",
        "t_leftbc = torch.reshape(t_leftbc, (-1, 1))\n",
        "u_leftbc = torch.reshape(u_leftbc, (-1, 1))\n",
        "v_leftbc = torch.reshape(v_leftbc, (-1, 1))\n",
        "\n",
        "x_leftbc.shape , y_leftbc.shape, t_leftbc.shape, u_leftbc.shape, v_leftbc.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgTCjan2QAUX",
        "outputId": "c1213620-2eb4-4d5c-b65b-9cd0b2e118d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x_upperbc, t_upperbc = torch.meshgrid(x_upperbc, t_bc)\n",
        "y_upperbc, t_upperbc = torch.meshgrid(y_upperbc, t_bc)\n",
        "u_upperbc, t_upperbc = torch.meshgrid(u_upperbc, t_bc)\n",
        "v_upperbc, t_upperbc = torch.meshgrid(v_upperbc, t_bc)\n",
        "\n",
        "x_upperbc = torch.reshape(x_upperbc, (-1, 1))\n",
        "y_upperbc = torch.reshape(y_upperbc, (-1, 1))\n",
        "t_upperbc = torch.reshape(t_upperbc, (-1, 1))\n",
        "u_upperbc = torch.reshape(u_upperbc, (-1, 1))\n",
        "v_upperbc = torch.reshape(v_upperbc, (-1, 1))\n",
        "\n",
        "x_upperbc.shape , y_upperbc.shape, t_upperbc.shape, u_upperbc.shape, v_upperbc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J6G7F0sQAZh",
        "outputId": "5be5ab50-4de8-4c3a-9071-421c16c523b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]),\n",
              " torch.Size([1600, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x_lowerbc, t_lowerbc = torch.meshgrid(x_lowerbc, t_bc)\n",
        "y_lowerbc, t_lowerbc = torch.meshgrid(y_lowerbc, t_bc)\n",
        "u_lowerbc, t_lowerbc = torch.meshgrid(u_lowerbc, t_bc)\n",
        "v_lowerbc, t_lowerbc = torch.meshgrid(v_lowerbc, t_bc)\n",
        "\n",
        "x_lowerbc = torch.reshape(x_lowerbc, (-1, 1))\n",
        "y_lowerbc = torch.reshape(y_lowerbc, (-1, 1))\n",
        "t_lowerbc = torch.reshape(t_lowerbc, (-1, 1))\n",
        "u_lowerbc = torch.reshape(u_lowerbc, (-1, 1))\n",
        "v_lowerbc = torch.reshape(v_lowerbc, (-1, 1))\n",
        "\n",
        "x_lowerbc.shape , y_lowerbc.shape, t_lowerbc.shape, u_lowerbc.shape, v_lowerbc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wm2aoU4W2af"
      },
      "outputs": [],
      "source": [
        "x_bc = torch.cat((x_leftbc, x_upperbc, x_lowerbc), dim=0).to(device)\n",
        "y_bc = torch.cat((y_leftbc, y_upperbc, y_lowerbc), dim=0).to(device)\n",
        "u_bc = torch.cat((u_leftbc, u_upperbc, u_lowerbc), dim=0).to(device)\n",
        "v_bc = torch.cat((v_leftbc, v_upperbc, v_lowerbc), dim=0).to(device)\n",
        "t_bc = torch.cat((t_leftbc, t_upperbc, t_lowerbc), dim=0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy7EoI9kX13N",
        "outputId": "e325ca14-462b-44db-a2bf-98efc36bd373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4800, 1]),\n",
              " torch.Size([4800, 1]),\n",
              " torch.Size([4800, 1]),\n",
              " torch.Size([4800, 1]),\n",
              " torch.Size([4800, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x_bc.shape , y_bc.shape , t_bc.shape, u_bc.shape , v_bc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSapBPngZI4e",
        "outputId": "194b255b-7631-4bba-b1f0-ce4ecc91b4e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([160000, 1]), torch.Size([160000, 1]), torch.Size([160000, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Defining interior domain (Total - boundaries)\n",
        "#.view(-1,1).\n",
        "t_collo = torch.linspace(t_min,t_max,time_steps).requires_grad_(True)\n",
        "x_collo = torch.linspace(x_min+1e-3,x_max-1e-3,total_points).requires_grad_(True)\n",
        "y_collo = torch.linspace(y_min+1e-3,y_max-1e-3,total_points).requires_grad_(True)\n",
        "\n",
        "x_collo, y_collo, t_collo = torch.meshgrid(x_collo, y_collo, t_collo)\n",
        "x_collo = torch.reshape(x_collo, (-1, 1)).to(device)\n",
        "y_collo = torch.reshape(y_collo, (-1, 1)).to(device)\n",
        "t_collo = torch.reshape(t_collo, (-1, 1)).to(device)\n",
        "\n",
        "x_collo.shape , y_collo.shape , t_collo.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWXigCNY_vu7",
        "outputId": "afe59266-1a84-44ba-ee4c-43a49fda7c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINN_Net(\n",
            "  (base): Sequential(\n",
            "    (0linear): Linear(in_features=3, out_features=20, bias=True)\n",
            "    (0Activation): Tanh()\n",
            "    (1linear): Linear(in_features=20, out_features=20, bias=True)\n",
            "    (1Activation): Tanh()\n",
            "    (2linear): Linear(in_features=20, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Network's parameters\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "layers = [3, 20, 20, 2]\n",
        "lr = 1e-4\n",
        "n_epochs = 5000\n",
        "Re=100\n",
        "PINN = PINN_Net(layers)\n",
        "PINN.to(device)\n",
        "print(PINN)\n",
        "params = list(PINN.parameters())\n",
        "optimizer = torch.optim.Adam(PINN.parameters(),lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81NafgWAQ7PF"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(seed)\n",
        "\n",
        "# loss_list = []\n",
        "# print(\"Training Loss-----Test Loss\")\n",
        "# for i in range(n_epochs):\n",
        "#     print('epoch: ', i+1)\n",
        "#     x_bc, y_bc, t_bc, u_bc, v_bc = x_bc.to(device), y_bc.to(device), t_bc.to(device), u_bc.to(device), v_bc.to(device)\n",
        "#     x_collo, y_collo, t_collo = x_collo.to(device), y_collo.to(device), t_collo.to(device)\n",
        "#     x_rightbc, y_rightbc, t_rightbc, p_rightbc = x_rightbc.to(device), y_rightbc.to(device), t_rightbc.to(device), p_rightbc.to(device)\n",
        "\n",
        "#     loss = PINN.total_loss(x_bc, y_bc, t_bc, u_bc, v_bc, x_collo, y_collo, t_collo, x_rightbc, y_rightbc, t_rightbc, p_rightbc)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward(retain_graph=True)\n",
        "#     optimizer.step()\n",
        "\n",
        "#     # if i%(steps/10)==0:\n",
        "#     #   with torch.no_grad():\n",
        "#     #     test_loss=PINN.lossBC(x, y, t, u, v)\n",
        "\n",
        "#     print('train loss :', loss.detach().cpu().numpy())\n",
        "#     loss_list.append(loss.detach().cpu().numpy())\n",
        "\n",
        "# plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "iter = 0\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()\n",
        "    # x_bc, y_bc, t_bc, u_bc, v_bc = x_bc.to(device), y_bc.to(device), t_bc.to(device), u_bc.to(device), v_bc.to(device)\n",
        "    # x_collo, y_collo, t_collo = x_collo.to(device), y_collo.to(device), t_collo.to(device)\n",
        "    # x_rightbc, y_rightbc, t_rightbc, p_rightbc = x_rightbc.to(device), y_rightbc.to(device), t_rightbc.to(device), p_rightbc.to(device)\n",
        "    loss = PINN.total_loss(x_bc, y_bc, t_bc, u_bc, v_bc, x_collo, y_collo, t_collo, x_rightbc, y_rightbc, t_rightbc, p_rightbc)\n",
        "    loss.backward(retain_graph=True)\n",
        "    global iter\n",
        "    iter += 1\n",
        "    print(f\" iteration: {iter}  loss: {loss.item()}\")\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = torch.optim.LBFGS(PINN.parameters(),\n",
        "                                lr=1,\n",
        "                                max_iter=10000,\n",
        "                                max_eval=50000,\n",
        "                                history_size=50,\n",
        "                                tolerance_grad=1e-05,\n",
        "                                tolerance_change=0.5 * np.finfo(float).eps,\n",
        "                                line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "# the optimizer.step requires the closure function to be a callable function without inputs\n",
        "# therefore we need to define a partial function and pass it to the optimizer\n",
        "# closure_fn = partial(closure, PINN, optimizer,  x_bc, y_bc, t_bc, u_bc, v_bc, x_collo, y_collo, t_collo, x_rightbc, y_rightbc, t_rightbc, p_rightbc)\n",
        "optimizer.step(closure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSWVF4ir1Ltu",
        "outputId": "df394c22-7d5c-43f9-b5d0-a2f609982f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " iteration: 5883  loss: 0.008414912037551403\n",
            " iteration: 5884  loss: 0.00841427966952324\n",
            " iteration: 5885  loss: 0.008413259871304035\n",
            " iteration: 5886  loss: 0.008412518538534641\n",
            " iteration: 5887  loss: 0.00841182004660368\n",
            " iteration: 5888  loss: 0.008411228656768799\n",
            " iteration: 5889  loss: 0.008410816080868244\n",
            " iteration: 5890  loss: 0.00841046404093504\n",
            " iteration: 5891  loss: 0.00841020792722702\n",
            " iteration: 5892  loss: 0.008409482426941395\n",
            " iteration: 5893  loss: 0.008408724330365658\n",
            " iteration: 5894  loss: 0.008408939465880394\n",
            " iteration: 5895  loss: 0.008408313617110252\n",
            " iteration: 5896  loss: 0.008407815359532833\n",
            " iteration: 5897  loss: 0.008407462388277054\n",
            " iteration: 5898  loss: 0.008407206274569035\n",
            " iteration: 5899  loss: 0.008406659588217735\n",
            " iteration: 5900  loss: 0.008405893109738827\n",
            " iteration: 5901  loss: 0.008405327796936035\n",
            " iteration: 5902  loss: 0.008404943160712719\n",
            " iteration: 5903  loss: 0.008404484950006008\n",
            " iteration: 5904  loss: 0.008404054678976536\n",
            " iteration: 5905  loss: 0.008403216488659382\n",
            " iteration: 5906  loss: 0.008402282372117043\n",
            " iteration: 5907  loss: 0.008401256985962391\n",
            " iteration: 5908  loss: 0.008400842547416687\n",
            " iteration: 5909  loss: 0.008399897255003452\n",
            " iteration: 5910  loss: 0.008399640209972858\n",
            " iteration: 5911  loss: 0.008398828096687794\n",
            " iteration: 5912  loss: 0.00839823018759489\n",
            " iteration: 5913  loss: 0.008397359400987625\n",
            " iteration: 5914  loss: 0.008396469987928867\n",
            " iteration: 5915  loss: 0.008397392928600311\n",
            " iteration: 5916  loss: 0.008396013639867306\n",
            " iteration: 5917  loss: 0.00839508231729269\n",
            " iteration: 5918  loss: 0.008394467644393444\n",
            " iteration: 5919  loss: 0.008393482305109501\n",
            " iteration: 5920  loss: 0.008392835967242718\n",
            " iteration: 5921  loss: 0.008392293937504292\n",
            " iteration: 5922  loss: 0.00839187577366829\n",
            " iteration: 5923  loss: 0.008391572162508965\n",
            " iteration: 5924  loss: 0.008391162380576134\n",
            " iteration: 5925  loss: 0.008395807817578316\n",
            " iteration: 5926  loss: 0.008390926755964756\n",
            " iteration: 5927  loss: 0.008390624076128006\n",
            " iteration: 5928  loss: 0.008389925584197044\n",
            " iteration: 5929  loss: 0.008389578200876713\n",
            " iteration: 5930  loss: 0.008388707414269447\n",
            " iteration: 5931  loss: 0.008390502072870731\n",
            " iteration: 5932  loss: 0.008388510905206203\n",
            " iteration: 5933  loss: 0.008388139307498932\n",
            " iteration: 5934  loss: 0.008387534879148006\n",
            " iteration: 5935  loss: 0.008386999368667603\n",
            " iteration: 5936  loss: 0.008386455476284027\n",
            " iteration: 5937  loss: 0.008385644294321537\n",
            " iteration: 5938  loss: 0.008385121822357178\n",
            " iteration: 5939  loss: 0.00838427059352398\n",
            " iteration: 5940  loss: 0.008383408188819885\n",
            " iteration: 5941  loss: 0.008382182568311691\n",
            " iteration: 5942  loss: 0.008394559845328331\n",
            " iteration: 5943  loss: 0.008381980471313\n",
            " iteration: 5944  loss: 0.0083810705691576\n",
            " iteration: 5945  loss: 0.008380232378840446\n",
            " iteration: 5946  loss: 0.0083792544901371\n",
            " iteration: 5947  loss: 0.008378250524401665\n",
            " iteration: 5948  loss: 0.008376667276024818\n",
            " iteration: 5949  loss: 0.008375832810997963\n",
            " iteration: 5950  loss: 0.008374636992812157\n",
            " iteration: 5951  loss: 0.008374040015041828\n",
            " iteration: 5952  loss: 0.008373662829399109\n",
            " iteration: 5953  loss: 0.008373131044209003\n",
            " iteration: 5954  loss: 0.008372589014470577\n",
            " iteration: 5955  loss: 0.008372128941118717\n",
            " iteration: 5956  loss: 0.008371603675186634\n",
            " iteration: 5957  loss: 0.008371331728994846\n",
            " iteration: 5958  loss: 0.008370460011065006\n",
            " iteration: 5959  loss: 0.008369763381779194\n",
            " iteration: 5960  loss: 0.00836945977061987\n",
            " iteration: 5961  loss: 0.008368855342268944\n",
            " iteration: 5962  loss: 0.008368312381207943\n",
            " iteration: 5963  loss: 0.008367867209017277\n",
            " iteration: 5964  loss: 0.008367008529603481\n",
            " iteration: 5965  loss: 0.008375724777579308\n",
            " iteration: 5966  loss: 0.008366602472960949\n",
            " iteration: 5967  loss: 0.008365346118807793\n",
            " iteration: 5968  loss: 0.008364482782781124\n",
            " iteration: 5969  loss: 0.008363410830497742\n",
            " iteration: 5970  loss: 0.008362135849893093\n",
            " iteration: 5971  loss: 0.008360886946320534\n",
            " iteration: 5972  loss: 0.008361436426639557\n",
            " iteration: 5973  loss: 0.008360018953680992\n",
            " iteration: 5974  loss: 0.008359373547136784\n",
            " iteration: 5975  loss: 0.008358701132237911\n",
            " iteration: 5976  loss: 0.008358347229659557\n",
            " iteration: 5977  loss: 0.008357486687600613\n",
            " iteration: 5978  loss: 0.00835733488202095\n",
            " iteration: 5979  loss: 0.008356218226253986\n",
            " iteration: 5980  loss: 0.008355708792805672\n",
            " iteration: 5981  loss: 0.008355275727808475\n",
            " iteration: 5982  loss: 0.008354861289262772\n",
            " iteration: 5983  loss: 0.00835400726646185\n",
            " iteration: 5984  loss: 0.008352742530405521\n",
            " iteration: 5985  loss: 0.008411391638219357\n",
            " iteration: 5986  loss: 0.008352301083505154\n",
            " iteration: 5987  loss: 0.008351432159543037\n",
            " iteration: 5988  loss: 0.008350812830030918\n",
            " iteration: 5989  loss: 0.008350375108420849\n",
            " iteration: 5990  loss: 0.008349966257810593\n",
            " iteration: 5991  loss: 0.00834912434220314\n",
            " iteration: 5992  loss: 0.008348243311047554\n",
            " iteration: 5993  loss: 0.008348183706402779\n",
            " iteration: 5994  loss: 0.008347757160663605\n",
            " iteration: 5995  loss: 0.008347325026988983\n",
            " iteration: 5996  loss: 0.008346546441316605\n",
            " iteration: 5997  loss: 0.008345589973032475\n",
            " iteration: 5998  loss: 0.008344290778040886\n",
            " iteration: 5999  loss: 0.008345512673258781\n",
            " iteration: 6000  loss: 0.008343849331140518\n",
            " iteration: 6001  loss: 0.00834287516772747\n",
            " iteration: 6002  loss: 0.008342347107827663\n",
            " iteration: 6003  loss: 0.008341493085026741\n",
            " iteration: 6004  loss: 0.008340155705809593\n",
            " iteration: 6005  loss: 0.00833892822265625\n",
            " iteration: 6006  loss: 0.008337797597050667\n",
            " iteration: 6007  loss: 0.008337532170116901\n",
            " iteration: 6008  loss: 0.00833678338676691\n",
            " iteration: 6009  loss: 0.008336278609931469\n",
            " iteration: 6010  loss: 0.008334442973136902\n",
            " iteration: 6011  loss: 0.008333245292305946\n",
            " iteration: 6012  loss: 0.00833271723240614\n",
            " iteration: 6013  loss: 0.008332299999892712\n",
            " iteration: 6014  loss: 0.008331813849508762\n",
            " iteration: 6015  loss: 0.008331323973834515\n",
            " iteration: 6016  loss: 0.008330588229000568\n",
            " iteration: 6017  loss: 0.008329981006681919\n",
            " iteration: 6018  loss: 0.0083351144567132\n",
            " iteration: 6019  loss: 0.008329798467457294\n",
            " iteration: 6020  loss: 0.008329412899911404\n",
            " iteration: 6021  loss: 0.008328991010785103\n",
            " iteration: 6022  loss: 0.008328331634402275\n",
            " iteration: 6023  loss: 0.00832848809659481\n",
            " iteration: 6024  loss: 0.008328045718371868\n",
            " iteration: 6025  loss: 0.008327321149408817\n",
            " iteration: 6026  loss: 0.00832696259021759\n",
            " iteration: 6027  loss: 0.008326461538672447\n",
            " iteration: 6028  loss: 0.008325924165546894\n",
            " iteration: 6029  loss: 0.00832496676594019\n",
            " iteration: 6030  loss: 0.00832411926239729\n",
            " iteration: 6031  loss: 0.00832335278391838\n",
            " iteration: 6032  loss: 0.008334862999618053\n",
            " iteration: 6033  loss: 0.008322845213115215\n",
            " iteration: 6034  loss: 0.008321878500282764\n",
            " iteration: 6035  loss: 0.008326024748384953\n",
            " iteration: 6036  loss: 0.00832115113735199\n",
            " iteration: 6037  loss: 0.008320553228259087\n",
            " iteration: 6038  loss: 0.008320155553519726\n",
            " iteration: 6039  loss: 0.008319543674588203\n",
            " iteration: 6040  loss: 0.008318954147398472\n",
            " iteration: 6041  loss: 0.00831773690879345\n",
            " iteration: 6042  loss: 0.008316351100802422\n",
            " iteration: 6043  loss: 0.008315161801874638\n",
            " iteration: 6044  loss: 0.008314549922943115\n",
            " iteration: 6045  loss: 0.008318821899592876\n",
            " iteration: 6046  loss: 0.008314299397170544\n",
            " iteration: 6047  loss: 0.008313787169754505\n",
            " iteration: 6048  loss: 0.008313019759953022\n",
            " iteration: 6049  loss: 0.00831236969679594\n",
            " iteration: 6050  loss: 0.008311837911605835\n",
            " iteration: 6051  loss: 0.0083115603774786\n",
            " iteration: 6052  loss: 0.008311287499964237\n",
            " iteration: 6053  loss: 0.008310861885547638\n",
            " iteration: 6054  loss: 0.008310162462294102\n",
            " iteration: 6055  loss: 0.008309045806527138\n",
            " iteration: 6056  loss: 0.008308002725243568\n",
            " iteration: 6057  loss: 0.008306663483381271\n",
            " iteration: 6058  loss: 0.008305561728775501\n",
            " iteration: 6059  loss: 0.008304719813168049\n",
            " iteration: 6060  loss: 0.008304276503622532\n",
            " iteration: 6061  loss: 0.008302793838083744\n",
            " iteration: 6062  loss: 0.008301918394863605\n",
            " iteration: 6063  loss: 0.008300265297293663\n",
            " iteration: 6064  loss: 0.008299029432237148\n",
            " iteration: 6065  loss: 0.00829787366092205\n",
            " iteration: 6066  loss: 0.008296466432511806\n",
            " iteration: 6067  loss: 0.008299817331135273\n",
            " iteration: 6068  loss: 0.008295858278870583\n",
            " iteration: 6069  loss: 0.008294597268104553\n",
            " iteration: 6070  loss: 0.008293441496789455\n",
            " iteration: 6071  loss: 0.0082924235612154\n",
            " iteration: 6072  loss: 0.008291833102703094\n",
            " iteration: 6073  loss: 0.008291051723062992\n",
            " iteration: 6074  loss: 0.008290191181004047\n",
            " iteration: 6075  loss: 0.008289129473268986\n",
            " iteration: 6076  loss: 0.008287759497761726\n",
            " iteration: 6077  loss: 0.00828607752919197\n",
            " iteration: 6078  loss: 0.008284492418169975\n",
            " iteration: 6079  loss: 0.008283110335469246\n",
            " iteration: 6080  loss: 0.008282551541924477\n",
            " iteration: 6081  loss: 0.00828174315392971\n",
            " iteration: 6082  loss: 0.008281271904706955\n",
            " iteration: 6083  loss: 0.008280826732516289\n",
            " iteration: 6084  loss: 0.008279859088361263\n",
            " iteration: 6085  loss: 0.008278458379209042\n",
            " iteration: 6086  loss: 0.008277375251054764\n",
            " iteration: 6087  loss: 0.008274580352008343\n",
            " iteration: 6088  loss: 0.008272875100374222\n",
            " iteration: 6089  loss: 0.008272071368992329\n",
            " iteration: 6090  loss: 0.008272117003798485\n",
            " iteration: 6091  loss: 0.00827178917825222\n",
            " iteration: 6092  loss: 0.008271651342511177\n",
            " iteration: 6093  loss: 0.008271404542028904\n",
            " iteration: 6094  loss: 0.008270925842225552\n",
            " iteration: 6095  loss: 0.008269570767879486\n",
            " iteration: 6096  loss: 0.008271927013993263\n",
            " iteration: 6097  loss: 0.008269126527011395\n",
            " iteration: 6098  loss: 0.008268104866147041\n",
            " iteration: 6099  loss: 0.008267120458185673\n",
            " iteration: 6100  loss: 0.008266090415418148\n",
            " iteration: 6101  loss: 0.008266529999673367\n",
            " iteration: 6102  loss: 0.008265534415841103\n",
            " iteration: 6103  loss: 0.008264548145234585\n",
            " iteration: 6104  loss: 0.008263305760920048\n",
            " iteration: 6105  loss: 0.008261813782155514\n",
            " iteration: 6106  loss: 0.008260881528258324\n",
            " iteration: 6107  loss: 0.008260034956037998\n",
            " iteration: 6108  loss: 0.00825909711420536\n",
            " iteration: 6109  loss: 0.008257893845438957\n",
            " iteration: 6110  loss: 0.008257362060248852\n",
            " iteration: 6111  loss: 0.008256974630057812\n",
            " iteration: 6112  loss: 0.00825648382306099\n",
            " iteration: 6113  loss: 0.00825610850006342\n",
            " iteration: 6114  loss: 0.00825513992458582\n",
            " iteration: 6115  loss: 0.008254190906882286\n",
            " iteration: 6116  loss: 0.008252876810729504\n",
            " iteration: 6117  loss: 0.008251526392996311\n",
            " iteration: 6118  loss: 0.00825093500316143\n",
            " iteration: 6119  loss: 0.00824962928891182\n",
            " iteration: 6120  loss: 0.008249156177043915\n",
            " iteration: 6121  loss: 0.008248090744018555\n",
            " iteration: 6122  loss: 0.008247320540249348\n",
            " iteration: 6123  loss: 0.008248616941273212\n",
            " iteration: 6124  loss: 0.008246876299381256\n",
            " iteration: 6125  loss: 0.008245705626904964\n",
            " iteration: 6126  loss: 0.008244478143751621\n",
            " iteration: 6127  loss: 0.00824284553527832\n",
            " iteration: 6128  loss: 0.008242794312536716\n",
            " iteration: 6129  loss: 0.00824222806841135\n",
            " iteration: 6130  loss: 0.008241387084126472\n",
            " iteration: 6131  loss: 0.008240538649260998\n",
            " iteration: 6132  loss: 0.008239730261266232\n",
            " iteration: 6133  loss: 0.008238909766077995\n",
            " iteration: 6134  loss: 0.00823827926069498\n",
            " iteration: 6135  loss: 0.008237830363214016\n",
            " iteration: 6136  loss: 0.00823698565363884\n",
            " iteration: 6137  loss: 0.00823893491178751\n",
            " iteration: 6138  loss: 0.008236723020672798\n",
            " iteration: 6139  loss: 0.00823615025728941\n",
            " iteration: 6140  loss: 0.008235613815486431\n",
            " iteration: 6141  loss: 0.0082349618896842\n",
            " iteration: 6142  loss: 0.008234147913753986\n",
            " iteration: 6143  loss: 0.008233471773564816\n",
            " iteration: 6144  loss: 0.008232492953538895\n",
            " iteration: 6145  loss: 0.008231308311223984\n",
            " iteration: 6146  loss: 0.008230309002101421\n",
            " iteration: 6147  loss: 0.00822930596768856\n",
            " iteration: 6148  loss: 0.008229660801589489\n",
            " iteration: 6149  loss: 0.008228939957916737\n",
            " iteration: 6150  loss: 0.008228401653468609\n",
            " iteration: 6151  loss: 0.008228029124438763\n",
            " iteration: 6152  loss: 0.008227103389799595\n",
            " iteration: 6153  loss: 0.008226423524320126\n",
            " iteration: 6154  loss: 0.008226650767028332\n",
            " iteration: 6155  loss: 0.008225937373936176\n",
            " iteration: 6156  loss: 0.008225458674132824\n",
            " iteration: 6157  loss: 0.008224999532103539\n",
            " iteration: 6158  loss: 0.008224242366850376\n",
            " iteration: 6159  loss: 0.008223528042435646\n",
            " iteration: 6160  loss: 0.008223013021051884\n",
            " iteration: 6161  loss: 0.00822184793651104\n",
            " iteration: 6162  loss: 0.008220996707677841\n",
            " iteration: 6163  loss: 0.008219538256525993\n",
            " iteration: 6164  loss: 0.008226495236158371\n",
            " iteration: 6165  loss: 0.008219265379011631\n",
            " iteration: 6166  loss: 0.008218330331146717\n",
            " iteration: 6167  loss: 0.00821742881089449\n",
            " iteration: 6168  loss: 0.008216633461415768\n",
            " iteration: 6169  loss: 0.008216703310608864\n",
            " iteration: 6170  loss: 0.008215971291065216\n",
            " iteration: 6171  loss: 0.008215202018618584\n",
            " iteration: 6172  loss: 0.008214516565203667\n",
            " iteration: 6173  loss: 0.008213850669562817\n",
            " iteration: 6174  loss: 0.008212873712182045\n",
            " iteration: 6175  loss: 0.008210992440581322\n",
            " iteration: 6176  loss: 0.008214431814849377\n",
            " iteration: 6177  loss: 0.008210173808038235\n",
            " iteration: 6178  loss: 0.008208624087274075\n",
            " iteration: 6179  loss: 0.008207074366509914\n",
            " iteration: 6180  loss: 0.008205575868487358\n",
            " iteration: 6181  loss: 0.008204851299524307\n",
            " iteration: 6182  loss: 0.00820250529795885\n",
            " iteration: 6183  loss: 0.008205468766391277\n",
            " iteration: 6184  loss: 0.008201058954000473\n",
            " iteration: 6185  loss: 0.00819848570972681\n",
            " iteration: 6186  loss: 0.008196652866899967\n",
            " iteration: 6187  loss: 0.008195197209715843\n",
            " iteration: 6188  loss: 0.008194698952138424\n",
            " iteration: 6189  loss: 0.008193934336304665\n",
            " iteration: 6190  loss: 0.00819290429353714\n",
            " iteration: 6191  loss: 0.008191674947738647\n",
            " iteration: 6192  loss: 0.008190721273422241\n",
            " iteration: 6193  loss: 0.008190130814909935\n",
            " iteration: 6194  loss: 0.008189607411623001\n",
            " iteration: 6195  loss: 0.008188973180949688\n",
            " iteration: 6196  loss: 0.00818906631320715\n",
            " iteration: 6197  loss: 0.008188612759113312\n",
            " iteration: 6198  loss: 0.008187814615666866\n",
            " iteration: 6199  loss: 0.008186863735318184\n",
            " iteration: 6200  loss: 0.008185879327356815\n",
            " iteration: 6201  loss: 0.008186398074030876\n",
            " iteration: 6202  loss: 0.008185611106455326\n",
            " iteration: 6203  loss: 0.008185244165360928\n",
            " iteration: 6204  loss: 0.008184663020074368\n",
            " iteration: 6205  loss: 0.008183497935533524\n",
            " iteration: 6206  loss: 0.008182324469089508\n",
            " iteration: 6207  loss: 0.008192485198378563\n",
            " iteration: 6208  loss: 0.008182127960026264\n",
            " iteration: 6209  loss: 0.008180956356227398\n",
            " iteration: 6210  loss: 0.008179585449397564\n",
            " iteration: 6211  loss: 0.008177399635314941\n",
            " iteration: 6212  loss: 0.008175515569746494\n",
            " iteration: 6213  loss: 0.008173194713890553\n",
            " iteration: 6214  loss: 0.008171544410288334\n",
            " iteration: 6215  loss: 0.00817039329558611\n",
            " iteration: 6216  loss: 0.008169286884367466\n",
            " iteration: 6217  loss: 0.008168412372469902\n",
            " iteration: 6218  loss: 0.008167698048055172\n",
            " iteration: 6219  loss: 0.008167175576090813\n",
            " iteration: 6220  loss: 0.00816834345459938\n",
            " iteration: 6221  loss: 0.008166935294866562\n",
            " iteration: 6222  loss: 0.008166108280420303\n",
            " iteration: 6223  loss: 0.008165141567587852\n",
            " iteration: 6224  loss: 0.008164366707205772\n",
            " iteration: 6225  loss: 0.00816359743475914\n",
            " iteration: 6226  loss: 0.0081631476059556\n",
            " iteration: 6227  loss: 0.008162567391991615\n",
            " iteration: 6228  loss: 0.008162287063896656\n",
            " iteration: 6229  loss: 0.008162126876413822\n",
            " iteration: 6230  loss: 0.008161261677742004\n",
            " iteration: 6231  loss: 0.008160538971424103\n",
            " iteration: 6232  loss: 0.008159228600561619\n",
            " iteration: 6233  loss: 0.008158445358276367\n",
            " iteration: 6234  loss: 0.008157731965184212\n",
            " iteration: 6235  loss: 0.008156769908964634\n",
            " iteration: 6236  loss: 0.008155974559485912\n",
            " iteration: 6237  loss: 0.008155093528330326\n",
            " iteration: 6238  loss: 0.00815370213240385\n",
            " iteration: 6239  loss: 0.008152827620506287\n",
            " iteration: 6240  loss: 0.00815222691744566\n",
            " iteration: 6241  loss: 0.008151711896061897\n",
            " iteration: 6242  loss: 0.008151190355420113\n",
            " iteration: 6243  loss: 0.008150546811521053\n",
            " iteration: 6244  loss: 0.008150305598974228\n",
            " iteration: 6245  loss: 0.008149920962750912\n",
            " iteration: 6246  loss: 0.008149613626301289\n",
            " iteration: 6247  loss: 0.008149259723722935\n",
            " iteration: 6248  loss: 0.008148756809532642\n",
            " iteration: 6249  loss: 0.008147776126861572\n",
            " iteration: 6250  loss: 0.008146928623318672\n",
            " iteration: 6251  loss: 0.008146061562001705\n",
            " iteration: 6252  loss: 0.00814531184732914\n",
            " iteration: 6253  loss: 0.00814450066536665\n",
            " iteration: 6254  loss: 0.008143552578985691\n",
            " iteration: 6255  loss: 0.008141916245222092\n",
            " iteration: 6256  loss: 0.008151143789291382\n",
            " iteration: 6257  loss: 0.00814148411154747\n",
            " iteration: 6258  loss: 0.008140071295201778\n",
            " iteration: 6259  loss: 0.008138979785144329\n",
            " iteration: 6260  loss: 0.008138233795762062\n",
            " iteration: 6261  loss: 0.008137683384120464\n",
            " iteration: 6262  loss: 0.008137119002640247\n",
            " iteration: 6263  loss: 0.008136644028127193\n",
            " iteration: 6264  loss: 0.008136289194226265\n",
            " iteration: 6265  loss: 0.008135994896292686\n",
            " iteration: 6266  loss: 0.008135726675391197\n",
            " iteration: 6267  loss: 0.008134864270687103\n",
            " iteration: 6268  loss: 0.00813386682420969\n",
            " iteration: 6269  loss: 0.008132973685860634\n",
            " iteration: 6270  loss: 0.008132485672831535\n",
            " iteration: 6271  loss: 0.00813522469252348\n",
            " iteration: 6272  loss: 0.008132046088576317\n",
            " iteration: 6273  loss: 0.008131425827741623\n",
            " iteration: 6274  loss: 0.008130618371069431\n",
            " iteration: 6275  loss: 0.008129849098622799\n",
            " iteration: 6276  loss: 0.008129218593239784\n",
            " iteration: 6277  loss: 0.008128276094794273\n",
            " iteration: 6278  loss: 0.008127734065055847\n",
            " iteration: 6279  loss: 0.00812731496989727\n",
            " iteration: 6280  loss: 0.008126686327159405\n",
            " iteration: 6281  loss: 0.00812621135264635\n",
            " iteration: 6282  loss: 0.008125781081616879\n",
            " iteration: 6283  loss: 0.008125325664877892\n",
            " iteration: 6284  loss: 0.008124606683850288\n",
            " iteration: 6285  loss: 0.00812403205782175\n",
            " iteration: 6286  loss: 0.008123434148728848\n",
            " iteration: 6287  loss: 0.008123993873596191\n",
            " iteration: 6288  loss: 0.008122983388602734\n",
            " iteration: 6289  loss: 0.00812247209250927\n",
            " iteration: 6290  loss: 0.008121807128190994\n",
            " iteration: 6291  loss: 0.008121279999613762\n",
            " iteration: 6292  loss: 0.008120392449200153\n",
            " iteration: 6293  loss: 0.008119551464915276\n",
            " iteration: 6294  loss: 0.008118822239339352\n",
            " iteration: 6295  loss: 0.008118213154375553\n",
            " iteration: 6296  loss: 0.008117813616991043\n",
            " iteration: 6297  loss: 0.008117462508380413\n",
            " iteration: 6298  loss: 0.008117002435028553\n",
            " iteration: 6299  loss: 0.008116226643323898\n",
            " iteration: 6300  loss: 0.008115761913359165\n",
            " iteration: 6301  loss: 0.008114955388009548\n",
            " iteration: 6302  loss: 0.008114224299788475\n",
            " iteration: 6303  loss: 0.008113623596727848\n",
            " iteration: 6304  loss: 0.00811315793544054\n",
            " iteration: 6305  loss: 0.008112221956253052\n",
            " iteration: 6306  loss: 0.008112526498734951\n",
            " iteration: 6307  loss: 0.008111802861094475\n",
            " iteration: 6308  loss: 0.008110908791422844\n",
            " iteration: 6309  loss: 0.008109916932880878\n",
            " iteration: 6310  loss: 0.008108404465019703\n",
            " iteration: 6311  loss: 0.008108214475214481\n",
            " iteration: 6312  loss: 0.008106496185064316\n",
            " iteration: 6313  loss: 0.008105630986392498\n",
            " iteration: 6314  loss: 0.008104557171463966\n",
            " iteration: 6315  loss: 0.00810365192592144\n",
            " iteration: 6316  loss: 0.008102710358798504\n",
            " iteration: 6317  loss: 0.008101903833448887\n",
            " iteration: 6318  loss: 0.008101445622742176\n",
            " iteration: 6319  loss: 0.008101115934550762\n",
            " iteration: 6320  loss: 0.00810083094984293\n",
            " iteration: 6321  loss: 0.008100314997136593\n",
            " iteration: 6322  loss: 0.008099882863461971\n",
            " iteration: 6323  loss: 0.008099312894046307\n",
            " iteration: 6324  loss: 0.008099067956209183\n",
            " iteration: 6325  loss: 0.008098804391920567\n",
            " iteration: 6326  loss: 0.008098188787698746\n",
            " iteration: 6327  loss: 0.008097763173282146\n",
            " iteration: 6328  loss: 0.008097399957478046\n",
            " iteration: 6329  loss: 0.008097118698060513\n",
            " iteration: 6330  loss: 0.008096885867416859\n",
            " iteration: 6331  loss: 0.00809641182422638\n",
            " iteration: 6332  loss: 0.008095747791230679\n",
            " iteration: 6333  loss: 0.008095946162939072\n",
            " iteration: 6334  loss: 0.008095440454781055\n",
            " iteration: 6335  loss: 0.00809495896100998\n",
            " iteration: 6336  loss: 0.00809455756098032\n",
            " iteration: 6337  loss: 0.008094138465821743\n",
            " iteration: 6338  loss: 0.00809372030198574\n",
            " iteration: 6339  loss: 0.008093354292213917\n",
            " iteration: 6340  loss: 0.008095016703009605\n",
            " iteration: 6341  loss: 0.008093098178505898\n",
            " iteration: 6342  loss: 0.008092641830444336\n",
            " iteration: 6343  loss: 0.008092234842479229\n",
            " iteration: 6344  loss: 0.008093073032796383\n",
            " iteration: 6345  loss: 0.00809203740209341\n",
            " iteration: 6346  loss: 0.00809158943593502\n",
            " iteration: 6347  loss: 0.008091074414551258\n",
            " iteration: 6348  loss: 0.008090642280876637\n",
            " iteration: 6349  loss: 0.008090212941169739\n",
            " iteration: 6350  loss: 0.008089832961559296\n",
            " iteration: 6351  loss: 0.008089032955467701\n",
            " iteration: 6352  loss: 0.008088553324341774\n",
            " iteration: 6353  loss: 0.008088881149888039\n",
            " iteration: 6354  loss: 0.008088135160505772\n",
            " iteration: 6355  loss: 0.008097570389509201\n",
            " iteration: 6356  loss: 0.008087568916380405\n",
            " iteration: 6357  loss: 0.008086850866675377\n",
            " iteration: 6358  loss: 0.008085986599326134\n",
            " iteration: 6359  loss: 0.008085318841040134\n",
            " iteration: 6360  loss: 0.008083807304501534\n",
            " iteration: 6361  loss: 0.00809180736541748\n",
            " iteration: 6362  loss: 0.008083450607955456\n",
            " iteration: 6363  loss: 0.008082526735961437\n",
            " iteration: 6364  loss: 0.008081144653260708\n",
            " iteration: 6365  loss: 0.008079946972429752\n",
            " iteration: 6366  loss: 0.008078730665147305\n",
            " iteration: 6367  loss: 0.008077732287347317\n",
            " iteration: 6368  loss: 0.008077209815382957\n",
            " iteration: 6369  loss: 0.008076316677033901\n",
            " iteration: 6370  loss: 0.008075861260294914\n",
            " iteration: 6371  loss: 0.008075371384620667\n",
            " iteration: 6372  loss: 0.00807464774698019\n",
            " iteration: 6373  loss: 0.008074051700532436\n",
            " iteration: 6374  loss: 0.008073460310697556\n",
            " iteration: 6375  loss: 0.008072438649833202\n",
            " iteration: 6376  loss: 0.008071384392678738\n",
            " iteration: 6377  loss: 0.008070066571235657\n",
            " iteration: 6378  loss: 0.008068649098277092\n",
            " iteration: 6379  loss: 0.008067679591476917\n",
            " iteration: 6380  loss: 0.008066941052675247\n",
            " iteration: 6381  loss: 0.00806640274822712\n",
            " iteration: 6382  loss: 0.008065835572779179\n",
            " iteration: 6383  loss: 0.008065671660006046\n",
            " iteration: 6384  loss: 0.008063704706728458\n",
            " iteration: 6385  loss: 0.008063161745667458\n",
            " iteration: 6386  loss: 0.008062779903411865\n",
            " iteration: 6387  loss: 0.008061956614255905\n",
            " iteration: 6388  loss: 0.008061453700065613\n",
            " iteration: 6389  loss: 0.008060605265200138\n",
            " iteration: 6390  loss: 0.008059938438236713\n",
            " iteration: 6391  loss: 0.008059302344918251\n",
            " iteration: 6392  loss: 0.008058867417275906\n",
            " iteration: 6393  loss: 0.008059144020080566\n",
            " iteration: 6394  loss: 0.008058472536504269\n",
            " iteration: 6395  loss: 0.008057878352701664\n",
            " iteration: 6396  loss: 0.008057164028286934\n",
            " iteration: 6397  loss: 0.008056744001805782\n",
            " iteration: 6398  loss: 0.008056225255131721\n",
            " iteration: 6399  loss: 0.008055909536778927\n",
            " iteration: 6400  loss: 0.00805574469268322\n",
            " iteration: 6401  loss: 0.008055562153458595\n",
            " iteration: 6402  loss: 0.008055277168750763\n",
            " iteration: 6403  loss: 0.00805596075952053\n",
            " iteration: 6404  loss: 0.008055162616074085\n",
            " iteration: 6405  loss: 0.0080548832193017\n",
            " iteration: 6406  loss: 0.008054745383560658\n",
            " iteration: 6407  loss: 0.008054605685174465\n",
            " iteration: 6408  loss: 0.008054456673562527\n",
            " iteration: 6409  loss: 0.008054228499531746\n",
            " iteration: 6410  loss: 0.008053852245211601\n",
            " iteration: 6411  loss: 0.008053475990891457\n",
            " iteration: 6412  loss: 0.008053041063249111\n",
            " iteration: 6413  loss: 0.008052472956478596\n",
            " iteration: 6414  loss: 0.008051712065935135\n",
            " iteration: 6415  loss: 0.008050850592553616\n",
            " iteration: 6416  loss: 0.008050401695072651\n",
            " iteration: 6417  loss: 0.008049890398979187\n",
            " iteration: 6418  loss: 0.008048991672694683\n",
            " iteration: 6419  loss: 0.008057579398155212\n",
            " iteration: 6420  loss: 0.008048725314438343\n",
            " iteration: 6421  loss: 0.008047923445701599\n",
            " iteration: 6422  loss: 0.008047599345445633\n",
            " iteration: 6423  loss: 0.008046959526836872\n",
            " iteration: 6424  loss: 0.008046364411711693\n",
            " iteration: 6425  loss: 0.008045771159231663\n",
            " iteration: 6426  loss: 0.008045362308621407\n",
            " iteration: 6427  loss: 0.008045083843171597\n",
            " iteration: 6428  loss: 0.008044861257076263\n",
            " iteration: 6429  loss: 0.008044594898819923\n",
            " iteration: 6430  loss: 0.008044443093240261\n",
            " iteration: 6431  loss: 0.00804511271417141\n",
            " iteration: 6432  loss: 0.008043844252824783\n",
            " iteration: 6433  loss: 0.00804336927831173\n",
            " iteration: 6434  loss: 0.008042673580348492\n",
            " iteration: 6435  loss: 0.008042420260608196\n",
            " iteration: 6436  loss: 0.008041439577937126\n",
            " iteration: 6437  loss: 0.008040993474423885\n",
            " iteration: 6438  loss: 0.008040587417781353\n",
            " iteration: 6439  loss: 0.008040281943976879\n",
            " iteration: 6440  loss: 0.008039604872465134\n",
            " iteration: 6441  loss: 0.008039225824177265\n",
            " iteration: 6442  loss: 0.008038639090955257\n",
            " iteration: 6443  loss: 0.008038260042667389\n",
            " iteration: 6444  loss: 0.008037861436605453\n",
            " iteration: 6445  loss: 0.008037496358156204\n",
            " iteration: 6446  loss: 0.008037055842578411\n",
            " iteration: 6447  loss: 0.00803639367222786\n",
            " iteration: 6448  loss: 0.008044064044952393\n",
            " iteration: 6449  loss: 0.008036293089389801\n",
            " iteration: 6450  loss: 0.008035719394683838\n",
            " iteration: 6451  loss: 0.00803552009165287\n",
            " iteration: 6452  loss: 0.00803463440388441\n",
            " iteration: 6453  loss: 0.008033985272049904\n",
            " iteration: 6454  loss: 0.008033445104956627\n",
            " iteration: 6455  loss: 0.00803312286734581\n",
            " iteration: 6456  loss: 0.008032714948058128\n",
            " iteration: 6457  loss: 0.00803219061344862\n",
            " iteration: 6458  loss: 0.008031555451452732\n",
            " iteration: 6459  loss: 0.008031236939132214\n",
            " iteration: 6460  loss: 0.008030762895941734\n",
            " iteration: 6461  loss: 0.008030327036976814\n",
            " iteration: 6462  loss: 0.008029663935303688\n",
            " iteration: 6463  loss: 0.008029107935726643\n",
            " iteration: 6464  loss: 0.00802835077047348\n",
            " iteration: 6465  loss: 0.00802767463028431\n",
            " iteration: 6466  loss: 0.0080271540209651\n",
            " iteration: 6467  loss: 0.00802680291235447\n",
            " iteration: 6468  loss: 0.008026204071938992\n",
            " iteration: 6469  loss: 0.008025954477488995\n",
            " iteration: 6470  loss: 0.008025163784623146\n",
            " iteration: 6471  loss: 0.00802457146346569\n",
            " iteration: 6472  loss: 0.008023645728826523\n",
            " iteration: 6473  loss: 0.00802257563918829\n",
            " iteration: 6474  loss: 0.008022598922252655\n",
            " iteration: 6475  loss: 0.008021712303161621\n",
            " iteration: 6476  loss: 0.008020608685910702\n",
            " iteration: 6477  loss: 0.008019870147109032\n",
            " iteration: 6478  loss: 0.008019531145691872\n",
            " iteration: 6479  loss: 0.008021126501262188\n",
            " iteration: 6480  loss: 0.008019332773983479\n",
            " iteration: 6481  loss: 0.008019031025469303\n",
            " iteration: 6482  loss: 0.00801884476095438\n",
            " iteration: 6483  loss: 0.00801865104585886\n",
            " iteration: 6484  loss: 0.008018393069505692\n",
            " iteration: 6485  loss: 0.008018101565539837\n",
            " iteration: 6486  loss: 0.00801747478544712\n",
            " iteration: 6487  loss: 0.008018174208700657\n",
            " iteration: 6488  loss: 0.008017189800739288\n",
            " iteration: 6489  loss: 0.00801659282296896\n",
            " iteration: 6490  loss: 0.00801583006978035\n",
            " iteration: 6491  loss: 0.008015143685042858\n",
            " iteration: 6492  loss: 0.008014291524887085\n",
            " iteration: 6493  loss: 0.00801948830485344\n",
            " iteration: 6494  loss: 0.008014040999114513\n",
            " iteration: 6495  loss: 0.008013468235731125\n",
            " iteration: 6496  loss: 0.008012807928025723\n",
            " iteration: 6497  loss: 0.008011244237422943\n",
            " iteration: 6498  loss: 0.008009673096239567\n",
            " iteration: 6499  loss: 0.00800801906734705\n",
            " iteration: 6500  loss: 0.008006742224097252\n",
            " iteration: 6501  loss: 0.008005916140973568\n",
            " iteration: 6502  loss: 0.00800542626529932\n",
            " iteration: 6503  loss: 0.008005041629076004\n",
            " iteration: 6504  loss: 0.008004321716725826\n",
            " iteration: 6505  loss: 0.008003825321793556\n",
            " iteration: 6506  loss: 0.008013848215341568\n",
            " iteration: 6507  loss: 0.00800353940576315\n",
            " iteration: 6508  loss: 0.008003173395991325\n",
            " iteration: 6509  loss: 0.008002722635865211\n",
            " iteration: 6510  loss: 0.008002379909157753\n",
            " iteration: 6511  loss: 0.008001698181033134\n",
            " iteration: 6512  loss: 0.008000873029232025\n",
            " iteration: 6513  loss: 0.008003372699022293\n",
            " iteration: 6514  loss: 0.008000398054718971\n",
            " iteration: 6515  loss: 0.007999178022146225\n",
            " iteration: 6516  loss: 0.007998513989150524\n",
            " iteration: 6517  loss: 0.008039726875722408\n",
            " iteration: 6518  loss: 0.00799829326570034\n",
            " iteration: 6519  loss: 0.007997081615030766\n",
            " iteration: 6520  loss: 0.007995795458555222\n",
            " iteration: 6521  loss: 0.007993667386472225\n",
            " iteration: 6522  loss: 0.008020573295652866\n",
            " iteration: 6523  loss: 0.007993144914507866\n",
            " iteration: 6524  loss: 0.007991756312549114\n",
            " iteration: 6525  loss: 0.007990960963070393\n",
            " iteration: 6526  loss: 0.00798905361443758\n",
            " iteration: 6527  loss: 0.007988073863089085\n",
            " iteration: 6528  loss: 0.007986532524228096\n",
            " iteration: 6529  loss: 0.007985413074493408\n",
            " iteration: 6530  loss: 0.007985440082848072\n",
            " iteration: 6531  loss: 0.007984909228980541\n",
            " iteration: 6532  loss: 0.007984274998307228\n",
            " iteration: 6533  loss: 0.007983888499438763\n",
            " iteration: 6534  loss: 0.00798338744789362\n",
            " iteration: 6535  loss: 0.007983090355992317\n",
            " iteration: 6536  loss: 0.007982710376381874\n",
            " iteration: 6537  loss: 0.007982386276125908\n",
            " iteration: 6538  loss: 0.007981808856129646\n",
            " iteration: 6539  loss: 0.007981437258422375\n",
            " iteration: 6540  loss: 0.00798067171126604\n",
            " iteration: 6541  loss: 0.007983281277120113\n",
            " iteration: 6542  loss: 0.00798046961426735\n",
            " iteration: 6543  loss: 0.007979894988238811\n",
            " iteration: 6544  loss: 0.007979577407240868\n",
            " iteration: 6545  loss: 0.007978680543601513\n",
            " iteration: 6546  loss: 0.007978066802024841\n",
            " iteration: 6547  loss: 0.007977575995028019\n",
            " iteration: 6548  loss: 0.007976090535521507\n",
            " iteration: 6549  loss: 0.007975626736879349\n",
            " iteration: 6550  loss: 0.007974459789693356\n",
            " iteration: 6551  loss: 0.007973174564540386\n",
            " iteration: 6552  loss: 0.007972036488354206\n",
            " iteration: 6553  loss: 0.0079708993434906\n",
            " iteration: 6554  loss: 0.007969558238983154\n",
            " iteration: 6555  loss: 0.007967657409608364\n",
            " iteration: 6556  loss: 0.00796670001000166\n",
            " iteration: 6557  loss: 0.007965772412717342\n",
            " iteration: 6558  loss: 0.007965227589011192\n",
            " iteration: 6559  loss: 0.007964317686855793\n",
            " iteration: 6560  loss: 0.007963466458022594\n",
            " iteration: 6561  loss: 0.007962473668158054\n",
            " iteration: 6562  loss: 0.007961473427712917\n",
            " iteration: 6563  loss: 0.007961062714457512\n",
            " iteration: 6564  loss: 0.007960657589137554\n",
            " iteration: 6565  loss: 0.007960407994687557\n",
            " iteration: 6566  loss: 0.00796019472181797\n",
            " iteration: 6567  loss: 0.007960007525980473\n",
            " iteration: 6568  loss: 0.007959450595080853\n",
            " iteration: 6569  loss: 0.007958613336086273\n",
            " iteration: 6570  loss: 0.007957090623676777\n",
            " iteration: 6571  loss: 0.007959533482789993\n",
            " iteration: 6572  loss: 0.007956362329423428\n",
            " iteration: 6573  loss: 0.007954919710755348\n",
            " iteration: 6574  loss: 0.007954179309308529\n",
            " iteration: 6575  loss: 0.007953502237796783\n",
            " iteration: 6576  loss: 0.007952759973704815\n",
            " iteration: 6577  loss: 0.007951249368488789\n",
            " iteration: 6578  loss: 0.007950203493237495\n",
            " iteration: 6579  loss: 0.007949615828692913\n",
            " iteration: 6580  loss: 0.00795011781156063\n",
            " iteration: 6581  loss: 0.007949230261147022\n",
            " iteration: 6582  loss: 0.007948585785925388\n",
            " iteration: 6583  loss: 0.007947838865220547\n",
            " iteration: 6584  loss: 0.007946720346808434\n",
            " iteration: 6585  loss: 0.007945558987557888\n",
            " iteration: 6586  loss: 0.007945303805172443\n",
            " iteration: 6587  loss: 0.00794441718608141\n",
            " iteration: 6588  loss: 0.007944053038954735\n",
            " iteration: 6589  loss: 0.007943838834762573\n",
            " iteration: 6590  loss: 0.007943501695990562\n",
            " iteration: 6591  loss: 0.007943137548863888\n",
            " iteration: 6592  loss: 0.00794288981705904\n",
            " iteration: 6593  loss: 0.007942664436995983\n",
            " iteration: 6594  loss: 0.007942458614706993\n",
            " iteration: 6595  loss: 0.00794176384806633\n",
            " iteration: 6596  loss: 0.00794291589409113\n",
            " iteration: 6597  loss: 0.007941565476357937\n",
            " iteration: 6598  loss: 0.007941089570522308\n",
            " iteration: 6599  loss: 0.007940765470266342\n",
            " iteration: 6600  loss: 0.00794027466326952\n",
            " iteration: 6601  loss: 0.007939769886434078\n",
            " iteration: 6602  loss: 0.007940683513879776\n",
            " iteration: 6603  loss: 0.007939537055790424\n",
            " iteration: 6604  loss: 0.00793909840285778\n",
            " iteration: 6605  loss: 0.007938092574477196\n",
            " iteration: 6606  loss: 0.00793746579438448\n",
            " iteration: 6607  loss: 0.007936877198517323\n",
            " iteration: 6608  loss: 0.007936290465295315\n",
            " iteration: 6609  loss: 0.007935593836009502\n",
            " iteration: 6610  loss: 0.007935048080980778\n",
            " iteration: 6611  loss: 0.007934491150081158\n",
            " iteration: 6612  loss: 0.007933685556054115\n",
            " iteration: 6613  loss: 0.007933062501251698\n",
            " iteration: 6614  loss: 0.007932406850159168\n",
            " iteration: 6615  loss: 0.007933427579700947\n",
            " iteration: 6616  loss: 0.007932158187031746\n",
            " iteration: 6617  loss: 0.007931848987936974\n",
            " iteration: 6618  loss: 0.007931637577712536\n",
            " iteration: 6619  loss: 0.007931257598102093\n",
            " iteration: 6620  loss: 0.007930883206427097\n",
            " iteration: 6621  loss: 0.007930402643978596\n",
            " iteration: 6622  loss: 0.007929898798465729\n",
            " iteration: 6623  loss: 0.00792937632650137\n",
            " iteration: 6624  loss: 0.007928636856377125\n",
            " iteration: 6625  loss: 0.007928090170025826\n",
            " iteration: 6626  loss: 0.007927250117063522\n",
            " iteration: 6627  loss: 0.007926526479423046\n",
            " iteration: 6628  loss: 0.007926040329039097\n",
            " iteration: 6629  loss: 0.007925734855234623\n",
            " iteration: 6630  loss: 0.00792550016194582\n",
            " iteration: 6631  loss: 0.007925261743366718\n",
            " iteration: 6632  loss: 0.007924995385110378\n",
            " iteration: 6633  loss: 0.007924600504338741\n",
            " iteration: 6634  loss: 0.007923992350697517\n",
            " iteration: 6635  loss: 0.007924335077404976\n",
            " iteration: 6636  loss: 0.007923610508441925\n",
            " iteration: 6637  loss: 0.00792216882109642\n",
            " iteration: 6638  loss: 0.007921772077679634\n",
            " iteration: 6639  loss: 0.007920186035335064\n",
            " iteration: 6640  loss: 0.007919054478406906\n",
            " iteration: 6641  loss: 0.007918047718703747\n",
            " iteration: 6642  loss: 0.007916612550616264\n",
            " iteration: 6643  loss: 0.007924754172563553\n",
            " iteration: 6644  loss: 0.007915861904621124\n",
            " iteration: 6645  loss: 0.007914703339338303\n",
            " iteration: 6646  loss: 0.007913478650152683\n",
            " iteration: 6647  loss: 0.00791262835264206\n",
            " iteration: 6648  loss: 0.007911200635135174\n",
            " iteration: 6649  loss: 0.007910040207207203\n",
            " iteration: 6650  loss: 0.007909347303211689\n",
            " iteration: 6651  loss: 0.007908854633569717\n",
            " iteration: 6652  loss: 0.007912090048193932\n",
            " iteration: 6653  loss: 0.007908730767667294\n",
            " iteration: 6654  loss: 0.007908370345830917\n",
            " iteration: 6655  loss: 0.007907615043222904\n",
            " iteration: 6656  loss: 0.007906867191195488\n",
            " iteration: 6657  loss: 0.00790580827742815\n",
            " iteration: 6658  loss: 0.007905170321464539\n",
            " iteration: 6659  loss: 0.007913053967058659\n",
            " iteration: 6660  loss: 0.007905060425400734\n",
            " iteration: 6661  loss: 0.007904693484306335\n",
            " iteration: 6662  loss: 0.007904481142759323\n",
            " iteration: 6663  loss: 0.007904190570116043\n",
            " iteration: 6664  loss: 0.00790407508611679\n",
            " iteration: 6665  loss: 0.00790358241647482\n",
            " iteration: 6666  loss: 0.007903059013187885\n",
            " iteration: 6667  loss: 0.007902517914772034\n",
            " iteration: 6668  loss: 0.007901689037680626\n",
            " iteration: 6669  loss: 0.007900913245975971\n",
            " iteration: 6670  loss: 0.007900192402303219\n",
            " iteration: 6671  loss: 0.007899531163275242\n",
            " iteration: 6672  loss: 0.00789883267134428\n",
            " iteration: 6673  loss: 0.007898123003542423\n",
            " iteration: 6674  loss: 0.007897465489804745\n",
            " iteration: 6675  loss: 0.007896723225712776\n",
            " iteration: 6676  loss: 0.00789610855281353\n",
            " iteration: 6677  loss: 0.007895750924944878\n",
            " iteration: 6678  loss: 0.00789708737283945\n",
            " iteration: 6679  loss: 0.007895516231656075\n",
            " iteration: 6680  loss: 0.007895037531852722\n",
            " iteration: 6681  loss: 0.00789438933134079\n",
            " iteration: 6682  loss: 0.007894189096987247\n",
            " iteration: 6683  loss: 0.007894331589341164\n",
            " iteration: 6684  loss: 0.007894016802310944\n",
            " iteration: 6685  loss: 0.007893762551248074\n",
            " iteration: 6686  loss: 0.007893314585089684\n",
            " iteration: 6687  loss: 0.007902401499450207\n",
            " iteration: 6688  loss: 0.007893107831478119\n",
            " iteration: 6689  loss: 0.007892707362771034\n",
            " iteration: 6690  loss: 0.007892177440226078\n",
            " iteration: 6691  loss: 0.00789348129183054\n",
            " iteration: 6692  loss: 0.007891782559454441\n",
            " iteration: 6693  loss: 0.007891208864748478\n",
            " iteration: 6694  loss: 0.007890289649367332\n",
            " iteration: 6695  loss: 0.007889838889241219\n",
            " iteration: 6696  loss: 0.007889200001955032\n",
            " iteration: 6697  loss: 0.007888715714216232\n",
            " iteration: 6698  loss: 0.007888205349445343\n",
            " iteration: 6699  loss: 0.007887440733611584\n",
            " iteration: 6700  loss: 0.007892762310802937\n",
            " iteration: 6701  loss: 0.007887221872806549\n",
            " iteration: 6702  loss: 0.007886496372520924\n",
            " iteration: 6703  loss: 0.007886247709393501\n",
            " iteration: 6704  loss: 0.007885678671300411\n",
            " iteration: 6705  loss: 0.007885459810495377\n",
            " iteration: 6706  loss: 0.00788510125130415\n",
            " iteration: 6707  loss: 0.00788478460162878\n",
            " iteration: 6708  loss: 0.007884378544986248\n",
            " iteration: 6709  loss: 0.007883919402956963\n",
            " iteration: 6710  loss: 0.00788323674350977\n",
            " iteration: 6711  loss: 0.007883120328187943\n",
            " iteration: 6712  loss: 0.007882297970354557\n",
            " iteration: 6713  loss: 0.007881971076130867\n",
            " iteration: 6714  loss: 0.007881715893745422\n",
            " iteration: 6715  loss: 0.007881498895585537\n",
            " iteration: 6716  loss: 0.00788111425936222\n",
            " iteration: 6717  loss: 0.007882689125835896\n",
            " iteration: 6718  loss: 0.007880855351686478\n",
            " iteration: 6719  loss: 0.00788046419620514\n",
            " iteration: 6720  loss: 0.007880001328885555\n",
            " iteration: 6721  loss: 0.00787972193211317\n",
            " iteration: 6722  loss: 0.007879366166889668\n",
            " iteration: 6723  loss: 0.007879037410020828\n",
            " iteration: 6724  loss: 0.007878217846155167\n",
            " iteration: 6725  loss: 0.007877694442868233\n",
            " iteration: 6726  loss: 0.007876954041421413\n",
            " iteration: 6727  loss: 0.007876192219555378\n",
            " iteration: 6728  loss: 0.00787554681301117\n",
            " iteration: 6729  loss: 0.007876889780163765\n",
            " iteration: 6730  loss: 0.007875279523432255\n",
            " iteration: 6731  loss: 0.00787446741014719\n",
            " iteration: 6732  loss: 0.007873895578086376\n",
            " iteration: 6733  loss: 0.007873404771089554\n",
            " iteration: 6734  loss: 0.00787263736128807\n",
            " iteration: 6735  loss: 0.007871842011809349\n",
            " iteration: 6736  loss: 0.00787108764052391\n",
            " iteration: 6737  loss: 0.007870671339333057\n",
            " iteration: 6738  loss: 0.007870044559240341\n",
            " iteration: 6739  loss: 0.007869469001889229\n",
            " iteration: 6740  loss: 0.007868935354053974\n",
            " iteration: 6741  loss: 0.00786859542131424\n",
            " iteration: 6742  loss: 0.007868162356317043\n",
            " iteration: 6743  loss: 0.007867823354899883\n",
            " iteration: 6744  loss: 0.007867416366934776\n",
            " iteration: 6745  loss: 0.007866881787776947\n",
            " iteration: 6746  loss: 0.007866302505135536\n",
            " iteration: 6747  loss: 0.007864945568144321\n",
            " iteration: 6748  loss: 0.00786421075463295\n",
            " iteration: 6749  loss: 0.007864159531891346\n",
            " iteration: 6750  loss: 0.007863746024668217\n",
            " iteration: 6751  loss: 0.007862856611609459\n",
            " iteration: 6752  loss: 0.007862257771193981\n",
            " iteration: 6753  loss: 0.007861772552132607\n",
            " iteration: 6754  loss: 0.007861394435167313\n",
            " iteration: 6755  loss: 0.007860596291720867\n",
            " iteration: 6756  loss: 0.00785994902253151\n",
            " iteration: 6757  loss: 0.007859556004405022\n",
            " iteration: 6758  loss: 0.00785890780389309\n",
            " iteration: 6759  loss: 0.007861129008233547\n",
            " iteration: 6760  loss: 0.007858667522668839\n",
            " iteration: 6761  loss: 0.007858090102672577\n",
            " iteration: 6762  loss: 0.007857240736484528\n",
            " iteration: 6763  loss: 0.007856827229261398\n",
            " iteration: 6764  loss: 0.007856426760554314\n",
            " iteration: 6765  loss: 0.007857512682676315\n",
            " iteration: 6766  loss: 0.00785613153129816\n",
            " iteration: 6767  loss: 0.007855702191591263\n",
            " iteration: 6768  loss: 0.007854733616113663\n",
            " iteration: 6769  loss: 0.007854080758988857\n",
            " iteration: 6770  loss: 0.007853726856410503\n",
            " iteration: 6771  loss: 0.007853186689317226\n",
            " iteration: 6772  loss: 0.007852860726416111\n",
            " iteration: 6773  loss: 0.007852510549128056\n",
            " iteration: 6774  loss: 0.00785208772867918\n",
            " iteration: 6775  loss: 0.007851632311940193\n",
            " iteration: 6776  loss: 0.007850561290979385\n",
            " iteration: 6777  loss: 0.007856703363358974\n",
            " iteration: 6778  loss: 0.0078504029661417\n",
            " iteration: 6779  loss: 0.00785011425614357\n",
            " iteration: 6780  loss: 0.007849526591598988\n",
            " iteration: 6781  loss: 0.007849021814763546\n",
            " iteration: 6782  loss: 0.007848299108445644\n",
            " iteration: 6783  loss: 0.007847366854548454\n",
            " iteration: 6784  loss: 0.007847855798900127\n",
            " iteration: 6785  loss: 0.00784672424197197\n",
            " iteration: 6786  loss: 0.007845480926334858\n",
            " iteration: 6787  loss: 0.00784487184137106\n",
            " iteration: 6788  loss: 0.007844248786568642\n",
            " iteration: 6789  loss: 0.00784801971167326\n",
            " iteration: 6790  loss: 0.007844052277505398\n",
            " iteration: 6791  loss: 0.007843581959605217\n",
            " iteration: 6792  loss: 0.007842687889933586\n",
            " iteration: 6793  loss: 0.007842113263905048\n",
            " iteration: 6794  loss: 0.007841233164072037\n",
            " iteration: 6795  loss: 0.007840218022465706\n",
            " iteration: 6796  loss: 0.007840284146368504\n",
            " iteration: 6797  loss: 0.007839566096663475\n",
            " iteration: 6798  loss: 0.007837945595383644\n",
            " iteration: 6799  loss: 0.007836801931262016\n",
            " iteration: 6800  loss: 0.007835736498236656\n",
            " iteration: 6801  loss: 0.007835726253688335\n",
            " iteration: 6802  loss: 0.007835373282432556\n",
            " iteration: 6803  loss: 0.007835065945982933\n",
            " iteration: 6804  loss: 0.007834257557988167\n",
            " iteration: 6805  loss: 0.007833953015506268\n",
            " iteration: 6806  loss: 0.007833464071154594\n",
            " iteration: 6807  loss: 0.00783263985067606\n",
            " iteration: 6808  loss: 0.00783146359026432\n",
            " iteration: 6809  loss: 0.007830929942429066\n",
            " iteration: 6810  loss: 0.007829427719116211\n",
            " iteration: 6811  loss: 0.007828785106539726\n",
            " iteration: 6812  loss: 0.007828307338058949\n",
            " iteration: 6813  loss: 0.007827711291611195\n",
            " iteration: 6814  loss: 0.007826891727745533\n",
            " iteration: 6815  loss: 0.00782552920281887\n",
            " iteration: 6816  loss: 0.007823755964636803\n",
            " iteration: 6817  loss: 0.007821712642908096\n",
            " iteration: 6818  loss: 0.007822629995644093\n",
            " iteration: 6819  loss: 0.007821064442396164\n",
            " iteration: 6820  loss: 0.007820282131433487\n",
            " iteration: 6821  loss: 0.007819011807441711\n",
            " iteration: 6822  loss: 0.007818305864930153\n",
            " iteration: 6823  loss: 0.007817745208740234\n",
            " iteration: 6824  loss: 0.007817295379936695\n",
            " iteration: 6825  loss: 0.007816653698682785\n",
            " iteration: 6826  loss: 0.007816036231815815\n",
            " iteration: 6827  loss: 0.007815053686499596\n",
            " iteration: 6828  loss: 0.007814330980181694\n",
            " iteration: 6829  loss: 0.007813354022800922\n",
            " iteration: 6830  loss: 0.007812626659870148\n",
            " iteration: 6831  loss: 0.00781164551153779\n",
            " iteration: 6832  loss: 0.007810922339558601\n",
            " iteration: 6833  loss: 0.007810078561306\n",
            " iteration: 6834  loss: 0.007809125818312168\n",
            " iteration: 6835  loss: 0.007808464113622904\n",
            " iteration: 6836  loss: 0.007807325106114149\n",
            " iteration: 6837  loss: 0.007806835230439901\n",
            " iteration: 6838  loss: 0.007805648725479841\n",
            " iteration: 6839  loss: 0.007804867345839739\n",
            " iteration: 6840  loss: 0.007804257795214653\n",
            " iteration: 6841  loss: 0.007803428918123245\n",
            " iteration: 6842  loss: 0.007802222389727831\n",
            " iteration: 6843  loss: 0.0078012640587985516\n",
            " iteration: 6844  loss: 0.0078002773225307465\n",
            " iteration: 6845  loss: 0.007799458224326372\n",
            " iteration: 6846  loss: 0.007798922248184681\n",
            " iteration: 6847  loss: 0.007797992322593927\n",
            " iteration: 6848  loss: 0.007797286380082369\n",
            " iteration: 6849  loss: 0.007796099875122309\n",
            " iteration: 6850  loss: 0.007795372046530247\n",
            " iteration: 6851  loss: 0.007794899865984917\n",
            " iteration: 6852  loss: 0.007794546894729137\n",
            " iteration: 6853  loss: 0.007794286124408245\n",
            " iteration: 6854  loss: 0.007793762255460024\n",
            " iteration: 6855  loss: 0.00779508613049984\n",
            " iteration: 6856  loss: 0.007793447468429804\n",
            " iteration: 6857  loss: 0.007792774122208357\n",
            " iteration: 6858  loss: 0.007791568990796804\n",
            " iteration: 6859  loss: 0.007790568750351667\n",
            " iteration: 6860  loss: 0.007789277471601963\n",
            " iteration: 6861  loss: 0.0077915554866194725\n",
            " iteration: 6862  loss: 0.007788850925862789\n",
            " iteration: 6863  loss: 0.007788193412125111\n",
            " iteration: 6864  loss: 0.007787621114403009\n",
            " iteration: 6865  loss: 0.007787303999066353\n",
            " iteration: 6866  loss: 0.0077866967767477036\n",
            " iteration: 6867  loss: 0.00778593122959137\n",
            " iteration: 6868  loss: 0.0077859098091721535\n",
            " iteration: 6869  loss: 0.007785456720739603\n",
            " iteration: 6870  loss: 0.007784647401422262\n",
            " iteration: 6871  loss: 0.0077842227183282375\n",
            " iteration: 6872  loss: 0.007783802691847086\n",
            " iteration: 6873  loss: 0.007783087901771069\n",
            " iteration: 6874  loss: 0.007782827131450176\n",
            " iteration: 6875  loss: 0.007782681379467249\n",
            " iteration: 6876  loss: 0.0077825142070651054\n",
            " iteration: 6877  loss: 0.00778229720890522\n",
            " iteration: 6878  loss: 0.007783780340105295\n",
            " iteration: 6879  loss: 0.007782147265970707\n",
            " iteration: 6880  loss: 0.007781632244586945\n",
            " iteration: 6881  loss: 0.00778110371902585\n",
            " iteration: 6882  loss: 0.007780678104609251\n",
            " iteration: 6883  loss: 0.007780410815030336\n",
            " iteration: 6884  loss: 0.007780603598803282\n",
            " iteration: 6885  loss: 0.007780143059790134\n",
            " iteration: 6886  loss: 0.007779785897582769\n",
            " iteration: 6887  loss: 0.007778624538332224\n",
            " iteration: 6888  loss: 0.00777837261557579\n",
            " iteration: 6889  loss: 0.0077780126594007015\n",
            " iteration: 6890  loss: 0.0077776252292096615\n",
            " iteration: 6891  loss: 0.007777134422212839\n",
            " iteration: 6892  loss: 0.007776147220283747\n",
            " iteration: 6893  loss: 0.007774979341775179\n",
            " iteration: 6894  loss: 0.007775295991450548\n",
            " iteration: 6895  loss: 0.007774533703923225\n",
            " iteration: 6896  loss: 0.007773755583912134\n",
            " iteration: 6897  loss: 0.007773216813802719\n",
            " iteration: 6898  loss: 0.007772776298224926\n",
            " iteration: 6899  loss: 0.007771699223667383\n",
            " iteration: 6900  loss: 0.007771620061248541\n",
            " iteration: 6901  loss: 0.007771053817123175\n",
            " iteration: 6902  loss: 0.007770297583192587\n",
            " iteration: 6903  loss: 0.0077695720829069614\n",
            " iteration: 6904  loss: 0.007769038900732994\n",
            " iteration: 6905  loss: 0.007768666837364435\n",
            " iteration: 6906  loss: 0.0077681527473032475\n",
            " iteration: 6907  loss: 0.007767556235194206\n",
            " iteration: 6908  loss: 0.007766930386424065\n",
            " iteration: 6909  loss: 0.0077663445845246315\n",
            " iteration: 6910  loss: 0.007765984162688255\n",
            " iteration: 6911  loss: 0.007765764370560646\n",
            " iteration: 6912  loss: 0.007765523623675108\n",
            " iteration: 6913  loss: 0.007765212561935186\n",
            " iteration: 6914  loss: 0.007764434441924095\n",
            " iteration: 6915  loss: 0.007763523142784834\n",
            " iteration: 6916  loss: 0.007762854918837547\n",
            " iteration: 6917  loss: 0.007762315683066845\n",
            " iteration: 6918  loss: 0.00776148634031415\n",
            " iteration: 6919  loss: 0.007761287037283182\n",
            " iteration: 6920  loss: 0.007760379463434219\n",
            " iteration: 6921  loss: 0.007759092375636101\n",
            " iteration: 6922  loss: 0.0077582006342709064\n",
            " iteration: 6923  loss: 0.0077568781562149525\n",
            " iteration: 6924  loss: 0.007755882106721401\n",
            " iteration: 6925  loss: 0.007802712731063366\n",
            " iteration: 6926  loss: 0.007755710277706385\n",
            " iteration: 6927  loss: 0.007755045313388109\n",
            " iteration: 6928  loss: 0.007754964288324118\n",
            " iteration: 6929  loss: 0.007754565216600895\n",
            " iteration: 6930  loss: 0.007754437159746885\n",
            " iteration: 6931  loss: 0.007754140067845583\n",
            " iteration: 6932  loss: 0.007753997575491667\n",
            " iteration: 6933  loss: 0.007753681857138872\n",
            " iteration: 6934  loss: 0.007753512356430292\n",
            " iteration: 6935  loss: 0.00775332935154438\n",
            " iteration: 6936  loss: 0.0077530937269330025\n",
            " iteration: 6937  loss: 0.007752473000437021\n",
            " iteration: 6938  loss: 0.0077710505574941635\n",
            " iteration: 6939  loss: 0.0077523645013570786\n",
            " iteration: 6940  loss: 0.007751700468361378\n",
            " iteration: 6941  loss: 0.007750529330223799\n",
            " iteration: 6942  loss: 0.0077498359605669975\n",
            " iteration: 6943  loss: 0.007749469019472599\n",
            " iteration: 6944  loss: 0.007749028969556093\n",
            " iteration: 6945  loss: 0.00774864898994565\n",
            " iteration: 6946  loss: 0.007748065982013941\n",
            " iteration: 6947  loss: 0.0077474480494856834\n",
            " iteration: 6948  loss: 0.007746370043605566\n",
            " iteration: 6949  loss: 0.007745400071144104\n",
            " iteration: 6950  loss: 0.007744916249066591\n",
            " iteration: 6951  loss: 0.007744484581053257\n",
            " iteration: 6952  loss: 0.007744218222796917\n",
            " iteration: 6953  loss: 0.007743810303509235\n",
            " iteration: 6954  loss: 0.007743312977254391\n",
            " iteration: 6955  loss: 0.007742736022919416\n",
            " iteration: 6956  loss: 0.0077422987669706345\n",
            " iteration: 6957  loss: 0.007741726469248533\n",
            " iteration: 6958  loss: 0.00774230994284153\n",
            " iteration: 6959  loss: 0.007741220761090517\n",
            " iteration: 6960  loss: 0.007740584202110767\n",
            " iteration: 6961  loss: 0.007739059627056122\n",
            " iteration: 6962  loss: 0.007737908978015184\n",
            " iteration: 6963  loss: 0.007738232146948576\n",
            " iteration: 6964  loss: 0.007736954838037491\n",
            " iteration: 6965  loss: 0.0077360509894788265\n",
            " iteration: 6966  loss: 0.007735022343695164\n",
            " iteration: 6967  loss: 0.007734091021120548\n",
            " iteration: 6968  loss: 0.007733101025223732\n",
            " iteration: 6969  loss: 0.007733767386525869\n",
            " iteration: 6970  loss: 0.007732510566711426\n",
            " iteration: 6971  loss: 0.0077319396659731865\n",
            " iteration: 6972  loss: 0.007731369696557522\n",
            " iteration: 6973  loss: 0.00773086491972208\n",
            " iteration: 6974  loss: 0.0077300225384533405\n",
            " iteration: 6975  loss: 0.007729568984359503\n",
            " iteration: 6976  loss: 0.007729322649538517\n",
            " iteration: 6977  loss: 0.007728858385235071\n",
            " iteration: 6978  loss: 0.00772864231839776\n",
            " iteration: 6979  loss: 0.00772845046594739\n",
            " iteration: 6980  loss: 0.007728095632046461\n",
            " iteration: 6981  loss: 0.007727320771664381\n",
            " iteration: 6982  loss: 0.007734522223472595\n",
            " iteration: 6983  loss: 0.0077271717600524426\n",
            " iteration: 6984  loss: 0.007726381998509169\n",
            " iteration: 6985  loss: 0.007725662551820278\n",
            " iteration: 6986  loss: 0.007725250907242298\n",
            " iteration: 6987  loss: 0.007724979426711798\n",
            " iteration: 6988  loss: 0.0077246855944395065\n",
            " iteration: 6989  loss: 0.007724317722022533\n",
            " iteration: 6990  loss: 0.007723989896476269\n",
            " iteration: 6991  loss: 0.007723310962319374\n",
            " iteration: 6992  loss: 0.007722522597759962\n",
            " iteration: 6993  loss: 0.007721823174506426\n",
            " iteration: 6994  loss: 0.007721350993961096\n",
            " iteration: 6995  loss: 0.007724198512732983\n",
            " iteration: 6996  loss: 0.007721256464719772\n",
            " iteration: 6997  loss: 0.007720991503447294\n",
            " iteration: 6998  loss: 0.00772064970806241\n",
            " iteration: 6999  loss: 0.007720363326370716\n",
            " iteration: 7000  loss: 0.007719636429101229\n",
            " iteration: 7001  loss: 0.007719219196587801\n",
            " iteration: 7002  loss: 0.007718875538557768\n",
            " iteration: 7003  loss: 0.007718488108366728\n",
            " iteration: 7004  loss: 0.007718289270997047\n",
            " iteration: 7005  loss: 0.007718164008110762\n",
            " iteration: 7006  loss: 0.007717879489064217\n",
            " iteration: 7007  loss: 0.007718071341514587\n",
            " iteration: 7008  loss: 0.007717691827565432\n",
            " iteration: 7009  loss: 0.007717416621744633\n",
            " iteration: 7010  loss: 0.007717090658843517\n",
            " iteration: 7011  loss: 0.00771690858528018\n",
            " iteration: 7012  loss: 0.007716512773185968\n",
            " iteration: 7013  loss: 0.0077220601961016655\n",
            " iteration: 7014  loss: 0.007716411259025335\n",
            " iteration: 7015  loss: 0.0077159288339316845\n",
            " iteration: 7016  loss: 0.007715372834354639\n",
            " iteration: 7017  loss: 0.007714658044278622\n",
            " iteration: 7018  loss: 0.007713888306170702\n",
            " iteration: 7019  loss: 0.007712892256677151\n",
            " iteration: 7020  loss: 0.007711661048233509\n",
            " iteration: 7021  loss: 0.0077104102820158005\n",
            " iteration: 7022  loss: 0.007709231227636337\n",
            " iteration: 7023  loss: 0.007708000019192696\n",
            " iteration: 7024  loss: 0.007707169279456139\n",
            " iteration: 7025  loss: 0.007706404663622379\n",
            " iteration: 7026  loss: 0.007706009317189455\n",
            " iteration: 7027  loss: 0.007705518510192633\n",
            " iteration: 7028  loss: 0.007704807911068201\n",
            " iteration: 7029  loss: 0.007704166229814291\n",
            " iteration: 7030  loss: 0.007703498005867004\n",
            " iteration: 7031  loss: 0.007702909409999847\n",
            " iteration: 7032  loss: 0.0077020470052957535\n",
            " iteration: 7033  loss: 0.007700809743255377\n",
            " iteration: 7034  loss: 0.007699879817664623\n",
            " iteration: 7035  loss: 0.007698515895754099\n",
            " iteration: 7036  loss: 0.00769676873460412\n",
            " iteration: 7037  loss: 0.0076959566213190556\n",
            " iteration: 7038  loss: 0.007694522850215435\n",
            " iteration: 7039  loss: 0.007700180634856224\n",
            " iteration: 7040  loss: 0.00769422110170126\n",
            " iteration: 7041  loss: 0.00769328186288476\n",
            " iteration: 7042  loss: 0.007692862302064896\n",
            " iteration: 7043  loss: 0.007692551705986261\n",
            " iteration: 7044  loss: 0.007692322600632906\n",
            " iteration: 7045  loss: 0.007691951468586922\n",
            " iteration: 7046  loss: 0.007693462539464235\n",
            " iteration: 7047  loss: 0.00769184622913599\n",
            " iteration: 7048  loss: 0.007691503968089819\n",
            " iteration: 7049  loss: 0.007691189646720886\n",
            " iteration: 7050  loss: 0.0076906196773052216\n",
            " iteration: 7051  loss: 0.00769218523055315\n",
            " iteration: 7052  loss: 0.007690472528338432\n",
            " iteration: 7053  loss: 0.007690125610679388\n",
            " iteration: 7054  loss: 0.00768978102132678\n",
            " iteration: 7055  loss: 0.0076891169883310795\n",
            " iteration: 7056  loss: 0.007687881588935852\n",
            " iteration: 7057  loss: 0.007687178906053305\n",
            " iteration: 7058  loss: 0.007685782853513956\n",
            " iteration: 7059  loss: 0.007684680167585611\n",
            " iteration: 7060  loss: 0.007683436386287212\n",
            " iteration: 7061  loss: 0.007681960705667734\n",
            " iteration: 7062  loss: 0.007680618669837713\n",
            " iteration: 7063  loss: 0.007679849863052368\n",
            " iteration: 7064  loss: 0.007679328788071871\n",
            " iteration: 7065  loss: 0.007679937873035669\n",
            " iteration: 7066  loss: 0.007678791414946318\n",
            " iteration: 7067  loss: 0.007678215857595205\n",
            " iteration: 7068  loss: 0.007677225396037102\n",
            " iteration: 7069  loss: 0.007676560431718826\n",
            " iteration: 7070  loss: 0.0076754167675971985\n",
            " iteration: 7071  loss: 0.007680036127567291\n",
            " iteration: 7072  loss: 0.007675115950405598\n",
            " iteration: 7073  loss: 0.0076743788085877895\n",
            " iteration: 7074  loss: 0.007673915475606918\n",
            " iteration: 7075  loss: 0.007673314772546291\n",
            " iteration: 7076  loss: 0.007672323379665613\n",
            " iteration: 7077  loss: 0.007671038620173931\n",
            " iteration: 7078  loss: 0.007669823244214058\n",
            " iteration: 7079  loss: 0.0076681687496602535\n",
            " iteration: 7080  loss: 0.007666824385523796\n",
            " iteration: 7081  loss: 0.007665287237614393\n",
            " iteration: 7082  loss: 0.007663213647902012\n",
            " iteration: 7083  loss: 0.00766141340136528\n",
            " iteration: 7084  loss: 0.007660390809178352\n",
            " iteration: 7085  loss: 0.007659796625375748\n",
            " iteration: 7086  loss: 0.007658969145268202\n",
            " iteration: 7087  loss: 0.0076574115082621574\n",
            " iteration: 7088  loss: 0.007657065521925688\n",
            " iteration: 7089  loss: 0.007655447814613581\n",
            " iteration: 7090  loss: 0.007654811255633831\n",
            " iteration: 7091  loss: 0.007653944194316864\n",
            " iteration: 7092  loss: 0.007653096225112677\n",
            " iteration: 7093  loss: 0.007651461288332939\n",
            " iteration: 7094  loss: 0.007649845443665981\n",
            " iteration: 7095  loss: 0.00765120517462492\n",
            " iteration: 7096  loss: 0.00764891691505909\n",
            " iteration: 7097  loss: 0.007648474536836147\n",
            " iteration: 7098  loss: 0.007647127378731966\n",
            " iteration: 7099  loss: 0.007646902464330196\n",
            " iteration: 7100  loss: 0.007646738085895777\n",
            " iteration: 7101  loss: 0.007646405138075352\n",
            " iteration: 7102  loss: 0.007648099679499865\n",
            " iteration: 7103  loss: 0.007646058686077595\n",
            " iteration: 7104  loss: 0.007645442150533199\n",
            " iteration: 7105  loss: 0.007644521072506905\n",
            " iteration: 7106  loss: 0.007643795106559992\n",
            " iteration: 7107  loss: 0.007643361575901508\n",
            " iteration: 7108  loss: 0.007642840966582298\n",
            " iteration: 7109  loss: 0.0076456861570477486\n",
            " iteration: 7110  loss: 0.007642574608325958\n",
            " iteration: 7111  loss: 0.007642114534974098\n",
            " iteration: 7112  loss: 0.007641365751624107\n",
            " iteration: 7113  loss: 0.007640434429049492\n",
            " iteration: 7114  loss: 0.0076398178935050964\n",
            " iteration: 7115  loss: 0.007639024872332811\n",
            " iteration: 7116  loss: 0.007638538256287575\n",
            " iteration: 7117  loss: 0.007637924514710903\n",
            " iteration: 7118  loss: 0.0076374756172299385\n",
            " iteration: 7119  loss: 0.007636338006705046\n",
            " iteration: 7120  loss: 0.007635266520082951\n",
            " iteration: 7121  loss: 0.0076343403197824955\n",
            " iteration: 7122  loss: 0.007633883506059647\n",
            " iteration: 7123  loss: 0.007633598987013102\n",
            " iteration: 7124  loss: 0.007633267901837826\n",
            " iteration: 7125  loss: 0.007632756605744362\n",
            " iteration: 7126  loss: 0.007631925866007805\n",
            " iteration: 7127  loss: 0.007654781453311443\n",
            " iteration: 7128  loss: 0.007631710264831781\n",
            " iteration: 7129  loss: 0.007631149608641863\n",
            " iteration: 7130  loss: 0.007630391977727413\n",
            " iteration: 7131  loss: 0.0076296827755868435\n",
            " iteration: 7132  loss: 0.007628819905221462\n",
            " iteration: 7133  loss: 0.007628192193806171\n",
            " iteration: 7134  loss: 0.007627528160810471\n",
            " iteration: 7135  loss: 0.007627230137586594\n",
            " iteration: 7136  loss: 0.007626380771398544\n",
            " iteration: 7137  loss: 0.007625950966030359\n",
            " iteration: 7138  loss: 0.00762573815882206\n",
            " iteration: 7139  loss: 0.0076249404810369015\n",
            " iteration: 7140  loss: 0.007624673191457987\n",
            " iteration: 7141  loss: 0.007624178193509579\n",
            " iteration: 7142  loss: 0.007623699493706226\n",
            " iteration: 7143  loss: 0.007623265963047743\n",
            " iteration: 7144  loss: 0.007622687146067619\n",
            " iteration: 7145  loss: 0.007622173987329006\n",
            " iteration: 7146  loss: 0.007621899247169495\n",
            " iteration: 7147  loss: 0.007621733006089926\n",
            " iteration: 7148  loss: 0.0076214224100112915\n",
            " iteration: 7149  loss: 0.007620993070304394\n",
            " iteration: 7150  loss: 0.007620350923389196\n",
            " iteration: 7151  loss: 0.0076273223385214806\n",
            " iteration: 7152  loss: 0.007620209828019142\n",
            " iteration: 7153  loss: 0.007619600277394056\n",
            " iteration: 7154  loss: 0.00761911878362298\n",
            " iteration: 7155  loss: 0.00761870201677084\n",
            " iteration: 7156  loss: 0.007618374656885862\n",
            " iteration: 7157  loss: 0.007617616560310125\n",
            " iteration: 7158  loss: 0.007616986520588398\n",
            " iteration: 7159  loss: 0.0076168798841536045\n",
            " iteration: 7160  loss: 0.007616410497575998\n",
            " iteration: 7161  loss: 0.007616220507770777\n",
            " iteration: 7162  loss: 0.007615956477820873\n",
            " iteration: 7163  loss: 0.007615614682435989\n",
            " iteration: 7164  loss: 0.00761543121188879\n",
            " iteration: 7165  loss: 0.007614973001182079\n",
            " iteration: 7166  loss: 0.007614689879119396\n",
            " iteration: 7167  loss: 0.007614344824105501\n",
            " iteration: 7168  loss: 0.007615840528160334\n",
            " iteration: 7169  loss: 0.0076142833568155766\n",
            " iteration: 7170  loss: 0.007614090107381344\n",
            " iteration: 7171  loss: 0.007613781839609146\n",
            " iteration: 7172  loss: 0.0076134903356432915\n",
            " iteration: 7173  loss: 0.007613114081323147\n",
            " iteration: 7174  loss: 0.00761257391422987\n",
            " iteration: 7175  loss: 0.007612551096826792\n",
            " iteration: 7176  loss: 0.007612148765474558\n",
            " iteration: 7177  loss: 0.007611374836415052\n",
            " iteration: 7178  loss: 0.007610873784869909\n",
            " iteration: 7179  loss: 0.007610340137034655\n",
            " iteration: 7180  loss: 0.007609793450683355\n",
            " iteration: 7181  loss: 0.00760927889496088\n",
            " iteration: 7182  loss: 0.0076088495552539825\n",
            " iteration: 7183  loss: 0.007608563173562288\n",
            " iteration: 7184  loss: 0.007608313579112291\n",
            " iteration: 7185  loss: 0.007607791107147932\n",
            " iteration: 7186  loss: 0.007607371546328068\n",
            " iteration: 7187  loss: 0.007606797851622105\n",
            " iteration: 7188  loss: 0.007606920320540667\n",
            " iteration: 7189  loss: 0.007606539875268936\n",
            " iteration: 7190  loss: 0.00760617246851325\n",
            " iteration: 7191  loss: 0.007605730555951595\n",
            " iteration: 7192  loss: 0.00760516244918108\n",
            " iteration: 7193  loss: 0.007604640908539295\n",
            " iteration: 7194  loss: 0.007604178041219711\n",
            " iteration: 7195  loss: 0.007603683043271303\n",
            " iteration: 7196  loss: 0.007603545673191547\n",
            " iteration: 7197  loss: 0.007603402249515057\n",
            " iteration: 7198  loss: 0.007603309582918882\n",
            " iteration: 7199  loss: 0.007603187579661608\n",
            " iteration: 7200  loss: 0.007602984551340342\n",
            " iteration: 7201  loss: 0.007602846249938011\n",
            " iteration: 7202  loss: 0.007602587807923555\n",
            " iteration: 7203  loss: 0.007602239493280649\n",
            " iteration: 7204  loss: 0.007601858116686344\n",
            " iteration: 7205  loss: 0.0076018390245735645\n",
            " iteration: 7206  loss: 0.007601497694849968\n",
            " iteration: 7207  loss: 0.007601207587867975\n",
            " iteration: 7208  loss: 0.00760133471339941\n",
            " iteration: 7209  loss: 0.007600944023579359\n",
            " iteration: 7210  loss: 0.007600655313581228\n",
            " iteration: 7211  loss: 0.0076004681177437305\n",
            " iteration: 7212  loss: 0.00760016031563282\n",
            " iteration: 7213  loss: 0.007599952165037394\n",
            " iteration: 7214  loss: 0.007601871155202389\n",
            " iteration: 7215  loss: 0.007599762640893459\n",
            " iteration: 7216  loss: 0.007599476259201765\n",
            " iteration: 7217  loss: 0.007599188946187496\n",
            " iteration: 7218  loss: 0.007598972413688898\n",
            " iteration: 7219  loss: 0.0075986445881426334\n",
            " iteration: 7220  loss: 0.007598317228257656\n",
            " iteration: 7221  loss: 0.007598053198307753\n",
            " iteration: 7222  loss: 0.007597668562084436\n",
            " iteration: 7223  loss: 0.007597439922392368\n",
            " iteration: 7224  loss: 0.007597100455313921\n",
            " iteration: 7225  loss: 0.007599581032991409\n",
            " iteration: 7226  loss: 0.007596889976412058\n",
            " iteration: 7227  loss: 0.007596608251333237\n",
            " iteration: 7228  loss: 0.007596387527883053\n",
            " iteration: 7229  loss: 0.007596111856400967\n",
            " iteration: 7230  loss: 0.007595833390951157\n",
            " iteration: 7231  loss: 0.007595565635710955\n",
            " iteration: 7232  loss: 0.007595188450068235\n",
            " iteration: 7233  loss: 0.0075947875156998634\n",
            " iteration: 7234  loss: 0.007594164460897446\n",
            " iteration: 7235  loss: 0.007593590300530195\n",
            " iteration: 7236  loss: 0.007592435460537672\n",
            " iteration: 7237  loss: 0.007591932080686092\n",
            " iteration: 7238  loss: 0.007591352332383394\n",
            " iteration: 7239  loss: 0.007590834517031908\n",
            " iteration: 7240  loss: 0.007590643595904112\n",
            " iteration: 7241  loss: 0.007590799126774073\n",
            " iteration: 7242  loss: 0.007590519264340401\n",
            " iteration: 7243  loss: 0.007590245921164751\n",
            " iteration: 7244  loss: 0.007589839864522219\n",
            " iteration: 7245  loss: 0.007589038461446762\n",
            " iteration: 7246  loss: 0.007588432170450687\n",
            " iteration: 7247  loss: 0.007587946020066738\n",
            " iteration: 7248  loss: 0.00758717255666852\n",
            " iteration: 7249  loss: 0.007585726212710142\n",
            " iteration: 7250  loss: 0.007584679871797562\n",
            " iteration: 7251  loss: 0.007583877071738243\n",
            " iteration: 7252  loss: 0.007583307567983866\n",
            " iteration: 7253  loss: 0.007591011002659798\n",
            " iteration: 7254  loss: 0.0075831483118236065\n",
            " iteration: 7255  loss: 0.0075828139670193195\n",
            " iteration: 7256  loss: 0.007582308258861303\n",
            " iteration: 7257  loss: 0.007581957150250673\n",
            " iteration: 7258  loss: 0.007581453770399094\n",
            " iteration: 7259  loss: 0.007581103593111038\n",
            " iteration: 7260  loss: 0.007580579724162817\n",
            " iteration: 7261  loss: 0.007580175995826721\n",
            " iteration: 7262  loss: 0.007579953875392675\n",
            " iteration: 7263  loss: 0.007579712197184563\n",
            " iteration: 7264  loss: 0.007579285651445389\n",
            " iteration: 7265  loss: 0.007579457946121693\n",
            " iteration: 7266  loss: 0.007579118479043245\n",
            " iteration: 7267  loss: 0.00757890148088336\n",
            " iteration: 7268  loss: 0.00757845351472497\n",
            " iteration: 7269  loss: 0.007577940821647644\n",
            " iteration: 7270  loss: 0.007577300537377596\n",
            " iteration: 7271  loss: 0.007576583884656429\n",
            " iteration: 7272  loss: 0.007576038129627705\n",
            " iteration: 7273  loss: 0.007575297262519598\n",
            " iteration: 7274  loss: 0.007574739400297403\n",
            " iteration: 7275  loss: 0.0075743128545582294\n",
            " iteration: 7276  loss: 0.007573723793029785\n",
            " iteration: 7277  loss: 0.0075729647651314735\n",
            " iteration: 7278  loss: 0.007572283502668142\n",
            " iteration: 7279  loss: 0.007571764290332794\n",
            " iteration: 7280  loss: 0.007571310270577669\n",
            " iteration: 7281  loss: 0.007571410853415728\n",
            " iteration: 7282  loss: 0.007571122609078884\n",
            " iteration: 7283  loss: 0.007570832967758179\n",
            " iteration: 7284  loss: 0.007570620160549879\n",
            " iteration: 7285  loss: 0.007570337038487196\n",
            " iteration: 7286  loss: 0.007569989655166864\n",
            " iteration: 7287  loss: 0.007569446228444576\n",
            " iteration: 7288  loss: 0.007568885572254658\n",
            " iteration: 7289  loss: 0.007570213172584772\n",
            " iteration: 7290  loss: 0.0075685447081923485\n",
            " iteration: 7291  loss: 0.007568123750388622\n",
            " iteration: 7292  loss: 0.007567794062197208\n",
            " iteration: 7293  loss: 0.007567608263343573\n",
            " iteration: 7294  loss: 0.007567255292087793\n",
            " iteration: 7295  loss: 0.007566935382783413\n",
            " iteration: 7296  loss: 0.007568230852484703\n",
            " iteration: 7297  loss: 0.007566756568849087\n",
            " iteration: 7298  loss: 0.007566459476947784\n",
            " iteration: 7299  loss: 0.007565975654870272\n",
            " iteration: 7300  loss: 0.007565575186163187\n",
            " iteration: 7301  loss: 0.007565122097730637\n",
            " iteration: 7302  loss: 0.007565260399132967\n",
            " iteration: 7303  loss: 0.007564872968941927\n",
            " iteration: 7304  loss: 0.007564521860331297\n",
            " iteration: 7305  loss: 0.007564118131995201\n",
            " iteration: 7306  loss: 0.007563835941255093\n",
            " iteration: 7307  loss: 0.007563388906419277\n",
            " iteration: 7308  loss: 0.0075631048530340195\n",
            " iteration: 7309  loss: 0.0075627537444233894\n",
            " iteration: 7310  loss: 0.007562365848571062\n",
            " iteration: 7311  loss: 0.0075640627183020115\n",
            " iteration: 7312  loss: 0.007562257815152407\n",
            " iteration: 7313  loss: 0.007561713922768831\n",
            " iteration: 7314  loss: 0.007561264559626579\n",
            " iteration: 7315  loss: 0.0075605823658406734\n",
            " iteration: 7316  loss: 0.007559902034699917\n",
            " iteration: 7317  loss: 0.007559278514236212\n",
            " iteration: 7318  loss: 0.007562446407973766\n",
            " iteration: 7319  loss: 0.0075590466149151325\n",
            " iteration: 7320  loss: 0.00755865965038538\n",
            " iteration: 7321  loss: 0.0075583551079034805\n",
            " iteration: 7322  loss: 0.007558227051049471\n",
            " iteration: 7323  loss: 0.007557968609035015\n",
            " iteration: 7324  loss: 0.007557600270956755\n",
            " iteration: 7325  loss: 0.007557157892733812\n",
            " iteration: 7326  loss: 0.007556765340268612\n",
            " iteration: 7327  loss: 0.007556277327239513\n",
            " iteration: 7328  loss: 0.007556015159934759\n",
            " iteration: 7329  loss: 0.007555547170341015\n",
            " iteration: 7330  loss: 0.007556610740721226\n",
            " iteration: 7331  loss: 0.007555391639471054\n",
            " iteration: 7332  loss: 0.007555060554295778\n",
            " iteration: 7333  loss: 0.007554672658443451\n",
            " iteration: 7334  loss: 0.007554206997156143\n",
            " iteration: 7335  loss: 0.007553634699434042\n",
            " iteration: 7336  loss: 0.007567446678876877\n",
            " iteration: 7337  loss: 0.007553447503596544\n",
            " iteration: 7338  loss: 0.007552756927907467\n",
            " iteration: 7339  loss: 0.007552199065685272\n",
            " iteration: 7340  loss: 0.007551681715995073\n",
            " iteration: 7341  loss: 0.0075514428317546844\n",
            " iteration: 7342  loss: 0.007551356218755245\n",
            " iteration: 7343  loss: 0.007551159244030714\n",
            " iteration: 7344  loss: 0.007551097776740789\n",
            " iteration: 7345  loss: 0.007551002781838179\n",
            " iteration: 7346  loss: 0.007550811395049095\n",
            " iteration: 7347  loss: 0.007550458423793316\n",
            " iteration: 7348  loss: 0.007550222799181938\n",
            " iteration: 7349  loss: 0.0075500705279409885\n",
            " iteration: 7350  loss: 0.007549903821200132\n",
            " iteration: 7351  loss: 0.007549699861556292\n",
            " iteration: 7352  loss: 0.007549400441348553\n",
            " iteration: 7353  loss: 0.007549016736447811\n",
            " iteration: 7354  loss: 0.007548545952886343\n",
            " iteration: 7355  loss: 0.007548221852630377\n",
            " iteration: 7356  loss: 0.007547985762357712\n",
            " iteration: 7357  loss: 0.007547799032181501\n",
            " iteration: 7358  loss: 0.0075476341880857944\n",
            " iteration: 7359  loss: 0.007547853048890829\n",
            " iteration: 7360  loss: 0.007547541055828333\n",
            " iteration: 7361  loss: 0.007547416724264622\n",
            " iteration: 7362  loss: 0.007547350134700537\n",
            " iteration: 7363  loss: 0.00754718529060483\n",
            " iteration: 7364  loss: 0.007546997629106045\n",
            " iteration: 7365  loss: 0.007546702399849892\n",
            " iteration: 7366  loss: 0.00754800159484148\n",
            " iteration: 7367  loss: 0.007546363864094019\n",
            " iteration: 7368  loss: 0.0075460257939994335\n",
            " iteration: 7369  loss: 0.007545861415565014\n",
            " iteration: 7370  loss: 0.00754592614248395\n",
            " iteration: 7371  loss: 0.0075456672348082066\n",
            " iteration: 7372  loss: 0.0075455764308571815\n",
            " iteration: 7373  loss: 0.007545230910181999\n",
            " iteration: 7374  loss: 0.007544883992522955\n",
            " iteration: 7375  loss: 0.0075454371981322765\n",
            " iteration: 7376  loss: 0.007544822990894318\n",
            " iteration: 7377  loss: 0.007544631138443947\n",
            " iteration: 7378  loss: 0.007544498424977064\n",
            " iteration: 7379  loss: 0.007544326595962048\n",
            " iteration: 7380  loss: 0.007544107269495726\n",
            " iteration: 7381  loss: 0.007543720304965973\n",
            " iteration: 7382  loss: 0.00754576176404953\n",
            " iteration: 7383  loss: 0.007543652318418026\n",
            " iteration: 7384  loss: 0.007543249521404505\n",
            " iteration: 7385  loss: 0.007542798295617104\n",
            " iteration: 7386  loss: 0.007542298641055822\n",
            " iteration: 7387  loss: 0.00754175242036581\n",
            " iteration: 7388  loss: 0.007541481405496597\n",
            " iteration: 7389  loss: 0.007540974300354719\n",
            " iteration: 7390  loss: 0.007540709804743528\n",
            " iteration: 7391  loss: 0.0075402758084237576\n",
            " iteration: 7392  loss: 0.007539754267781973\n",
            " iteration: 7393  loss: 0.007539279758930206\n",
            " iteration: 7394  loss: 0.00754139618948102\n",
            " iteration: 7395  loss: 0.00753899896517396\n",
            " iteration: 7396  loss: 0.0075386096723377705\n",
            " iteration: 7397  loss: 0.007537794765084982\n",
            " iteration: 7398  loss: 0.0075369528494775295\n",
            " iteration: 7399  loss: 0.007536130491644144\n",
            " iteration: 7400  loss: 0.0075356545858085155\n",
            " iteration: 7401  loss: 0.00753544457256794\n",
            " iteration: 7402  loss: 0.007535272743552923\n",
            " iteration: 7403  loss: 0.007535077165812254\n",
            " iteration: 7404  loss: 0.00753482012078166\n",
            " iteration: 7405  loss: 0.007534472271800041\n",
            " iteration: 7406  loss: 0.007534344680607319\n",
            " iteration: 7407  loss: 0.00753398472443223\n",
            " iteration: 7408  loss: 0.007533773314207792\n",
            " iteration: 7409  loss: 0.007533415220677853\n",
            " iteration: 7410  loss: 0.007532963994890451\n",
            " iteration: 7411  loss: 0.007532407995313406\n",
            " iteration: 7412  loss: 0.007531857583671808\n",
            " iteration: 7413  loss: 0.007531540468335152\n",
            " iteration: 7414  loss: 0.007531290408223867\n",
            " iteration: 7415  loss: 0.007530894596129656\n",
            " iteration: 7416  loss: 0.007530753966420889\n",
            " iteration: 7417  loss: 0.007530492264777422\n",
            " iteration: 7418  loss: 0.007530436851084232\n",
            " iteration: 7419  loss: 0.007530264090746641\n",
            " iteration: 7420  loss: 0.007530000060796738\n",
            " iteration: 7421  loss: 0.007529703434556723\n",
            " iteration: 7422  loss: 0.007529143709689379\n",
            " iteration: 7423  loss: 0.00752851739525795\n",
            " iteration: 7424  loss: 0.0075282263569533825\n",
            " iteration: 7425  loss: 0.00752787571400404\n",
            " iteration: 7426  loss: 0.007527348585426807\n",
            " iteration: 7427  loss: 0.007527098059654236\n",
            " iteration: 7428  loss: 0.007526756264269352\n",
            " iteration: 7429  loss: 0.007526533678174019\n",
            " iteration: 7430  loss: 0.00752642285078764\n",
            " iteration: 7431  loss: 0.007526210509240627\n",
            " iteration: 7432  loss: 0.007526402827352285\n",
            " iteration: 7433  loss: 0.007526072673499584\n",
            " iteration: 7434  loss: 0.007525910157710314\n",
            " iteration: 7435  loss: 0.007525770924985409\n",
            " iteration: 7436  loss: 0.007525648456066847\n",
            " iteration: 7437  loss: 0.0075254663825035095\n",
            " iteration: 7438  loss: 0.007525225169956684\n",
            " iteration: 7439  loss: 0.0075250184163451195\n",
            " iteration: 7440  loss: 0.007524709217250347\n",
            " iteration: 7441  loss: 0.007524592336267233\n",
            " iteration: 7442  loss: 0.00752414483577013\n",
            " iteration: 7443  loss: 0.007523831911385059\n",
            " iteration: 7444  loss: 0.007523512002080679\n",
            " iteration: 7445  loss: 0.007523202337324619\n",
            " iteration: 7446  loss: 0.007523049134761095\n",
            " iteration: 7447  loss: 0.007522924803197384\n",
            " iteration: 7448  loss: 0.0075226882472634315\n",
            " iteration: 7449  loss: 0.007522393483668566\n",
            " iteration: 7450  loss: 0.007522157393395901\n",
            " iteration: 7451  loss: 0.0075218030251562595\n",
            " iteration: 7452  loss: 0.007521404884755611\n",
            " iteration: 7453  loss: 0.007520919665694237\n",
            " iteration: 7454  loss: 0.007520685438066721\n",
            " iteration: 7455  loss: 0.00752036739140749\n",
            " iteration: 7456  loss: 0.007520216051489115\n",
            " iteration: 7457  loss: 0.007520001847296953\n",
            " iteration: 7458  loss: 0.007519803941249847\n",
            " iteration: 7459  loss: 0.007519620005041361\n",
            " iteration: 7460  loss: 0.007519450969994068\n",
            " iteration: 7461  loss: 0.007519224192947149\n",
            " iteration: 7462  loss: 0.00751905795186758\n",
            " iteration: 7463  loss: 0.007518792990595102\n",
            " iteration: 7464  loss: 0.007518768310546875\n",
            " iteration: 7465  loss: 0.007518398109823465\n",
            " iteration: 7466  loss: 0.007518219295889139\n",
            " iteration: 7467  loss: 0.0075180078856647015\n",
            " iteration: 7468  loss: 0.0075178188271820545\n",
            " iteration: 7469  loss: 0.007518100552260876\n",
            " iteration: 7470  loss: 0.00751775735989213\n",
            " iteration: 7471  loss: 0.007517626509070396\n",
            " iteration: 7472  loss: 0.007517320569604635\n",
            " iteration: 7473  loss: 0.007516857236623764\n",
            " iteration: 7474  loss: 0.007516146637499332\n",
            " iteration: 7475  loss: 0.007515390403568745\n",
            " iteration: 7476  loss: 0.007514542434364557\n",
            " iteration: 7477  loss: 0.0075139799155294895\n",
            " iteration: 7478  loss: 0.007513581775128841\n",
            " iteration: 7479  loss: 0.007513225078582764\n",
            " iteration: 7480  loss: 0.007512856740504503\n",
            " iteration: 7481  loss: 0.007512374315410852\n",
            " iteration: 7482  loss: 0.007515245582908392\n",
            " iteration: 7483  loss: 0.00751229515299201\n",
            " iteration: 7484  loss: 0.007512015290558338\n",
            " iteration: 7485  loss: 0.007511848118156195\n",
            " iteration: 7486  loss: 0.007511747535318136\n",
            " iteration: 7487  loss: 0.007511567790061235\n",
            " iteration: 7488  loss: 0.007511284202337265\n",
            " iteration: 7489  loss: 0.007510947994887829\n",
            " iteration: 7490  loss: 0.007510660216212273\n",
            " iteration: 7491  loss: 0.007510300260037184\n",
            " iteration: 7492  loss: 0.007509969640523195\n",
            " iteration: 7493  loss: 0.007509395945817232\n",
            " iteration: 7494  loss: 0.0075165280140936375\n",
            " iteration: 7495  loss: 0.007509344257414341\n",
            " iteration: 7496  loss: 0.007509126793593168\n",
            " iteration: 7497  loss: 0.00750888604670763\n",
            " iteration: 7498  loss: 0.007508541457355022\n",
            " iteration: 7499  loss: 0.007508256006985903\n",
            " iteration: 7500  loss: 0.007507973816245794\n",
            " iteration: 7501  loss: 0.0075076790526509285\n",
            " iteration: 7502  loss: 0.007507421541959047\n",
            " iteration: 7503  loss: 0.007507155649363995\n",
            " iteration: 7504  loss: 0.00750676030293107\n",
            " iteration: 7505  loss: 0.007506589870899916\n",
            " iteration: 7506  loss: 0.007506273686885834\n",
            " iteration: 7507  loss: 0.007505853194743395\n",
            " iteration: 7508  loss: 0.007505166810005903\n",
            " iteration: 7509  loss: 0.007508466485887766\n",
            " iteration: 7510  loss: 0.007504964247345924\n",
            " iteration: 7511  loss: 0.007504521869122982\n",
            " iteration: 7512  loss: 0.007504312787204981\n",
            " iteration: 7513  loss: 0.00750407250598073\n",
            " iteration: 7514  loss: 0.007503827568143606\n",
            " iteration: 7515  loss: 0.007503630593419075\n",
            " iteration: 7516  loss: 0.007503497879952192\n",
            " iteration: 7517  loss: 0.007503357715904713\n",
            " iteration: 7518  loss: 0.007502920925617218\n",
            " iteration: 7519  loss: 0.007502429652959108\n",
            " iteration: 7520  loss: 0.007504258304834366\n",
            " iteration: 7521  loss: 0.007502275984734297\n",
            " iteration: 7522  loss: 0.0075019970536231995\n",
            " iteration: 7523  loss: 0.00750184990465641\n",
            " iteration: 7524  loss: 0.0075017549097537994\n",
            " iteration: 7525  loss: 0.007501612883061171\n",
            " iteration: 7526  loss: 0.007501498330384493\n",
            " iteration: 7527  loss: 0.007501192390918732\n",
            " iteration: 7528  loss: 0.007501010317355394\n",
            " iteration: 7529  loss: 0.007500640582293272\n",
            " iteration: 7530  loss: 0.007500261999666691\n",
            " iteration: 7531  loss: 0.007499878294765949\n",
            " iteration: 7532  loss: 0.007499476429075003\n",
            " iteration: 7533  loss: 0.007498915307223797\n",
            " iteration: 7534  loss: 0.0074979402124881744\n",
            " iteration: 7535  loss: 0.007501226384192705\n",
            " iteration: 7536  loss: 0.007497740909457207\n",
            " iteration: 7537  loss: 0.007497518323361874\n",
            " iteration: 7538  loss: 0.007497192360460758\n",
            " iteration: 7539  loss: 0.007496857084333897\n",
            " iteration: 7540  loss: 0.007499063853174448\n",
            " iteration: 7541  loss: 0.007496722042560577\n",
            " iteration: 7542  loss: 0.0074962228536605835\n",
            " iteration: 7543  loss: 0.007495764642953873\n",
            " iteration: 7544  loss: 0.007495268248021603\n",
            " iteration: 7545  loss: 0.007494809105992317\n",
            " iteration: 7546  loss: 0.00749436067417264\n",
            " iteration: 7547  loss: 0.007494137156754732\n",
            " iteration: 7548  loss: 0.007493719458580017\n",
            " iteration: 7549  loss: 0.007493145763874054\n",
            " iteration: 7550  loss: 0.007493100594729185\n",
            " iteration: 7551  loss: 0.0074927653186023235\n",
            " iteration: 7552  loss: 0.007491894066333771\n",
            " iteration: 7553  loss: 0.00749120581895113\n",
            " iteration: 7554  loss: 0.007490695454180241\n",
            " iteration: 7555  loss: 0.007489925716072321\n",
            " iteration: 7556  loss: 0.007489305455237627\n",
            " iteration: 7557  loss: 0.007488966453820467\n",
            " iteration: 7558  loss: 0.00748842628672719\n",
            " iteration: 7559  loss: 0.007488096598535776\n",
            " iteration: 7560  loss: 0.007487977389246225\n",
            " iteration: 7561  loss: 0.00748772406950593\n",
            " iteration: 7562  loss: 0.007487304043024778\n",
            " iteration: 7563  loss: 0.007487013470381498\n",
            " iteration: 7564  loss: 0.007486565969884396\n",
            " iteration: 7565  loss: 0.007486100308597088\n",
            " iteration: 7566  loss: 0.0074881333857774734\n",
            " iteration: 7567  loss: 0.00748576782643795\n",
            " iteration: 7568  loss: 0.007485005538910627\n",
            " iteration: 7569  loss: 0.007484293542802334\n",
            " iteration: 7570  loss: 0.007483503315597773\n",
            " iteration: 7571  loss: 0.00748308515176177\n",
            " iteration: 7572  loss: 0.007482575252652168\n",
            " iteration: 7573  loss: 0.007482217624783516\n",
            " iteration: 7574  loss: 0.007481999229639769\n",
            " iteration: 7575  loss: 0.007481724955141544\n",
            " iteration: 7576  loss: 0.007481235545128584\n",
            " iteration: 7577  loss: 0.007480384781956673\n",
            " iteration: 7578  loss: 0.007481681648641825\n",
            " iteration: 7579  loss: 0.007479697931557894\n",
            " iteration: 7580  loss: 0.007479068357497454\n",
            " iteration: 7581  loss: 0.007479466497898102\n",
            " iteration: 7582  loss: 0.007478914223611355\n",
            " iteration: 7583  loss: 0.00747867813333869\n",
            " iteration: 7584  loss: 0.007478589657694101\n",
            " iteration: 7585  loss: 0.007477912120521069\n",
            " iteration: 7586  loss: 0.007477235980331898\n",
            " iteration: 7587  loss: 0.007477029226720333\n",
            " iteration: 7588  loss: 0.007476367987692356\n",
            " iteration: 7589  loss: 0.007475910242646933\n",
            " iteration: 7590  loss: 0.0074753835797309875\n",
            " iteration: 7591  loss: 0.007474854122847319\n",
            " iteration: 7592  loss: 0.007474552374333143\n",
            " iteration: 7593  loss: 0.007474320009350777\n",
            " iteration: 7594  loss: 0.0074739838019013405\n",
            " iteration: 7595  loss: 0.00747324712574482\n",
            " iteration: 7596  loss: 0.007473709527403116\n",
            " iteration: 7597  loss: 0.007472716271877289\n",
            " iteration: 7598  loss: 0.007471346762031317\n",
            " iteration: 7599  loss: 0.007470162119716406\n",
            " iteration: 7600  loss: 0.0074695381335914135\n",
            " iteration: 7601  loss: 0.007469187956303358\n",
            " iteration: 7602  loss: 0.007468990981578827\n",
            " iteration: 7603  loss: 0.007468867115676403\n",
            " iteration: 7604  loss: 0.007468430325388908\n",
            " iteration: 7605  loss: 0.007467884104698896\n",
            " iteration: 7606  loss: 0.007467588409781456\n",
            " iteration: 7607  loss: 0.007467107847332954\n",
            " iteration: 7608  loss: 0.007466830313205719\n",
            " iteration: 7609  loss: 0.0074666826985776424\n",
            " iteration: 7610  loss: 0.007466461509466171\n",
            " iteration: 7611  loss: 0.007467166520655155\n",
            " iteration: 7612  loss: 0.007466232869774103\n",
            " iteration: 7613  loss: 0.007465595379471779\n",
            " iteration: 7614  loss: 0.007465049624443054\n",
            " iteration: 7615  loss: 0.007464539725333452\n",
            " iteration: 7616  loss: 0.00746424775570631\n",
            " iteration: 7617  loss: 0.007464009337127209\n",
            " iteration: 7618  loss: 0.007463781628757715\n",
            " iteration: 7619  loss: 0.007463595364242792\n",
            " iteration: 7620  loss: 0.007463283371180296\n",
            " iteration: 7621  loss: 0.007462600711733103\n",
            " iteration: 7622  loss: 0.0074622901156544685\n",
            " iteration: 7623  loss: 0.007461478002369404\n",
            " iteration: 7624  loss: 0.00746105657890439\n",
            " iteration: 7625  loss: 0.007460433058440685\n",
            " iteration: 7626  loss: 0.007459792774170637\n",
            " iteration: 7627  loss: 0.007459212094545364\n",
            " iteration: 7628  loss: 0.0074588097631931305\n",
            " iteration: 7629  loss: 0.007458471693098545\n",
            " iteration: 7630  loss: 0.007458249107003212\n",
            " iteration: 7631  loss: 0.007457575295120478\n",
            " iteration: 7632  loss: 0.007456919178366661\n",
            " iteration: 7633  loss: 0.007456111256033182\n",
            " iteration: 7634  loss: 0.007456155493855476\n",
            " iteration: 7635  loss: 0.007455755956470966\n",
            " iteration: 7636  loss: 0.007474396377801895\n",
            " iteration: 7637  loss: 0.0074556320905685425\n",
            " iteration: 7638  loss: 0.007455292623490095\n",
            " iteration: 7639  loss: 0.007454602047801018\n",
            " iteration: 7640  loss: 0.007454110309481621\n",
            " iteration: 7641  loss: 0.007453455124050379\n",
            " iteration: 7642  loss: 0.007453251630067825\n",
            " iteration: 7643  loss: 0.007452856749296188\n",
            " iteration: 7644  loss: 0.007452705875039101\n",
            " iteration: 7645  loss: 0.007452443242073059\n",
            " iteration: 7646  loss: 0.007452117279171944\n",
            " iteration: 7647  loss: 0.00745132751762867\n",
            " iteration: 7648  loss: 0.007450425066053867\n",
            " iteration: 7649  loss: 0.007449933793395758\n",
            " iteration: 7650  loss: 0.007449534721672535\n",
            " iteration: 7651  loss: 0.007448954973369837\n",
            " iteration: 7652  loss: 0.007448031101375818\n",
            " iteration: 7653  loss: 0.007447674870491028\n",
            " iteration: 7654  loss: 0.007446943782269955\n",
            " iteration: 7655  loss: 0.007446537259966135\n",
            " iteration: 7656  loss: 0.007446130737662315\n",
            " iteration: 7657  loss: 0.007445862516760826\n",
            " iteration: 7658  loss: 0.007445475086569786\n",
            " iteration: 7659  loss: 0.007445165887475014\n",
            " iteration: 7660  loss: 0.007444730028510094\n",
            " iteration: 7661  loss: 0.007444459944963455\n",
            " iteration: 7662  loss: 0.0074437144212424755\n",
            " iteration: 7663  loss: 0.007443389389663935\n",
            " iteration: 7664  loss: 0.007442721165716648\n",
            " iteration: 7665  loss: 0.00744217773899436\n",
            " iteration: 7666  loss: 0.0074414717964828014\n",
            " iteration: 7667  loss: 0.007440901827067137\n",
            " iteration: 7668  loss: 0.007440455723553896\n",
            " iteration: 7669  loss: 0.007439878769218922\n",
            " iteration: 7670  loss: 0.0074393958784639835\n",
            " iteration: 7671  loss: 0.007438868284225464\n",
            " iteration: 7672  loss: 0.007438342086970806\n",
            " iteration: 7673  loss: 0.007440747693181038\n",
            " iteration: 7674  loss: 0.007438180968165398\n",
            " iteration: 7675  loss: 0.007437816821038723\n",
            " iteration: 7676  loss: 0.007436978165060282\n",
            " iteration: 7677  loss: 0.0074366433545947075\n",
            " iteration: 7678  loss: 0.007436355575919151\n",
            " iteration: 7679  loss: 0.007436017040163279\n",
            " iteration: 7680  loss: 0.0074355993419885635\n",
            " iteration: 7681  loss: 0.00743919238448143\n",
            " iteration: 7682  loss: 0.007435379084199667\n",
            " iteration: 7683  loss: 0.007434916216880083\n",
            " iteration: 7684  loss: 0.00743461400270462\n",
            " iteration: 7685  loss: 0.007434316910803318\n",
            " iteration: 7686  loss: 0.00743396021425724\n",
            " iteration: 7687  loss: 0.007433673366904259\n",
            " iteration: 7688  loss: 0.007432918529957533\n",
            " iteration: 7689  loss: 0.0074322219006717205\n",
            " iteration: 7690  loss: 0.007431356236338615\n",
            " iteration: 7691  loss: 0.0074320645071566105\n",
            " iteration: 7692  loss: 0.007431139703840017\n",
            " iteration: 7693  loss: 0.007430545054376125\n",
            " iteration: 7694  loss: 0.0074302987195551395\n",
            " iteration: 7695  loss: 0.007430050056427717\n",
            " iteration: 7696  loss: 0.007429908495396376\n",
            " iteration: 7697  loss: 0.0074295466765761375\n",
            " iteration: 7698  loss: 0.007429453544318676\n",
            " iteration: 7699  loss: 0.007428745273500681\n",
            " iteration: 7700  loss: 0.007428447715938091\n",
            " iteration: 7701  loss: 0.007428111042827368\n",
            " iteration: 7702  loss: 0.007427701260894537\n",
            " iteration: 7703  loss: 0.007427044678479433\n",
            " iteration: 7704  loss: 0.007426081225275993\n",
            " iteration: 7705  loss: 0.007425212301313877\n",
            " iteration: 7706  loss: 0.007424105424433947\n",
            " iteration: 7707  loss: 0.007423213683068752\n",
            " iteration: 7708  loss: 0.007422661874443293\n",
            " iteration: 7709  loss: 0.007422026712447405\n",
            " iteration: 7710  loss: 0.0074215480126440525\n",
            " iteration: 7711  loss: 0.007421294692903757\n",
            " iteration: 7712  loss: 0.007421097718179226\n",
            " iteration: 7713  loss: 0.007420917507261038\n",
            " iteration: 7714  loss: 0.007420478854328394\n",
            " iteration: 7715  loss: 0.0074202194809913635\n",
            " iteration: 7716  loss: 0.007419492118060589\n",
            " iteration: 7717  loss: 0.007418948691338301\n",
            " iteration: 7718  loss: 0.00741845415905118\n",
            " iteration: 7719  loss: 0.007418608758598566\n",
            " iteration: 7720  loss: 0.007418069988489151\n",
            " iteration: 7721  loss: 0.007417576853185892\n",
            " iteration: 7722  loss: 0.007416696287691593\n",
            " iteration: 7723  loss: 0.007416281849145889\n",
            " iteration: 7724  loss: 0.00741548603400588\n",
            " iteration: 7725  loss: 0.007415414787828922\n",
            " iteration: 7726  loss: 0.007414818275719881\n",
            " iteration: 7727  loss: 0.0074144951067864895\n",
            " iteration: 7728  loss: 0.007414016406983137\n",
            " iteration: 7729  loss: 0.0074135782197117805\n",
            " iteration: 7730  loss: 0.007413666695356369\n",
            " iteration: 7731  loss: 0.007413284853100777\n",
            " iteration: 7732  loss: 0.007412697654217482\n",
            " iteration: 7733  loss: 0.007412286009639502\n",
            " iteration: 7734  loss: 0.007411790080368519\n",
            " iteration: 7735  loss: 0.007411429658532143\n",
            " iteration: 7736  loss: 0.007411215920001268\n",
            " iteration: 7737  loss: 0.007411141414195299\n",
            " iteration: 7738  loss: 0.007410751655697823\n",
            " iteration: 7739  loss: 0.007410589139908552\n",
            " iteration: 7740  loss: 0.007410459220409393\n",
            " iteration: 7741  loss: 0.007410310208797455\n",
            " iteration: 7742  loss: 0.007409979589283466\n",
            " iteration: 7743  loss: 0.0074096182361245155\n",
            " iteration: 7744  loss: 0.007409147452563047\n",
            " iteration: 7745  loss: 0.007408964913338423\n",
            " iteration: 7746  loss: 0.007408664561808109\n",
            " iteration: 7747  loss: 0.007408554665744305\n",
            " iteration: 7748  loss: 0.007408405188471079\n",
            " iteration: 7749  loss: 0.007408000994473696\n",
            " iteration: 7750  loss: 0.007410008460283279\n",
            " iteration: 7751  loss: 0.007407873868942261\n",
            " iteration: 7752  loss: 0.007407344877719879\n",
            " iteration: 7753  loss: 0.007407087367027998\n",
            " iteration: 7754  loss: 0.007406853139400482\n",
            " iteration: 7755  loss: 0.007406357675790787\n",
            " iteration: 7756  loss: 0.007406031247228384\n",
            " iteration: 7757  loss: 0.0074057322926819324\n",
            " iteration: 7758  loss: 0.007405447773635387\n",
            " iteration: 7759  loss: 0.007405032869428396\n",
            " iteration: 7760  loss: 0.007404177915304899\n",
            " iteration: 7761  loss: 0.007403266150504351\n",
            " iteration: 7762  loss: 0.0074025364592671394\n",
            " iteration: 7763  loss: 0.007402242161333561\n",
            " iteration: 7764  loss: 0.007404339034110308\n",
            " iteration: 7765  loss: 0.007402130868285894\n",
            " iteration: 7766  loss: 0.007401961833238602\n",
            " iteration: 7767  loss: 0.007401847280561924\n",
            " iteration: 7768  loss: 0.007401720155030489\n",
            " iteration: 7769  loss: 0.007401646114885807\n",
            " iteration: 7770  loss: 0.007401498965919018\n",
            " iteration: 7771  loss: 0.007401199545711279\n",
            " iteration: 7772  loss: 0.0074007255025208\n",
            " iteration: 7773  loss: 0.007400921080261469\n",
            " iteration: 7774  loss: 0.00740046426653862\n",
            " iteration: 7775  loss: 0.007399992551654577\n",
            " iteration: 7776  loss: 0.007399612106382847\n",
            " iteration: 7777  loss: 0.007399435620754957\n",
            " iteration: 7778  loss: 0.007399138528853655\n",
            " iteration: 7779  loss: 0.0073987883515655994\n",
            " iteration: 7780  loss: 0.0073983147740364075\n",
            " iteration: 7781  loss: 0.007397832348942757\n",
            " iteration: 7782  loss: 0.007397662382572889\n",
            " iteration: 7783  loss: 0.00739698763936758\n",
            " iteration: 7784  loss: 0.0073966169729828835\n",
            " iteration: 7785  loss: 0.007396261673420668\n",
            " iteration: 7786  loss: 0.007395848631858826\n",
            " iteration: 7787  loss: 0.007395281456410885\n",
            " iteration: 7788  loss: 0.00739474268630147\n",
            " iteration: 7789  loss: 0.007394826505333185\n",
            " iteration: 7790  loss: 0.007394252810627222\n",
            " iteration: 7791  loss: 0.007393781095743179\n",
            " iteration: 7792  loss: 0.0073929536156356335\n",
            " iteration: 7793  loss: 0.007392672821879387\n",
            " iteration: 7794  loss: 0.007395818829536438\n",
            " iteration: 7795  loss: 0.007392601575702429\n",
            " iteration: 7796  loss: 0.007392432540655136\n",
            " iteration: 7797  loss: 0.007392094004899263\n",
            " iteration: 7798  loss: 0.0073915161192417145\n",
            " iteration: 7799  loss: 0.007391097489744425\n",
            " iteration: 7800  loss: 0.007390783168375492\n",
            " iteration: 7801  loss: 0.00739060016348958\n",
            " iteration: 7802  loss: 0.007390416692942381\n",
            " iteration: 7803  loss: 0.0073902723379433155\n",
            " iteration: 7804  loss: 0.007390095852315426\n",
            " iteration: 7805  loss: 0.0073899561539292336\n",
            " iteration: 7806  loss: 0.0073896548710763454\n",
            " iteration: 7807  loss: 0.007389424368739128\n",
            " iteration: 7808  loss: 0.007389334961771965\n",
            " iteration: 7809  loss: 0.00738915940746665\n",
            " iteration: 7810  loss: 0.007389027159661055\n",
            " iteration: 7811  loss: 0.007388886529952288\n",
            " iteration: 7812  loss: 0.007388601079583168\n",
            " iteration: 7813  loss: 0.007388112135231495\n",
            " iteration: 7814  loss: 0.00738800223916769\n",
            " iteration: 7815  loss: 0.007388006895780563\n",
            " iteration: 7816  loss: 0.007387635298073292\n",
            " iteration: 7817  loss: 0.007387129589915276\n",
            " iteration: 7818  loss: 0.0073868585750460625\n",
            " iteration: 7819  loss: 0.007386521436274052\n",
            " iteration: 7820  loss: 0.007386528421193361\n",
            " iteration: 7821  loss: 0.007386319804936647\n",
            " iteration: 7822  loss: 0.007385767996311188\n",
            " iteration: 7823  loss: 0.007385376375168562\n",
            " iteration: 7824  loss: 0.007385135628283024\n",
            " iteration: 7825  loss: 0.007385279983282089\n",
            " iteration: 7826  loss: 0.0073850383050739765\n",
            " iteration: 7827  loss: 0.0073849824257195\n",
            " iteration: 7828  loss: 0.007384767755866051\n",
            " iteration: 7829  loss: 0.007384527008980513\n",
            " iteration: 7830  loss: 0.0073838974349200726\n",
            " iteration: 7831  loss: 0.007382974494248629\n",
            " iteration: 7832  loss: 0.007382429204881191\n",
            " iteration: 7833  loss: 0.00738306762650609\n",
            " iteration: 7834  loss: 0.007381924893707037\n",
            " iteration: 7835  loss: 0.007381305564194918\n",
            " iteration: 7836  loss: 0.007380912080407143\n",
            " iteration: 7837  loss: 0.007380428723990917\n",
            " iteration: 7838  loss: 0.007379914168268442\n",
            " iteration: 7839  loss: 0.007379446644335985\n",
            " iteration: 7840  loss: 0.007379211485385895\n",
            " iteration: 7841  loss: 0.007378925569355488\n",
            " iteration: 7842  loss: 0.007378603797405958\n",
            " iteration: 7843  loss: 0.007378146052360535\n",
            " iteration: 7844  loss: 0.00737761938944459\n",
            " iteration: 7845  loss: 0.007377915550023317\n",
            " iteration: 7846  loss: 0.007377338595688343\n",
            " iteration: 7847  loss: 0.0073770941235125065\n",
            " iteration: 7848  loss: 0.007376768626272678\n",
            " iteration: 7849  loss: 0.00737627362832427\n",
            " iteration: 7850  loss: 0.007376064080744982\n",
            " iteration: 7851  loss: 0.007375653367489576\n",
            " iteration: 7852  loss: 0.007375312969088554\n",
            " iteration: 7853  loss: 0.007375007960945368\n",
            " iteration: 7854  loss: 0.007375759072601795\n",
            " iteration: 7855  loss: 0.007374885026365519\n",
            " iteration: 7856  loss: 0.007374581415206194\n",
            " iteration: 7857  loss: 0.007374327629804611\n",
            " iteration: 7858  loss: 0.007374104578047991\n",
            " iteration: 7859  loss: 0.007373938802629709\n",
            " iteration: 7860  loss: 0.007373634725809097\n",
            " iteration: 7861  loss: 0.007373383734375238\n",
            " iteration: 7862  loss: 0.007373178377747536\n",
            " iteration: 7863  loss: 0.0073729269206523895\n",
            " iteration: 7864  loss: 0.007372613996267319\n",
            " iteration: 7865  loss: 0.007371919229626656\n",
            " iteration: 7866  loss: 0.007371439132839441\n",
            " iteration: 7867  loss: 0.007372147403657436\n",
            " iteration: 7868  loss: 0.007371280808001757\n",
            " iteration: 7869  loss: 0.007371058221906424\n",
            " iteration: 7870  loss: 0.0073708901181817055\n",
            " iteration: 7871  loss: 0.007370661944150925\n",
            " iteration: 7872  loss: 0.007370129227638245\n",
            " iteration: 7873  loss: 0.007369800005108118\n",
            " iteration: 7874  loss: 0.007369340397417545\n",
            " iteration: 7875  loss: 0.00736896600574255\n",
            " iteration: 7876  loss: 0.007368666119873524\n",
            " iteration: 7877  loss: 0.007368328049778938\n",
            " iteration: 7878  loss: 0.0073679471388459206\n",
            " iteration: 7879  loss: 0.007367494981735945\n",
            " iteration: 7880  loss: 0.007367142476141453\n",
            " iteration: 7881  loss: 0.007366661913692951\n",
            " iteration: 7882  loss: 0.007366376928985119\n",
            " iteration: 7883  loss: 0.0073659177869558334\n",
            " iteration: 7884  loss: 0.007365520112216473\n",
            " iteration: 7885  loss: 0.007365059107542038\n",
            " iteration: 7886  loss: 0.0073647270910441875\n",
            " iteration: 7887  loss: 0.007364122197031975\n",
            " iteration: 7888  loss: 0.007363931275904179\n",
            " iteration: 7889  loss: 0.007363665848970413\n",
            " iteration: 7890  loss: 0.007363251410424709\n",
            " iteration: 7891  loss: 0.007362850476056337\n",
            " iteration: 7892  loss: 0.007362627889961004\n",
            " iteration: 7893  loss: 0.007361988071352243\n",
            " iteration: 7894  loss: 0.007361793890595436\n",
            " iteration: 7895  loss: 0.007361599709838629\n",
            " iteration: 7896  loss: 0.0073617249727249146\n",
            " iteration: 7897  loss: 0.007361465599387884\n",
            " iteration: 7898  loss: 0.00736130541190505\n",
            " iteration: 7899  loss: 0.007360983174294233\n",
            " iteration: 7900  loss: 0.007360679097473621\n",
            " iteration: 7901  loss: 0.007360577583312988\n",
            " iteration: 7902  loss: 0.007359973154962063\n",
            " iteration: 7903  loss: 0.007359805982559919\n",
            " iteration: 7904  loss: 0.007359636016190052\n",
            " iteration: 7905  loss: 0.007359400391578674\n",
            " iteration: 7906  loss: 0.007359207607805729\n",
            " iteration: 7907  loss: 0.0073588998056948185\n",
            " iteration: 7908  loss: 0.007358548231422901\n",
            " iteration: 7909  loss: 0.007358179427683353\n",
            " iteration: 7910  loss: 0.007358409930020571\n",
            " iteration: 7911  loss: 0.00735799316316843\n",
            " iteration: 7912  loss: 0.007357735652476549\n",
            " iteration: 7913  loss: 0.007357382215559483\n",
            " iteration: 7914  loss: 0.0073571642860770226\n",
            " iteration: 7915  loss: 0.007357016671448946\n",
            " iteration: 7916  loss: 0.0073569281958043575\n",
            " iteration: 7917  loss: 0.007356679532676935\n",
            " iteration: 7918  loss: 0.007356403395533562\n",
            " iteration: 7919  loss: 0.007356022484600544\n",
            " iteration: 7920  loss: 0.007355647161602974\n",
            " iteration: 7921  loss: 0.007366804871708155\n",
            " iteration: 7922  loss: 0.007355569861829281\n",
            " iteration: 7923  loss: 0.007355166133493185\n",
            " iteration: 7924  loss: 0.007354714907705784\n",
            " iteration: 7925  loss: 0.007354212924838066\n",
            " iteration: 7926  loss: 0.007353870198130608\n",
            " iteration: 7927  loss: 0.0073531861416995525\n",
            " iteration: 7928  loss: 0.007352360989898443\n",
            " iteration: 7929  loss: 0.007358788978308439\n",
            " iteration: 7930  loss: 0.007352057378739119\n",
            " iteration: 7931  loss: 0.0073515367694199085\n",
            " iteration: 7932  loss: 0.007350842002779245\n",
            " iteration: 7933  loss: 0.0073501477017998695\n",
            " iteration: 7934  loss: 0.007350682280957699\n",
            " iteration: 7935  loss: 0.0073497421108186245\n",
            " iteration: 7936  loss: 0.00734906317666173\n",
            " iteration: 7937  loss: 0.007348495069891214\n",
            " iteration: 7938  loss: 0.007348339539021254\n",
            " iteration: 7939  loss: 0.007348089013248682\n",
            " iteration: 7940  loss: 0.007347830571234226\n",
            " iteration: 7941  loss: 0.007347497623413801\n",
            " iteration: 7942  loss: 0.007347081787884235\n",
            " iteration: 7943  loss: 0.007347456179559231\n",
            " iteration: 7944  loss: 0.007346906699240208\n",
            " iteration: 7945  loss: 0.007346507627516985\n",
            " iteration: 7946  loss: 0.0073462314903736115\n",
            " iteration: 7947  loss: 0.007345954887568951\n",
            " iteration: 7948  loss: 0.007345790509134531\n",
            " iteration: 7949  loss: 0.007345446385443211\n",
            " iteration: 7950  loss: 0.007346049882471561\n",
            " iteration: 7951  loss: 0.007345239166170359\n",
            " iteration: 7952  loss: 0.0073450347408652306\n",
            " iteration: 7953  loss: 0.007344874553382397\n",
            " iteration: 7954  loss: 0.0073447683826088905\n",
            " iteration: 7955  loss: 0.00734449690207839\n",
            " iteration: 7956  loss: 0.007344146724790335\n",
            " iteration: 7957  loss: 0.007343810051679611\n",
            " iteration: 7958  loss: 0.007343596778810024\n",
            " iteration: 7959  loss: 0.007343416102230549\n",
            " iteration: 7960  loss: 0.007343661040067673\n",
            " iteration: 7961  loss: 0.0073433807119727135\n",
            " iteration: 7962  loss: 0.007343268021941185\n",
            " iteration: 7963  loss: 0.007343002129346132\n",
            " iteration: 7964  loss: 0.007342683617025614\n",
            " iteration: 7965  loss: 0.007342421915382147\n",
            " iteration: 7966  loss: 0.00734206847846508\n",
            " iteration: 7967  loss: 0.007341885007917881\n",
            " iteration: 7968  loss: 0.007342048455029726\n",
            " iteration: 7969  loss: 0.007341747172176838\n",
            " iteration: 7970  loss: 0.007341575343161821\n",
            " iteration: 7971  loss: 0.00734126940369606\n",
            " iteration: 7972  loss: 0.007340934127569199\n",
            " iteration: 7973  loss: 0.007341430988162756\n",
            " iteration: 7974  loss: 0.0073408158496022224\n",
            " iteration: 7975  loss: 0.007340611424297094\n",
            " iteration: 7976  loss: 0.0073404377326369286\n",
            " iteration: 7977  loss: 0.007340043317526579\n",
            " iteration: 7978  loss: 0.007339515723288059\n",
            " iteration: 7979  loss: 0.00733936158940196\n",
            " iteration: 7980  loss: 0.007338870316743851\n",
            " iteration: 7981  loss: 0.007338641211390495\n",
            " iteration: 7982  loss: 0.0073385038413107395\n",
            " iteration: 7983  loss: 0.007338082883507013\n",
            " iteration: 7984  loss: 0.0073377895168960094\n",
            " iteration: 7985  loss: 0.0073374612256884575\n",
            " iteration: 7986  loss: 0.007337114308029413\n",
            " iteration: 7987  loss: 0.007336703594774008\n",
            " iteration: 7988  loss: 0.0073363445699214935\n",
            " iteration: 7989  loss: 0.007335916627198458\n",
            " iteration: 7990  loss: 0.007335580885410309\n",
            " iteration: 7991  loss: 0.00733524514362216\n",
            " iteration: 7992  loss: 0.0073348283767700195\n",
            " iteration: 7993  loss: 0.007334539666771889\n",
            " iteration: 7994  loss: 0.0073340050876140594\n",
            " iteration: 7995  loss: 0.007333605084568262\n",
            " iteration: 7996  loss: 0.007333029061555862\n",
            " iteration: 7997  loss: 0.007332726381719112\n",
            " iteration: 7998  loss: 0.007332500535994768\n",
            " iteration: 7999  loss: 0.00733198132365942\n",
            " iteration: 8000  loss: 0.007332125678658485\n",
            " iteration: 8001  loss: 0.0073317717760801315\n",
            " iteration: 8002  loss: 0.007331348489969969\n",
            " iteration: 8003  loss: 0.007330865133553743\n",
            " iteration: 8004  loss: 0.0073304674588143826\n",
            " iteration: 8005  loss: 0.007330442778766155\n",
            " iteration: 8006  loss: 0.007330187596380711\n",
            " iteration: 8007  loss: 0.007329717744141817\n",
            " iteration: 8008  loss: 0.007329380605369806\n",
            " iteration: 8009  loss: 0.007329118438065052\n",
            " iteration: 8010  loss: 0.007328640669584274\n",
            " iteration: 8011  loss: 0.00732817966490984\n",
            " iteration: 8012  loss: 0.00732776615768671\n",
            " iteration: 8013  loss: 0.007327388506382704\n",
            " iteration: 8014  loss: 0.007327245082706213\n",
            " iteration: 8015  loss: 0.00732708303257823\n",
            " iteration: 8016  loss: 0.007327158469706774\n",
            " iteration: 8017  loss: 0.007327000144869089\n",
            " iteration: 8018  loss: 0.007326880469918251\n",
            " iteration: 8019  loss: 0.00732662295922637\n",
            " iteration: 8020  loss: 0.007326388731598854\n",
            " iteration: 8021  loss: 0.007325979880988598\n",
            " iteration: 8022  loss: 0.007325435522943735\n",
            " iteration: 8023  loss: 0.007324830628931522\n",
            " iteration: 8024  loss: 0.007324368227273226\n",
            " iteration: 8025  loss: 0.007323947735130787\n",
            " iteration: 8026  loss: 0.007323519792407751\n",
            " iteration: 8027  loss: 0.007322781719267368\n",
            " iteration: 8028  loss: 0.007322575431317091\n",
            " iteration: 8029  loss: 0.007321551907807589\n",
            " iteration: 8030  loss: 0.007321365177631378\n",
            " iteration: 8031  loss: 0.007321121636778116\n",
            " iteration: 8032  loss: 0.00732091348618269\n",
            " iteration: 8033  loss: 0.007320673670619726\n",
            " iteration: 8034  loss: 0.007320530246943235\n",
            " iteration: 8035  loss: 0.00732032535597682\n",
            " iteration: 8036  loss: 0.007320022210478783\n",
            " iteration: 8037  loss: 0.007319915574043989\n",
            " iteration: 8038  loss: 0.007319645956158638\n",
            " iteration: 8039  loss: 0.007319477386772633\n",
            " iteration: 8040  loss: 0.007319159340113401\n",
            " iteration: 8041  loss: 0.007318913005292416\n",
            " iteration: 8042  loss: 0.007318729069083929\n",
            " iteration: 8043  loss: 0.007318354211747646\n",
            " iteration: 8044  loss: 0.007317928597331047\n",
            " iteration: 8045  loss: 0.007317714858800173\n",
            " iteration: 8046  loss: 0.0073173753917217255\n",
            " iteration: 8047  loss: 0.007316917181015015\n",
            " iteration: 8048  loss: 0.007316547445952892\n",
            " iteration: 8049  loss: 0.007318108342587948\n",
            " iteration: 8050  loss: 0.007316402159631252\n",
            " iteration: 8051  loss: 0.00731610506772995\n",
            " iteration: 8052  loss: 0.0073159015737473965\n",
            " iteration: 8053  loss: 0.007316560950130224\n",
            " iteration: 8054  loss: 0.007315806113183498\n",
            " iteration: 8055  loss: 0.007315613329410553\n",
            " iteration: 8056  loss: 0.007315177004784346\n",
            " iteration: 8057  loss: 0.007314675487577915\n",
            " iteration: 8058  loss: 0.007314418442547321\n",
            " iteration: 8059  loss: 0.007314322516322136\n",
            " iteration: 8060  loss: 0.007314002141356468\n",
            " iteration: 8061  loss: 0.007313874084502459\n",
            " iteration: 8062  loss: 0.007313631474971771\n",
            " iteration: 8063  loss: 0.0073134214617311954\n",
            " iteration: 8064  loss: 0.00731324777007103\n",
            " iteration: 8065  loss: 0.007313262205570936\n",
            " iteration: 8066  loss: 0.00731295021250844\n",
            " iteration: 8067  loss: 0.0073128207586705685\n",
            " iteration: 8068  loss: 0.007312417030334473\n",
            " iteration: 8069  loss: 0.007312181405723095\n",
            " iteration: 8070  loss: 0.007311954163014889\n",
            " iteration: 8071  loss: 0.007311994209885597\n",
            " iteration: 8072  loss: 0.007311761379241943\n",
            " iteration: 8073  loss: 0.007311402354389429\n",
            " iteration: 8074  loss: 0.007310918997973204\n",
            " iteration: 8075  loss: 0.007310668472200632\n",
            " iteration: 8076  loss: 0.007310483604669571\n",
            " iteration: 8077  loss: 0.0073102242313325405\n",
            " iteration: 8078  loss: 0.007309937849640846\n",
            " iteration: 8079  loss: 0.0073097082786262035\n",
            " iteration: 8080  loss: 0.007309490814805031\n",
            " iteration: 8081  loss: 0.007309255190193653\n",
            " iteration: 8082  loss: 0.00730903772637248\n",
            " iteration: 8083  loss: 0.007308744825422764\n",
            " iteration: 8084  loss: 0.007308303378522396\n",
            " iteration: 8085  loss: 0.007332817651331425\n",
            " iteration: 8086  loss: 0.007308227941393852\n",
            " iteration: 8087  loss: 0.007307830266654491\n",
            " iteration: 8088  loss: 0.007307599298655987\n",
            " iteration: 8089  loss: 0.007307271473109722\n",
            " iteration: 8090  loss: 0.007306981831789017\n",
            " iteration: 8091  loss: 0.007306721992790699\n",
            " iteration: 8092  loss: 0.00730647798627615\n",
            " iteration: 8093  loss: 0.00730603002011776\n",
            " iteration: 8094  loss: 0.007305721752345562\n",
            " iteration: 8095  loss: 0.00730546098202467\n",
            " iteration: 8096  loss: 0.0073048947378993034\n",
            " iteration: 8097  loss: 0.0073046269826591015\n",
            " iteration: 8098  loss: 0.007304138038307428\n",
            " iteration: 8099  loss: 0.007303934078663588\n",
            " iteration: 8100  loss: 0.007303535472601652\n",
            " iteration: 8101  loss: 0.007303246296942234\n",
            " iteration: 8102  loss: 0.007302902173250914\n",
            " iteration: 8103  loss: 0.00730238389223814\n",
            " iteration: 8104  loss: 0.0073019275441765785\n",
            " iteration: 8105  loss: 0.007301844656467438\n",
            " iteration: 8106  loss: 0.007301334757357836\n",
            " iteration: 8107  loss: 0.007301148027181625\n",
            " iteration: 8108  loss: 0.007300915662199259\n",
            " iteration: 8109  loss: 0.007300673983991146\n",
            " iteration: 8110  loss: 0.007302784360945225\n",
            " iteration: 8111  loss: 0.007300511933863163\n",
            " iteration: 8112  loss: 0.007300194818526506\n",
            " iteration: 8113  loss: 0.007299688179045916\n",
            " iteration: 8114  loss: 0.0072993808425962925\n",
            " iteration: 8115  loss: 0.007299021817743778\n",
            " iteration: 8116  loss: 0.007298671640455723\n",
            " iteration: 8117  loss: 0.007299152668565512\n",
            " iteration: 8118  loss: 0.0072985487058758736\n",
            " iteration: 8119  loss: 0.007298198528587818\n",
            " iteration: 8120  loss: 0.00729787303134799\n",
            " iteration: 8121  loss: 0.0072975982911884785\n",
            " iteration: 8122  loss: 0.007297272328287363\n",
            " iteration: 8123  loss: 0.007297164294868708\n",
            " iteration: 8124  loss: 0.00729665532708168\n",
            " iteration: 8125  loss: 0.007296446710824966\n",
            " iteration: 8126  loss: 0.007296014577150345\n",
            " iteration: 8127  loss: 0.007296308409422636\n",
            " iteration: 8128  loss: 0.007295815274119377\n",
            " iteration: 8129  loss: 0.007295482791960239\n",
            " iteration: 8130  loss: 0.00729498453438282\n",
            " iteration: 8131  loss: 0.0072947219014167786\n",
            " iteration: 8132  loss: 0.007294236216694117\n",
            " iteration: 8133  loss: 0.007293859031051397\n",
            " iteration: 8134  loss: 0.007293726783245802\n",
            " iteration: 8135  loss: 0.007293101400136948\n",
            " iteration: 8136  loss: 0.0072927470318973064\n",
            " iteration: 8137  loss: 0.007292373105883598\n",
            " iteration: 8138  loss: 0.007292181719094515\n",
            " iteration: 8139  loss: 0.007291949819773436\n",
            " iteration: 8140  loss: 0.007291681133210659\n",
            " iteration: 8141  loss: 0.007292821537703276\n",
            " iteration: 8142  loss: 0.007291576825082302\n",
            " iteration: 8143  loss: 0.007291275542229414\n",
            " iteration: 8144  loss: 0.007290957495570183\n",
            " iteration: 8145  loss: 0.007290567271411419\n",
            " iteration: 8146  loss: 0.00729045644402504\n",
            " iteration: 8147  loss: 0.007290111389011145\n",
            " iteration: 8148  loss: 0.007289981935173273\n",
            " iteration: 8149  loss: 0.0072894929908216\n",
            " iteration: 8150  loss: 0.007288773078471422\n",
            " iteration: 8151  loss: 0.0072884331457316875\n",
            " iteration: 8152  loss: 0.007287786342203617\n",
            " iteration: 8153  loss: 0.007287549786269665\n",
            " iteration: 8154  loss: 0.007287328597158194\n",
            " iteration: 8155  loss: 0.007287326268851757\n",
            " iteration: 8156  loss: 0.007287171203643084\n",
            " iteration: 8157  loss: 0.007286917883902788\n",
            " iteration: 8158  loss: 0.007286595180630684\n",
            " iteration: 8159  loss: 0.007286144886165857\n",
            " iteration: 8160  loss: 0.007285835687071085\n",
            " iteration: 8161  loss: 0.007285572588443756\n",
            " iteration: 8162  loss: 0.007285240106284618\n",
            " iteration: 8163  loss: 0.007284705061465502\n",
            " iteration: 8164  loss: 0.007287613581866026\n",
            " iteration: 8165  loss: 0.007284470833837986\n",
            " iteration: 8166  loss: 0.007284001912921667\n",
            " iteration: 8167  loss: 0.007283471990376711\n",
            " iteration: 8168  loss: 0.0072832549922168255\n",
            " iteration: 8169  loss: 0.007283063139766455\n",
            " iteration: 8170  loss: 0.0072828661650419235\n",
            " iteration: 8171  loss: 0.00728270597755909\n",
            " iteration: 8172  loss: 0.0072821020148694515\n",
            " iteration: 8173  loss: 0.007281355559825897\n",
            " iteration: 8174  loss: 0.007280897349119186\n",
            " iteration: 8175  loss: 0.007280654739588499\n",
            " iteration: 8176  loss: 0.007280516438186169\n",
            " iteration: 8177  loss: 0.007280377671122551\n",
            " iteration: 8178  loss: 0.0072801485657691956\n",
            " iteration: 8179  loss: 0.007279827259480953\n",
            " iteration: 8180  loss: 0.007279540877789259\n",
            " iteration: 8181  loss: 0.007279241923242807\n",
            " iteration: 8182  loss: 0.007278941571712494\n",
            " iteration: 8183  loss: 0.007278671488165855\n",
            " iteration: 8184  loss: 0.00727844750508666\n",
            " iteration: 8185  loss: 0.00727820722386241\n",
            " iteration: 8186  loss: 0.007277990225702524\n",
            " iteration: 8187  loss: 0.0072778137400746346\n",
            " iteration: 8188  loss: 0.007277421187609434\n",
            " iteration: 8189  loss: 0.007276900578290224\n",
            " iteration: 8190  loss: 0.0072768875397741795\n",
            " iteration: 8191  loss: 0.007276437245309353\n",
            " iteration: 8192  loss: 0.00727586867287755\n",
            " iteration: 8193  loss: 0.0072753531858325005\n",
            " iteration: 8194  loss: 0.007274942006915808\n",
            " iteration: 8195  loss: 0.007274483796209097\n",
            " iteration: 8196  loss: 0.007274216506630182\n",
            " iteration: 8197  loss: 0.007274087052792311\n",
            " iteration: 8198  loss: 0.007273817900568247\n",
            " iteration: 8199  loss: 0.007273704279214144\n",
            " iteration: 8200  loss: 0.007273287512362003\n",
            " iteration: 8201  loss: 0.0072728670202195644\n",
            " iteration: 8202  loss: 0.007272432558238506\n",
            " iteration: 8203  loss: 0.007271939422935247\n",
            " iteration: 8204  loss: 0.007271467242389917\n",
            " iteration: 8205  loss: 0.007271265611052513\n",
            " iteration: 8206  loss: 0.007270806934684515\n",
            " iteration: 8207  loss: 0.00727049820125103\n",
            " iteration: 8208  loss: 0.007270036265254021\n",
            " iteration: 8209  loss: 0.007269787136465311\n",
            " iteration: 8210  loss: 0.007269605062901974\n",
            " iteration: 8211  loss: 0.007269404362887144\n",
            " iteration: 8212  loss: 0.007269116118550301\n",
            " iteration: 8213  loss: 0.007268775720149279\n",
            " iteration: 8214  loss: 0.007268406916409731\n",
            " iteration: 8215  loss: 0.007268032990396023\n",
            " iteration: 8216  loss: 0.007267553824931383\n",
            " iteration: 8217  loss: 0.007267216686159372\n",
            " iteration: 8218  loss: 0.007267402950674295\n",
            " iteration: 8219  loss: 0.007267005741596222\n",
            " iteration: 8220  loss: 0.00726673798635602\n",
            " iteration: 8221  loss: 0.007266462314873934\n",
            " iteration: 8222  loss: 0.007266489788889885\n",
            " iteration: 8223  loss: 0.007266375236213207\n",
            " iteration: 8224  loss: 0.007266169413924217\n",
            " iteration: 8225  loss: 0.0072656250558793545\n",
            " iteration: 8226  loss: 0.007265245541930199\n",
            " iteration: 8227  loss: 0.007264741696417332\n",
            " iteration: 8228  loss: 0.007264678832143545\n",
            " iteration: 8229  loss: 0.007264516782015562\n",
            " iteration: 8230  loss: 0.00726419547572732\n",
            " iteration: 8231  loss: 0.007264030631631613\n",
            " iteration: 8232  loss: 0.007263876497745514\n",
            " iteration: 8233  loss: 0.007263759151101112\n",
            " iteration: 8234  loss: 0.007263628765940666\n",
            " iteration: 8235  loss: 0.007263618987053633\n",
            " iteration: 8236  loss: 0.007263463456183672\n",
            " iteration: 8237  loss: 0.0072632827796041965\n",
            " iteration: 8238  loss: 0.007262736093252897\n",
            " iteration: 8239  loss: 0.007262359838932753\n",
            " iteration: 8240  loss: 0.00726167019456625\n",
            " iteration: 8241  loss: 0.007262151688337326\n",
            " iteration: 8242  loss: 0.007261499296873808\n",
            " iteration: 8243  loss: 0.007261114660650492\n",
            " iteration: 8244  loss: 0.007260835729539394\n",
            " iteration: 8245  loss: 0.007260674610733986\n",
            " iteration: 8246  loss: 0.007260332349687815\n",
            " iteration: 8247  loss: 0.007259806618094444\n",
            " iteration: 8248  loss: 0.00725964130833745\n",
            " iteration: 8249  loss: 0.007259123492985964\n",
            " iteration: 8250  loss: 0.007258891593664885\n",
            " iteration: 8251  loss: 0.007258550263941288\n",
            " iteration: 8252  loss: 0.007258616853505373\n",
            " iteration: 8253  loss: 0.0072583700530231\n",
            " iteration: 8254  loss: 0.0072580487467348576\n",
            " iteration: 8255  loss: 0.007257792633026838\n",
            " iteration: 8256  loss: 0.007257579360157251\n",
            " iteration: 8257  loss: 0.007257360033690929\n",
            " iteration: 8258  loss: 0.007257070858031511\n",
            " iteration: 8259  loss: 0.007256823591887951\n",
            " iteration: 8260  loss: 0.007256567478179932\n",
            " iteration: 8261  loss: 0.007256383541971445\n",
            " iteration: 8262  loss: 0.007256318349391222\n",
            " iteration: 8263  loss: 0.007255993317812681\n",
            " iteration: 8264  loss: 0.007255904376506805\n",
            " iteration: 8265  loss: 0.0072557986713945866\n",
            " iteration: 8266  loss: 0.007255661766976118\n",
            " iteration: 8267  loss: 0.007257258519530296\n",
            " iteration: 8268  loss: 0.007255565375089645\n",
            " iteration: 8269  loss: 0.007255284581333399\n",
            " iteration: 8270  loss: 0.007255047559738159\n",
            " iteration: 8271  loss: 0.00725487619638443\n",
            " iteration: 8272  loss: 0.007254734169691801\n",
            " iteration: 8273  loss: 0.007254573982208967\n",
            " iteration: 8274  loss: 0.007254399824887514\n",
            " iteration: 8275  loss: 0.007254189811646938\n",
            " iteration: 8276  loss: 0.007253831718116999\n",
            " iteration: 8277  loss: 0.007253381889313459\n",
            " iteration: 8278  loss: 0.007253300864249468\n",
            " iteration: 8279  loss: 0.007252934854477644\n",
            " iteration: 8280  loss: 0.0072527057491242886\n",
            " iteration: 8281  loss: 0.007252359297126532\n",
            " iteration: 8282  loss: 0.0072519611567258835\n",
            " iteration: 8283  loss: 0.007251481059938669\n",
            " iteration: 8284  loss: 0.007251298055052757\n",
            " iteration: 8285  loss: 0.007250777445733547\n",
            " iteration: 8286  loss: 0.00725055206567049\n",
            " iteration: 8287  loss: 0.0072501664981245995\n",
            " iteration: 8288  loss: 0.007249806541949511\n",
            " iteration: 8289  loss: 0.007252281531691551\n",
            " iteration: 8290  loss: 0.0072496430948376656\n",
            " iteration: 8291  loss: 0.007249290123581886\n",
            " iteration: 8292  loss: 0.0072487699799239635\n",
            " iteration: 8293  loss: 0.007248401641845703\n",
            " iteration: 8294  loss: 0.007248003501445055\n",
            " iteration: 8295  loss: 0.007247731555253267\n",
            " iteration: 8296  loss: 0.0072471387684345245\n",
            " iteration: 8297  loss: 0.007247790228575468\n",
            " iteration: 8298  loss: 0.007247003726661205\n",
            " iteration: 8299  loss: 0.007246736902743578\n",
            " iteration: 8300  loss: 0.0072464472614228725\n",
            " iteration: 8301  loss: 0.007246141787618399\n",
            " iteration: 8302  loss: 0.007245927583426237\n",
            " iteration: 8303  loss: 0.007245772518217564\n",
            " iteration: 8304  loss: 0.007245857734233141\n",
            " iteration: 8305  loss: 0.007245704997330904\n",
            " iteration: 8306  loss: 0.007245610002428293\n",
            " iteration: 8307  loss: 0.007245441898703575\n",
            " iteration: 8308  loss: 0.007245283108204603\n",
            " iteration: 8309  loss: 0.007244994398206472\n",
            " iteration: 8310  loss: 0.0072449008002877235\n",
            " iteration: 8311  loss: 0.007244323380291462\n",
            " iteration: 8312  loss: 0.00724409893155098\n",
            " iteration: 8313  loss: 0.0072438595816493034\n",
            " iteration: 8314  loss: 0.007243679370731115\n",
            " iteration: 8315  loss: 0.007243477273732424\n",
            " iteration: 8316  loss: 0.007243286818265915\n",
            " iteration: 8317  loss: 0.007243134547024965\n",
            " iteration: 8318  loss: 0.007242972496896982\n",
            " iteration: 8319  loss: 0.0072427415288984776\n",
            " iteration: 8320  loss: 0.007242634426802397\n",
            " iteration: 8321  loss: 0.007242387160658836\n",
            " iteration: 8322  loss: 0.007242195308208466\n",
            " iteration: 8323  loss: 0.007242000661790371\n",
            " iteration: 8324  loss: 0.007242105435580015\n",
            " iteration: 8325  loss: 0.007241879589855671\n",
            " iteration: 8326  loss: 0.007241616956889629\n",
            " iteration: 8327  loss: 0.007241471670567989\n",
            " iteration: 8328  loss: 0.0072412495501339436\n",
            " iteration: 8329  loss: 0.007241089828312397\n",
            " iteration: 8330  loss: 0.007240795064717531\n",
            " iteration: 8331  loss: 0.007240595296025276\n",
            " iteration: 8332  loss: 0.007240160368382931\n",
            " iteration: 8333  loss: 0.007239795755594969\n",
            " iteration: 8334  loss: 0.0072394669987261295\n",
            " iteration: 8335  loss: 0.007239120081067085\n",
            " iteration: 8336  loss: 0.007239079102873802\n",
            " iteration: 8337  loss: 0.007238791324198246\n",
            " iteration: 8338  loss: 0.007238646969199181\n",
            " iteration: 8339  loss: 0.007238356396555901\n",
            " iteration: 8340  loss: 0.0072395289316773415\n",
            " iteration: 8341  loss: 0.007238322868943214\n",
            " iteration: 8342  loss: 0.007238141726702452\n",
            " iteration: 8343  loss: 0.0072378478944301605\n",
            " iteration: 8344  loss: 0.007237543351948261\n",
            " iteration: 8345  loss: 0.0072372108697891235\n",
            " iteration: 8346  loss: 0.007239078171551228\n",
            " iteration: 8347  loss: 0.007237019017338753\n",
            " iteration: 8348  loss: 0.00723671168088913\n",
            " iteration: 8349  loss: 0.00723646767437458\n",
            " iteration: 8350  loss: 0.007236328441649675\n",
            " iteration: 8351  loss: 0.007236172445118427\n",
            " iteration: 8352  loss: 0.00723610632121563\n",
            " iteration: 8353  loss: 0.007235861383378506\n",
            " iteration: 8354  loss: 0.00723574124276638\n",
            " iteration: 8355  loss: 0.007235511671751738\n",
            " iteration: 8356  loss: 0.007235333323478699\n",
            " iteration: 8357  loss: 0.007235084660351276\n",
            " iteration: 8358  loss: 0.0072348471730947495\n",
            " iteration: 8359  loss: 0.007234580349177122\n",
            " iteration: 8360  loss: 0.007234374061226845\n",
            " iteration: 8361  loss: 0.007234166841953993\n",
            " iteration: 8362  loss: 0.0072340769693255424\n",
            " iteration: 8363  loss: 0.007233987096697092\n",
            " iteration: 8364  loss: 0.007233810611069202\n",
            " iteration: 8365  loss: 0.007233473472297192\n",
            " iteration: 8366  loss: 0.0072331675328314304\n",
            " iteration: 8367  loss: 0.007233027368783951\n",
            " iteration: 8368  loss: 0.007232956122606993\n",
            " iteration: 8369  loss: 0.0072328788228333\n",
            " iteration: 8370  loss: 0.00723270932212472\n",
            " iteration: 8371  loss: 0.007232545409351587\n",
            " iteration: 8372  loss: 0.0072322566993534565\n",
            " iteration: 8373  loss: 0.007233169861137867\n",
            " iteration: 8374  loss: 0.007232155650854111\n",
            " iteration: 8375  loss: 0.007231900468468666\n",
            " iteration: 8376  loss: 0.007231597788631916\n",
            " iteration: 8377  loss: 0.00723137054592371\n",
            " iteration: 8378  loss: 0.007231128867715597\n",
            " iteration: 8379  loss: 0.007231131661683321\n",
            " iteration: 8380  loss: 0.007230894640088081\n",
            " iteration: 8381  loss: 0.007230684626847506\n",
            " iteration: 8382  loss: 0.007230475079268217\n",
            " iteration: 8383  loss: 0.007230226881802082\n",
            " iteration: 8384  loss: 0.007229883689433336\n",
            " iteration: 8385  loss: 0.007229665759950876\n",
            " iteration: 8386  loss: 0.0072292760014534\n",
            " iteration: 8387  loss: 0.007229074370115995\n",
            " iteration: 8388  loss: 0.007228938862681389\n",
            " iteration: 8389  loss: 0.007228708826005459\n",
            " iteration: 8390  loss: 0.007228479254990816\n",
            " iteration: 8391  loss: 0.007228243630379438\n",
            " iteration: 8392  loss: 0.007228111382573843\n",
            " iteration: 8393  loss: 0.007228003349155188\n",
            " iteration: 8394  loss: 0.007227873429656029\n",
            " iteration: 8395  loss: 0.007227802183479071\n",
            " iteration: 8396  loss: 0.007227637805044651\n",
            " iteration: 8397  loss: 0.007227679248899221\n",
            " iteration: 8398  loss: 0.007227570749819279\n",
            " iteration: 8399  loss: 0.007227463647723198\n",
            " iteration: 8400  loss: 0.0072271800599992275\n",
            " iteration: 8401  loss: 0.007227002177387476\n",
            " iteration: 8402  loss: 0.0072267921641469\n",
            " iteration: 8403  loss: 0.007227824069559574\n",
            " iteration: 8404  loss: 0.00722671952098608\n",
            " iteration: 8405  loss: 0.0072266291826963425\n",
            " iteration: 8406  loss: 0.007226542569696903\n",
            " iteration: 8407  loss: 0.00722649646922946\n",
            " iteration: 8408  loss: 0.007226402871310711\n",
            " iteration: 8409  loss: 0.007226655259728432\n",
            " iteration: 8410  loss: 0.0072263553738594055\n",
            " iteration: 8411  loss: 0.007226262707263231\n",
            " iteration: 8412  loss: 0.007226183544844389\n",
            " iteration: 8413  loss: 0.0072261132299900055\n",
            " iteration: 8414  loss: 0.007226006127893925\n",
            " iteration: 8415  loss: 0.007225763518363237\n",
            " iteration: 8416  loss: 0.007225705776363611\n",
            " iteration: 8417  loss: 0.0072254398837685585\n",
            " iteration: 8418  loss: 0.0072253393009305\n",
            " iteration: 8419  loss: 0.007225226145237684\n",
            " iteration: 8420  loss: 0.007225020322948694\n",
            " iteration: 8421  loss: 0.0072248391807079315\n",
            " iteration: 8422  loss: 0.007224521599709988\n",
            " iteration: 8423  loss: 0.007224046625196934\n",
            " iteration: 8424  loss: 0.007223895750939846\n",
            " iteration: 8425  loss: 0.007223722990602255\n",
            " iteration: 8426  loss: 0.007223676890134811\n",
            " iteration: 8427  loss: 0.007223478518426418\n",
            " iteration: 8428  loss: 0.007223215885460377\n",
            " iteration: 8429  loss: 0.007223949301987886\n",
            " iteration: 8430  loss: 0.0072230808436870575\n",
            " iteration: 8431  loss: 0.007222901564091444\n",
            " iteration: 8432  loss: 0.007222727406769991\n",
            " iteration: 8433  loss: 0.007222623564302921\n",
            " iteration: 8434  loss: 0.007222399581223726\n",
            " iteration: 8435  loss: 0.0072221835143864155\n",
            " iteration: 8436  loss: 0.007222428452223539\n",
            " iteration: 8437  loss: 0.0072220866568386555\n",
            " iteration: 8438  loss: 0.007221934851258993\n",
            " iteration: 8439  loss: 0.0072217234410345554\n",
            " iteration: 8440  loss: 0.007221459876745939\n",
            " iteration: 8441  loss: 0.007221149746328592\n",
            " iteration: 8442  loss: 0.007220820989459753\n",
            " iteration: 8443  loss: 0.007220468018203974\n",
            " iteration: 8444  loss: 0.00722023518756032\n",
            " iteration: 8445  loss: 0.007219966500997543\n",
            " iteration: 8446  loss: 0.007219687569886446\n",
            " iteration: 8447  loss: 0.007219674065709114\n",
            " iteration: 8448  loss: 0.0072196112014353275\n",
            " iteration: 8449  loss: 0.007219475228339434\n",
            " iteration: 8450  loss: 0.007219285238534212\n",
            " iteration: 8451  loss: 0.007219050545245409\n",
            " iteration: 8452  loss: 0.007219227962195873\n",
            " iteration: 8453  loss: 0.00721890851855278\n",
            " iteration: 8454  loss: 0.007218698970973492\n",
            " iteration: 8455  loss: 0.007218573242425919\n",
            " iteration: 8456  loss: 0.007218511309474707\n",
            " iteration: 8457  loss: 0.007218388840556145\n",
            " iteration: 8458  loss: 0.007218193728476763\n",
            " iteration: 8459  loss: 0.007218054495751858\n",
            " iteration: 8460  loss: 0.007217905018478632\n",
            " iteration: 8461  loss: 0.007217733189463615\n",
            " iteration: 8462  loss: 0.0072175669483840466\n",
            " iteration: 8463  loss: 0.007217223756015301\n",
            " iteration: 8464  loss: 0.007216986734420061\n",
            " iteration: 8465  loss: 0.007216906640678644\n",
            " iteration: 8466  loss: 0.0072168041951954365\n",
            " iteration: 8467  loss: 0.007216584403067827\n",
            " iteration: 8468  loss: 0.0072164516896009445\n",
            " iteration: 8469  loss: 0.007216169498860836\n",
            " iteration: 8470  loss: 0.007215938065201044\n",
            " iteration: 8471  loss: 0.007215824443846941\n",
            " iteration: 8472  loss: 0.007215492427349091\n",
            " iteration: 8473  loss: 0.007215386256575584\n",
            " iteration: 8474  loss: 0.0072152274660766125\n",
            " iteration: 8475  loss: 0.007214984390884638\n",
            " iteration: 8476  loss: 0.00721474876627326\n",
            " iteration: 8477  loss: 0.007214355748146772\n",
            " iteration: 8478  loss: 0.007213901728391647\n",
            " iteration: 8479  loss: 0.007213533390313387\n",
            " iteration: 8480  loss: 0.0072136144153773785\n",
            " iteration: 8481  loss: 0.007213314529508352\n",
            " iteration: 8482  loss: 0.007213074713945389\n",
            " iteration: 8483  loss: 0.0072128502652049065\n",
            " iteration: 8484  loss: 0.007212682627141476\n",
            " iteration: 8485  loss: 0.007212436757981777\n",
            " iteration: 8486  loss: 0.007212252821773291\n",
            " iteration: 8487  loss: 0.007212265394628048\n",
            " iteration: 8488  loss: 0.007212165277451277\n",
            " iteration: 8489  loss: 0.007212046533823013\n",
            " iteration: 8490  loss: 0.007211952470242977\n",
            " iteration: 8491  loss: 0.007211816962808371\n",
            " iteration: 8492  loss: 0.007211640477180481\n",
            " iteration: 8493  loss: 0.007211357355117798\n",
            " iteration: 8494  loss: 0.007211120333522558\n",
            " iteration: 8495  loss: 0.007210928946733475\n",
            " iteration: 8496  loss: 0.00721074128523469\n",
            " iteration: 8497  loss: 0.007210522890090942\n",
            " iteration: 8498  loss: 0.007210288662463427\n",
            " iteration: 8499  loss: 0.007210185751318932\n",
            " iteration: 8500  loss: 0.00721007538959384\n",
            " iteration: 8501  loss: 0.007209984119981527\n",
            " iteration: 8502  loss: 0.007209871895611286\n",
            " iteration: 8503  loss: 0.007209715899080038\n",
            " iteration: 8504  loss: 0.007210083305835724\n",
            " iteration: 8505  loss: 0.007209662813693285\n",
            " iteration: 8506  loss: 0.00720950635150075\n",
            " iteration: 8507  loss: 0.007209318224340677\n",
            " iteration: 8508  loss: 0.007209246512502432\n",
            " iteration: 8509  loss: 0.007209195755422115\n",
            " iteration: 8510  loss: 0.007209090515971184\n",
            " iteration: 8511  loss: 0.007209013216197491\n",
            " iteration: 8512  loss: 0.007208928465843201\n",
            " iteration: 8513  loss: 0.007208759896457195\n",
            " iteration: 8514  loss: 0.007208614144474268\n",
            " iteration: 8515  loss: 0.007208501920104027\n",
            " iteration: 8516  loss: 0.007208224385976791\n",
            " iteration: 8517  loss: 0.007208538707345724\n",
            " iteration: 8518  loss: 0.007208017632365227\n",
            " iteration: 8519  loss: 0.007207606919109821\n",
            " iteration: 8520  loss: 0.0072074453346431255\n",
            " iteration: 8521  loss: 0.007207300513982773\n",
            " iteration: 8522  loss: 0.00720715569332242\n",
            " iteration: 8523  loss: 0.007206978742033243\n",
            " iteration: 8524  loss: 0.007206819485872984\n",
            " iteration: 8525  loss: 0.0072067128494381905\n",
            " iteration: 8526  loss: 0.007206588517874479\n",
            " iteration: 8527  loss: 0.007206430193036795\n",
            " iteration: 8528  loss: 0.007206248585134745\n",
            " iteration: 8529  loss: 0.007206557784229517\n",
            " iteration: 8530  loss: 0.007206151727586985\n",
            " iteration: 8531  loss: 0.007205959875136614\n",
            " iteration: 8532  loss: 0.007205890491604805\n",
            " iteration: 8533  loss: 0.00720570981502533\n",
            " iteration: 8534  loss: 0.007205595728009939\n",
            " iteration: 8535  loss: 0.0072053843177855015\n",
            " iteration: 8536  loss: 0.007205489557236433\n",
            " iteration: 8537  loss: 0.007205345202237368\n",
            " iteration: 8538  loss: 0.007205250672996044\n",
            " iteration: 8539  loss: 0.007205143105238676\n",
            " iteration: 8540  loss: 0.007205003406852484\n",
            " iteration: 8541  loss: 0.007204832509160042\n",
            " iteration: 8542  loss: 0.0072046988643705845\n",
            " iteration: 8543  loss: 0.007204558700323105\n",
            " iteration: 8544  loss: 0.007204421330243349\n",
            " iteration: 8545  loss: 0.007204204797744751\n",
            " iteration: 8546  loss: 0.007204273249953985\n",
            " iteration: 8547  loss: 0.007204070687294006\n",
            " iteration: 8548  loss: 0.007203852757811546\n",
            " iteration: 8549  loss: 0.0072036427445709705\n",
            " iteration: 8550  loss: 0.007203461602330208\n",
            " iteration: 8551  loss: 0.007203274872153997\n",
            " iteration: 8552  loss: 0.007203008513897657\n",
            " iteration: 8553  loss: 0.00720316031947732\n",
            " iteration: 8554  loss: 0.007202889770269394\n",
            " iteration: 8555  loss: 0.0072027817368507385\n",
            " iteration: 8556  loss: 0.007202646229416132\n",
            " iteration: 8557  loss: 0.00720249954611063\n",
            " iteration: 8558  loss: 0.007202280685305595\n",
            " iteration: 8559  loss: 0.007202054373919964\n",
            " iteration: 8560  loss: 0.0072015742771327496\n",
            " iteration: 8561  loss: 0.007201354019343853\n",
            " iteration: 8562  loss: 0.007201133295893669\n",
            " iteration: 8563  loss: 0.007200915832072496\n",
            " iteration: 8564  loss: 0.007200834807008505\n",
            " iteration: 8565  loss: 0.007200766354799271\n",
            " iteration: 8566  loss: 0.007200645748525858\n",
            " iteration: 8567  loss: 0.007200469728559256\n",
            " iteration: 8568  loss: 0.007200284395366907\n",
            " iteration: 8569  loss: 0.0072000171057879925\n",
            " iteration: 8570  loss: 0.007208577357232571\n",
            " iteration: 8571  loss: 0.007199984509497881\n",
            " iteration: 8572  loss: 0.007199875079095364\n",
            " iteration: 8573  loss: 0.0071997568011283875\n",
            " iteration: 8574  loss: 0.007199631538242102\n",
            " iteration: 8575  loss: 0.007199580781161785\n",
            " iteration: 8576  loss: 0.007199512328952551\n",
            " iteration: 8577  loss: 0.00719944154843688\n",
            " iteration: 8578  loss: 0.007199323270469904\n",
            " iteration: 8579  loss: 0.007199164014309645\n",
            " iteration: 8580  loss: 0.007198881823569536\n",
            " iteration: 8581  loss: 0.0071989623829722404\n",
            " iteration: 8582  loss: 0.007198686245828867\n",
            " iteration: 8583  loss: 0.0071984874084591866\n",
            " iteration: 8584  loss: 0.007198297884315252\n",
            " iteration: 8585  loss: 0.007198173552751541\n",
            " iteration: 8586  loss: 0.007198020815849304\n",
            " iteration: 8587  loss: 0.0071977549232542515\n",
            " iteration: 8588  loss: 0.007197591010481119\n",
            " iteration: 8589  loss: 0.007197348400950432\n",
            " iteration: 8590  loss: 0.007197098806500435\n",
            " iteration: 8591  loss: 0.007198224309831858\n",
            " iteration: 8592  loss: 0.0071969847194850445\n",
            " iteration: 8593  loss: 0.0071967411786317825\n",
            " iteration: 8594  loss: 0.007196630816906691\n",
            " iteration: 8595  loss: 0.007196441758424044\n",
            " iteration: 8596  loss: 0.007196323946118355\n",
            " iteration: 8597  loss: 0.007196126505732536\n",
            " iteration: 8598  loss: 0.007195891346782446\n",
            " iteration: 8599  loss: 0.007195647805929184\n",
            " iteration: 8600  loss: 0.007195429876446724\n",
            " iteration: 8601  loss: 0.0071953111328184605\n",
            " iteration: 8602  loss: 0.007195012643933296\n",
            " iteration: 8603  loss: 0.007194719277322292\n",
            " iteration: 8604  loss: 0.007194501347839832\n",
            " iteration: 8605  loss: 0.0071943667717278\n",
            " iteration: 8606  loss: 0.007194259203970432\n",
            " iteration: 8607  loss: 0.007194146513938904\n",
            " iteration: 8608  loss: 0.007193867117166519\n",
            " iteration: 8609  loss: 0.007193655241280794\n",
            " iteration: 8610  loss: 0.007193442899733782\n",
            " iteration: 8611  loss: 0.007193348836153746\n",
            " iteration: 8612  loss: 0.007193312048912048\n",
            " iteration: 8613  loss: 0.00719325989484787\n",
            " iteration: 8614  loss: 0.007193217519670725\n",
            " iteration: 8615  loss: 0.007193034514784813\n",
            " iteration: 8616  loss: 0.00719293812289834\n",
            " iteration: 8617  loss: 0.007192566990852356\n",
            " iteration: 8618  loss: 0.007192519493401051\n",
            " iteration: 8619  loss: 0.0071920291520655155\n",
            " iteration: 8620  loss: 0.00719181215390563\n",
            " iteration: 8621  loss: 0.007191591430455446\n",
            " iteration: 8622  loss: 0.00719136418774724\n",
            " iteration: 8623  loss: 0.007191262673586607\n",
            " iteration: 8624  loss: 0.007191207259893417\n",
            " iteration: 8625  loss: 0.00719109270721674\n",
            " iteration: 8626  loss: 0.007191043347120285\n",
            " iteration: 8627  loss: 0.007190994452685118\n",
            " iteration: 8628  loss: 0.007190922740846872\n",
            " iteration: 8629  loss: 0.007190810516476631\n",
            " iteration: 8630  loss: 0.007190790958702564\n",
            " iteration: 8631  loss: 0.007190663367509842\n",
            " iteration: 8632  loss: 0.007190624251961708\n",
            " iteration: 8633  loss: 0.007190561853349209\n",
            " iteration: 8634  loss: 0.0071904631331563\n",
            " iteration: 8635  loss: 0.007190659176558256\n",
            " iteration: 8636  loss: 0.007190399337559938\n",
            " iteration: 8637  loss: 0.007190258242189884\n",
            " iteration: 8638  loss: 0.007190170232206583\n",
            " iteration: 8639  loss: 0.007190067786723375\n",
            " iteration: 8640  loss: 0.007189999334514141\n",
            " iteration: 8641  loss: 0.007189919240772724\n",
            " iteration: 8642  loss: 0.007189769763499498\n",
            " iteration: 8643  loss: 0.007189654279500246\n",
            " iteration: 8644  loss: 0.007189513184130192\n",
            " iteration: 8645  loss: 0.007189345080405474\n",
            " iteration: 8646  loss: 0.007190861739218235\n",
            " iteration: 8647  loss: 0.0071892766281962395\n",
            " iteration: 8648  loss: 0.007189122028648853\n",
            " iteration: 8649  loss: 0.007188896182924509\n",
            " iteration: 8650  loss: 0.007188693154603243\n",
            " iteration: 8651  loss: 0.007188434712588787\n",
            " iteration: 8652  loss: 0.007188317831605673\n",
            " iteration: 8653  loss: 0.007187997922301292\n",
            " iteration: 8654  loss: 0.007187779061496258\n",
            " iteration: 8655  loss: 0.007187587674707174\n",
            " iteration: 8656  loss: 0.007187351584434509\n",
            " iteration: 8657  loss: 0.00718728918582201\n",
            " iteration: 8658  loss: 0.007187004201114178\n",
            " iteration: 8659  loss: 0.007186874747276306\n",
            " iteration: 8660  loss: 0.0071867527440190315\n",
            " iteration: 8661  loss: 0.007186637260019779\n",
            " iteration: 8662  loss: 0.007186506874859333\n",
            " iteration: 8663  loss: 0.0071863336488604546\n",
            " iteration: 8664  loss: 0.007186450995504856\n",
            " iteration: 8665  loss: 0.007186266593635082\n",
            " iteration: 8666  loss: 0.00718607846647501\n",
            " iteration: 8667  loss: 0.007185973227024078\n",
            " iteration: 8668  loss: 0.0071859098970890045\n",
            " iteration: 8669  loss: 0.007185882888734341\n",
            " iteration: 8670  loss: 0.0071858130395412445\n",
            " iteration: 8671  loss: 0.007185728754848242\n",
            " iteration: 8672  loss: 0.007185596041381359\n",
            " iteration: 8673  loss: 0.0071854665875434875\n",
            " iteration: 8674  loss: 0.007185323163866997\n",
            " iteration: 8675  loss: 0.007185216993093491\n",
            " iteration: 8676  loss: 0.007185162045061588\n",
            " iteration: 8677  loss: 0.007185209542512894\n",
            " iteration: 8678  loss: 0.007185072172433138\n",
            " iteration: 8679  loss: 0.007184966467320919\n",
            " iteration: 8680  loss: 0.007184899877756834\n",
            " iteration: 8681  loss: 0.007185389753431082\n",
            " iteration: 8682  loss: 0.007184902206063271\n",
            " iteration: 8683  loss: 0.0071848975494503975\n",
            " iteration: 8684  loss: 0.007184883113950491\n",
            " iteration: 8685  loss: 0.007184839341789484\n",
            " iteration: 8686  loss: 0.007184801157563925\n",
            " iteration: 8687  loss: 0.007184704765677452\n",
            " iteration: 8688  loss: 0.007185070309787989\n",
            " iteration: 8689  loss: 0.007184672635048628\n",
            " iteration: 8690  loss: 0.007184527814388275\n",
            " iteration: 8691  loss: 0.007184434216469526\n",
            " iteration: 8692  loss: 0.007184386253356934\n",
            " iteration: 8693  loss: 0.007184316869825125\n",
            " iteration: 8694  loss: 0.007184216752648354\n",
            " iteration: 8695  loss: 0.007184671703726053\n",
            " iteration: 8696  loss: 0.0071841343306005\n",
            " iteration: 8697  loss: 0.007184045389294624\n",
            " iteration: 8698  loss: 0.007183911744505167\n",
            " iteration: 8699  loss: 0.007183785084635019\n",
            " iteration: 8700  loss: 0.0071836127899587154\n",
            " iteration: 8701  loss: 0.007183290086686611\n",
            " iteration: 8702  loss: 0.007183308247476816\n",
            " iteration: 8703  loss: 0.007183106616139412\n",
            " iteration: 8704  loss: 0.00718274712562561\n",
            " iteration: 8705  loss: 0.007182503119111061\n",
            " iteration: 8706  loss: 0.007182327099144459\n",
            " iteration: 8707  loss: 0.007183489389717579\n",
            " iteration: 8708  loss: 0.007182269357144833\n",
            " iteration: 8709  loss: 0.007182105910032988\n",
            " iteration: 8710  loss: 0.007181845139712095\n",
            " iteration: 8711  loss: 0.007181557361036539\n",
            " iteration: 8712  loss: 0.007181436289101839\n",
            " iteration: 8713  loss: 0.007181313820183277\n",
            " iteration: 8714  loss: 0.007181140594184399\n",
            " iteration: 8715  loss: 0.007180976681411266\n",
            " iteration: 8716  loss: 0.007180781569331884\n",
            " iteration: 8717  loss: 0.007180597633123398\n",
            " iteration: 8718  loss: 0.007180483546108007\n",
            " iteration: 8719  loss: 0.007180285174399614\n",
            " iteration: 8720  loss: 0.007180094253271818\n",
            " iteration: 8721  loss: 0.007179868873208761\n",
            " iteration: 8722  loss: 0.007179675158113241\n",
            " iteration: 8723  loss: 0.007179574575275183\n",
            " iteration: 8724  loss: 0.007179426960647106\n",
            " iteration: 8725  loss: 0.00717935711145401\n",
            " iteration: 8726  loss: 0.007179310079663992\n",
            " iteration: 8727  loss: 0.007179573643952608\n",
            " iteration: 8728  loss: 0.00717927934601903\n",
            " iteration: 8729  loss: 0.00717922393232584\n",
            " iteration: 8730  loss: 0.007179168052971363\n",
            " iteration: 8731  loss: 0.007179069332778454\n",
            " iteration: 8732  loss: 0.00717887794598937\n",
            " iteration: 8733  loss: 0.007178743369877338\n",
            " iteration: 8734  loss: 0.007178646512329578\n",
            " iteration: 8735  loss: 0.007178583648055792\n",
            " iteration: 8736  loss: 0.007178514264523983\n",
            " iteration: 8737  loss: 0.007178446743637323\n",
            " iteration: 8738  loss: 0.0071783484891057014\n",
            " iteration: 8739  loss: 0.00717843696475029\n",
            " iteration: 8740  loss: 0.007178322412073612\n",
            " iteration: 8741  loss: 0.0071782637387514114\n",
            " iteration: 8742  loss: 0.007178173400461674\n",
            " iteration: 8743  loss: 0.007178098428994417\n",
            " iteration: 8744  loss: 0.007177942898124456\n",
            " iteration: 8745  loss: 0.007177752908319235\n",
            " iteration: 8746  loss: 0.007177647203207016\n",
            " iteration: 8747  loss: 0.007177509367465973\n",
            " iteration: 8748  loss: 0.007177450228482485\n",
            " iteration: 8749  loss: 0.007177336141467094\n",
            " iteration: 8750  loss: 0.007177166175097227\n",
            " iteration: 8751  loss: 0.007177441846579313\n",
            " iteration: 8752  loss: 0.007177078165113926\n",
            " iteration: 8753  loss: 0.007176924031227827\n",
            " iteration: 8754  loss: 0.007176851853728294\n",
            " iteration: 8755  loss: 0.007176768966019154\n",
            " iteration: 8756  loss: 0.0071767051704227924\n",
            " iteration: 8757  loss: 0.007176590617746115\n",
            " iteration: 8758  loss: 0.007176603656262159\n",
            " iteration: 8759  loss: 0.007176512852311134\n",
            " iteration: 8760  loss: 0.007176424376666546\n",
            " iteration: 8761  loss: 0.007176332641392946\n",
            " iteration: 8762  loss: 0.007176282349973917\n",
            " iteration: 8763  loss: 0.0071761407889425755\n",
            " iteration: 8764  loss: 0.00717600341886282\n",
            " iteration: 8765  loss: 0.007175794802606106\n",
            " iteration: 8766  loss: 0.007175741251558065\n",
            " iteration: 8767  loss: 0.0071756886318326\n",
            " iteration: 8768  loss: 0.0071756052784621716\n",
            " iteration: 8769  loss: 0.007175519596785307\n",
            " iteration: 8770  loss: 0.007175436243414879\n",
            " iteration: 8771  loss: 0.007175347302109003\n",
            " iteration: 8772  loss: 0.0071752239018678665\n",
            " iteration: 8773  loss: 0.007175029255449772\n",
            " iteration: 8774  loss: 0.007175765000283718\n",
            " iteration: 8775  loss: 0.007174989208579063\n",
            " iteration: 8776  loss: 0.007174819242209196\n",
            " iteration: 8777  loss: 0.007174634840339422\n",
            " iteration: 8778  loss: 0.007174457423388958\n",
            " iteration: 8779  loss: 0.007174273487180471\n",
            " iteration: 8780  loss: 0.007174170576035976\n",
            " iteration: 8781  loss: 0.0071741132996976376\n",
            " iteration: 8782  loss: 0.007174049969762564\n",
            " iteration: 8783  loss: 0.00717397453263402\n",
            " iteration: 8784  loss: 0.007173900958150625\n",
            " iteration: 8785  loss: 0.007175699342042208\n",
            " iteration: 8786  loss: 0.007173880469053984\n",
            " iteration: 8787  loss: 0.007173814345151186\n",
            " iteration: 8788  loss: 0.007173719350248575\n",
            " iteration: 8789  loss: 0.007173618301749229\n",
            " iteration: 8790  loss: 0.007173517718911171\n",
            " iteration: 8791  loss: 0.0071733747608959675\n",
            " iteration: 8792  loss: 0.007173245772719383\n",
            " iteration: 8793  loss: 0.007173073478043079\n",
            " iteration: 8794  loss: 0.007172988262027502\n",
            " iteration: 8795  loss: 0.007173216436058283\n",
            " iteration: 8796  loss: 0.0071729435585439205\n",
            " iteration: 8797  loss: 0.007172847632318735\n",
            " iteration: 8798  loss: 0.007172714918851852\n",
            " iteration: 8799  loss: 0.007172577083110809\n",
            " iteration: 8800  loss: 0.007172402460128069\n",
            " iteration: 8801  loss: 0.007172131910920143\n",
            " iteration: 8802  loss: 0.00717194564640522\n",
            " iteration: 8803  loss: 0.007171745412051678\n",
            " iteration: 8804  loss: 0.007171629928052425\n",
            " iteration: 8805  loss: 0.007171597797423601\n",
            " iteration: 8806  loss: 0.007171473931521177\n",
            " iteration: 8807  loss: 0.007171384058892727\n",
            " iteration: 8808  loss: 0.007171204313635826\n",
            " iteration: 8809  loss: 0.007171119563281536\n",
            " iteration: 8810  loss: 0.0071710520423948765\n",
            " iteration: 8811  loss: 0.007170993834733963\n",
            " iteration: 8812  loss: 0.007170855533331633\n",
            " iteration: 8813  loss: 0.007170873694121838\n",
            " iteration: 8814  loss: 0.007170804776251316\n",
            " iteration: 8815  loss: 0.007170660421252251\n",
            " iteration: 8816  loss: 0.007170443423092365\n",
            " iteration: 8817  loss: 0.007170314434915781\n",
            " iteration: 8818  loss: 0.00717030419036746\n",
            " iteration: 8819  loss: 0.0071702743880450726\n",
            " iteration: 8820  loss: 0.007170191965997219\n",
            " iteration: 8821  loss: 0.007170495577156544\n",
            " iteration: 8822  loss: 0.007170115131884813\n",
            " iteration: 8823  loss: 0.007169999182224274\n",
            " iteration: 8824  loss: 0.0071699125692248344\n",
            " iteration: 8825  loss: 0.007169774267822504\n",
            " iteration: 8826  loss: 0.007169680204242468\n",
            " iteration: 8827  loss: 0.0071695726364851\n",
            " iteration: 8828  loss: 0.007169405464082956\n",
            " iteration: 8829  loss: 0.007169260177761316\n",
            " iteration: 8830  loss: 0.007168996147811413\n",
            " iteration: 8831  loss: 0.0071688382886350155\n",
            " iteration: 8832  loss: 0.007168744690716267\n",
            " iteration: 8833  loss: 0.007168617099523544\n",
            " iteration: 8834  loss: 0.0071684932336211205\n",
            " iteration: 8835  loss: 0.0071682860143482685\n",
            " iteration: 8836  loss: 0.007167949341237545\n",
            " iteration: 8837  loss: 0.007168574724346399\n",
            " iteration: 8838  loss: 0.0071678683161735535\n",
            " iteration: 8839  loss: 0.007167658302932978\n",
            " iteration: 8840  loss: 0.007167560048401356\n",
            " iteration: 8841  loss: 0.007167472969740629\n",
            " iteration: 8842  loss: 0.007167412433773279\n",
            " iteration: 8843  loss: 0.007167257368564606\n",
            " iteration: 8844  loss: 0.0071671330370008945\n",
            " iteration: 8845  loss: 0.007166989613324404\n",
            " iteration: 8846  loss: 0.0071668545715510845\n",
            " iteration: 8847  loss: 0.007166746072471142\n",
            " iteration: 8848  loss: 0.007166668772697449\n",
            " iteration: 8849  loss: 0.00716658728197217\n",
            " iteration: 8850  loss: 0.007166621275246143\n",
            " iteration: 8851  loss: 0.007166517898440361\n",
            " iteration: 8852  loss: 0.007166484370827675\n",
            " iteration: 8853  loss: 0.007166457362473011\n",
            " iteration: 8854  loss: 0.007166410330682993\n",
            " iteration: 8855  loss: 0.007166364695876837\n",
            " iteration: 8856  loss: 0.007166331168264151\n",
            " iteration: 8857  loss: 0.007166264113038778\n",
            " iteration: 8858  loss: 0.007166128605604172\n",
            " iteration: 8859  loss: 0.007166040129959583\n",
            " iteration: 8860  loss: 0.007166152819991112\n",
            " iteration: 8861  loss: 0.007166000083088875\n",
            " iteration: 8862  loss: 0.007165906485170126\n",
            " iteration: 8863  loss: 0.007165758404880762\n",
            " iteration: 8864  loss: 0.0071656666696071625\n",
            " iteration: 8865  loss: 0.007165530230849981\n",
            " iteration: 8866  loss: 0.007165558636188507\n",
            " iteration: 8867  loss: 0.007165480405092239\n",
            " iteration: 8868  loss: 0.007165363058447838\n",
            " iteration: 8869  loss: 0.00716523127630353\n",
            " iteration: 8870  loss: 0.007165155839174986\n",
            " iteration: 8871  loss: 0.007165097631514072\n",
            " iteration: 8872  loss: 0.0071649858728051186\n",
            " iteration: 8873  loss: 0.007164841052144766\n",
            " iteration: 8874  loss: 0.007164635229855776\n",
            " iteration: 8875  loss: 0.007165013812482357\n",
            " iteration: 8876  loss: 0.007164494600147009\n",
            " iteration: 8877  loss: 0.0071642277762293816\n",
            " iteration: 8878  loss: 0.007164078298956156\n",
            " iteration: 8879  loss: 0.007163971662521362\n",
            " iteration: 8880  loss: 0.007163753733038902\n",
            " iteration: 8881  loss: 0.007163950242102146\n",
            " iteration: 8882  loss: 0.007163556292653084\n",
            " iteration: 8883  loss: 0.007163273636251688\n",
            " iteration: 8884  loss: 0.007163061294704676\n",
            " iteration: 8885  loss: 0.007162855006754398\n",
            " iteration: 8886  loss: 0.0071625965647399426\n",
            " iteration: 8887  loss: 0.007162335328757763\n",
            " iteration: 8888  loss: 0.007162123918533325\n",
            " iteration: 8889  loss: 0.007161876652389765\n",
            " iteration: 8890  loss: 0.007161667104810476\n",
            " iteration: 8891  loss: 0.0071614948101341724\n",
            " iteration: 8892  loss: 0.007161283865571022\n",
            " iteration: 8893  loss: 0.0071610333397984505\n",
            " iteration: 8894  loss: 0.007160806562751532\n",
            " iteration: 8895  loss: 0.007161338813602924\n",
            " iteration: 8896  loss: 0.007160751149058342\n",
            " iteration: 8897  loss: 0.007160652428865433\n",
            " iteration: 8898  loss: 0.0071605173870921135\n",
            " iteration: 8899  loss: 0.007160541135817766\n",
            " iteration: 8900  loss: 0.007160418666899204\n",
            " iteration: 8901  loss: 0.007160456385463476\n",
            " iteration: 8902  loss: 0.007160358130931854\n",
            " iteration: 8903  loss: 0.007160294335335493\n",
            " iteration: 8904  loss: 0.007160211447626352\n",
            " iteration: 8905  loss: 0.007160121574997902\n",
            " iteration: 8906  loss: 0.007159864995628595\n",
            " iteration: 8907  loss: 0.007159475237131119\n",
            " iteration: 8908  loss: 0.007164705544710159\n",
            " iteration: 8909  loss: 0.007159339729696512\n",
            " iteration: 8910  loss: 0.007159059401601553\n",
            " iteration: 8911  loss: 0.007158899679780006\n",
            " iteration: 8912  loss: 0.007158845197409391\n",
            " iteration: 8913  loss: 0.007158749271184206\n",
            " iteration: 8914  loss: 0.007158647757023573\n",
            " iteration: 8915  loss: 0.0071585699915885925\n",
            " iteration: 8916  loss: 0.00715849082916975\n",
            " iteration: 8917  loss: 0.007158447057008743\n",
            " iteration: 8918  loss: 0.007158251013606787\n",
            " iteration: 8919  loss: 0.007158111315220594\n",
            " iteration: 8920  loss: 0.00715789245441556\n",
            " iteration: 8921  loss: 0.007157663349062204\n",
            " iteration: 8922  loss: 0.007163449190557003\n",
            " iteration: 8923  loss: 0.007157592568546534\n",
            " iteration: 8924  loss: 0.007157434709370136\n",
            " iteration: 8925  loss: 0.00715732341632247\n",
            " iteration: 8926  loss: 0.007157197687774897\n",
            " iteration: 8927  loss: 0.007157066371291876\n",
            " iteration: 8928  loss: 0.007156917825341225\n",
            " iteration: 8929  loss: 0.007156846579164267\n",
            " iteration: 8930  loss: 0.007156713400036097\n",
            " iteration: 8931  loss: 0.007156613282859325\n",
            " iteration: 8932  loss: 0.007156537380069494\n",
            " iteration: 8933  loss: 0.00715634087100625\n",
            " iteration: 8934  loss: 0.007156512700021267\n",
            " iteration: 8935  loss: 0.0071562472730875015\n",
            " iteration: 8936  loss: 0.007155995815992355\n",
            " iteration: 8937  loss: 0.007155763451009989\n",
            " iteration: 8938  loss: 0.007155639119446278\n",
            " iteration: 8939  loss: 0.007155449129641056\n",
            " iteration: 8940  loss: 0.007155218161642551\n",
            " iteration: 8941  loss: 0.007155062165111303\n",
            " iteration: 8942  loss: 0.007154913153499365\n",
            " iteration: 8943  loss: 0.007154631428420544\n",
            " iteration: 8944  loss: 0.007154362741857767\n",
            " iteration: 8945  loss: 0.0071540758945047855\n",
            " iteration: 8946  loss: 0.00715377414599061\n",
            " iteration: 8947  loss: 0.007153663318604231\n",
            " iteration: 8948  loss: 0.007153033744543791\n",
            " iteration: 8949  loss: 0.007152671925723553\n",
            " iteration: 8950  loss: 0.00715234037488699\n",
            " iteration: 8951  loss: 0.0071518151089549065\n",
            " iteration: 8952  loss: 0.00715293176472187\n",
            " iteration: 8953  loss: 0.007151709869503975\n",
            " iteration: 8954  loss: 0.007151406724005938\n",
            " iteration: 8955  loss: 0.0071512372232973576\n",
            " iteration: 8956  loss: 0.00715099461376667\n",
            " iteration: 8957  loss: 0.007151074707508087\n",
            " iteration: 8958  loss: 0.007150908000767231\n",
            " iteration: 8959  loss: 0.007150759920477867\n",
            " iteration: 8960  loss: 0.007150514051318169\n",
            " iteration: 8961  loss: 0.007150229066610336\n",
            " iteration: 8962  loss: 0.00714985141530633\n",
            " iteration: 8963  loss: 0.0071493592113256454\n",
            " iteration: 8964  loss: 0.007149308454245329\n",
            " iteration: 8965  loss: 0.007148888893425465\n",
            " iteration: 8966  loss: 0.00714880833402276\n",
            " iteration: 8967  loss: 0.007148681674152613\n",
            " iteration: 8968  loss: 0.007148534990847111\n",
            " iteration: 8969  loss: 0.007157425861805677\n",
            " iteration: 8970  loss: 0.007148499134927988\n",
            " iteration: 8971  loss: 0.007148262578994036\n",
            " iteration: 8972  loss: 0.007147956173866987\n",
            " iteration: 8973  loss: 0.0071476418524980545\n",
            " iteration: 8974  loss: 0.0071474588476121426\n",
            " iteration: 8975  loss: 0.007147288415580988\n",
            " iteration: 8976  loss: 0.0071470425464212894\n",
            " iteration: 8977  loss: 0.007146526128053665\n",
            " iteration: 8978  loss: 0.00714599946513772\n",
            " iteration: 8979  loss: 0.00714607210829854\n",
            " iteration: 8980  loss: 0.007145850453525782\n",
            " iteration: 8981  loss: 0.007145657204091549\n",
            " iteration: 8982  loss: 0.007145492359995842\n",
            " iteration: 8983  loss: 0.007145522627979517\n",
            " iteration: 8984  loss: 0.007145340088754892\n",
            " iteration: 8985  loss: 0.00714530423283577\n",
            " iteration: 8986  loss: 0.007144981063902378\n",
            " iteration: 8987  loss: 0.00714479386806488\n",
            " iteration: 8988  loss: 0.007144626695662737\n",
            " iteration: 8989  loss: 0.007145945448428392\n",
            " iteration: 8990  loss: 0.007144500967115164\n",
            " iteration: 8991  loss: 0.007144381292164326\n",
            " iteration: 8992  loss: 0.007144192699342966\n",
            " iteration: 8993  loss: 0.007144015748053789\n",
            " iteration: 8994  loss: 0.0071436199359595776\n",
            " iteration: 8995  loss: 0.007143406663089991\n",
            " iteration: 8996  loss: 0.007143106777220964\n",
            " iteration: 8997  loss: 0.0071428557857871056\n",
            " iteration: 8998  loss: 0.007142676506191492\n",
            " iteration: 8999  loss: 0.007142343558371067\n",
            " iteration: 9000  loss: 0.007141992915421724\n",
            " iteration: 9001  loss: 0.007141691632568836\n",
            " iteration: 9002  loss: 0.007141358684748411\n",
            " iteration: 9003  loss: 0.007141129579395056\n",
            " iteration: 9004  loss: 0.0071408809162676334\n",
            " iteration: 9005  loss: 0.007140717469155788\n",
            " iteration: 9006  loss: 0.007140474859625101\n",
            " iteration: 9007  loss: 0.007140226662158966\n",
            " iteration: 9008  loss: 0.00713998731225729\n",
            " iteration: 9009  loss: 0.007139693945646286\n",
            " iteration: 9010  loss: 0.007139543071389198\n",
            " iteration: 9011  loss: 0.007139456458389759\n",
            " iteration: 9012  loss: 0.007139223627746105\n",
            " iteration: 9013  loss: 0.0071391211822628975\n",
            " iteration: 9014  loss: 0.007139209657907486\n",
            " iteration: 9015  loss: 0.007139058783650398\n",
            " iteration: 9016  loss: 0.007138906512409449\n",
            " iteration: 9017  loss: 0.007138746324926615\n",
            " iteration: 9018  loss: 0.007138359360396862\n",
            " iteration: 9019  loss: 0.007137929555028677\n",
            " iteration: 9020  loss: 0.007137093227356672\n",
            " iteration: 9021  loss: 0.007136390078812838\n",
            " iteration: 9022  loss: 0.007135903928428888\n",
            " iteration: 9023  loss: 0.007135557010769844\n",
            " iteration: 9024  loss: 0.007135334424674511\n",
            " iteration: 9025  loss: 0.007135178428143263\n",
            " iteration: 9026  loss: 0.007135093677788973\n",
            " iteration: 9027  loss: 0.007135022897273302\n",
            " iteration: 9028  loss: 0.00713496096432209\n",
            " iteration: 9029  loss: 0.007134916260838509\n",
            " iteration: 9030  loss: 0.007134838029742241\n",
            " iteration: 9031  loss: 0.007134719751775265\n",
            " iteration: 9032  loss: 0.007134631741791964\n",
            " iteration: 9033  loss: 0.007134473882615566\n",
            " iteration: 9034  loss: 0.007134223822504282\n",
            " iteration: 9035  loss: 0.007134343031793833\n",
            " iteration: 9036  loss: 0.007134090177714825\n",
            " iteration: 9037  loss: 0.007133928127586842\n",
            " iteration: 9038  loss: 0.007133824285119772\n",
            " iteration: 9039  loss: 0.00713370181620121\n",
            " iteration: 9040  loss: 0.007133500184863806\n",
            " iteration: 9041  loss: 0.0071343365125358105\n",
            " iteration: 9042  loss: 0.007133352570235729\n",
            " iteration: 9043  loss: 0.007133180741220713\n",
            " iteration: 9044  loss: 0.007133034057915211\n",
            " iteration: 9045  loss: 0.007132926024496555\n",
            " iteration: 9046  loss: 0.007132748607546091\n",
            " iteration: 9047  loss: 0.007132545113563538\n",
            " iteration: 9048  loss: 0.007132417056709528\n",
            " iteration: 9049  loss: 0.007132318336516619\n",
            " iteration: 9050  loss: 0.0071322061121463776\n",
            " iteration: 9051  loss: 0.007132003083825111\n",
            " iteration: 9052  loss: 0.007131798192858696\n",
            " iteration: 9053  loss: 0.007132409606128931\n",
            " iteration: 9054  loss: 0.007131725549697876\n",
            " iteration: 9055  loss: 0.007131601683795452\n",
            " iteration: 9056  loss: 0.007131478283554316\n",
            " iteration: 9057  loss: 0.007131368387490511\n",
            " iteration: 9058  loss: 0.00713111599907279\n",
            " iteration: 9059  loss: 0.007131005637347698\n",
            " iteration: 9060  loss: 0.0071309651248157024\n",
            " iteration: 9061  loss: 0.0071307336911559105\n",
            " iteration: 9062  loss: 0.007130626123398542\n",
            " iteration: 9063  loss: 0.007130391430109739\n",
            " iteration: 9064  loss: 0.007130144163966179\n",
            " iteration: 9065  loss: 0.0071298470720648766\n",
            " iteration: 9066  loss: 0.007130371406674385\n",
            " iteration: 9067  loss: 0.007129747420549393\n",
            " iteration: 9068  loss: 0.007129570469260216\n",
            " iteration: 9069  loss: 0.007129732519388199\n",
            " iteration: 9070  loss: 0.007129460573196411\n",
            " iteration: 9071  loss: 0.00712937256321311\n",
            " iteration: 9072  loss: 0.00712950062006712\n",
            " iteration: 9073  loss: 0.00712933111935854\n",
            " iteration: 9074  loss: 0.007129264529794455\n",
            " iteration: 9075  loss: 0.007129030302166939\n",
            " iteration: 9076  loss: 0.007128869649022818\n",
            " iteration: 9077  loss: 0.0071286410093307495\n",
            " iteration: 9078  loss: 0.007128502242267132\n",
            " iteration: 9079  loss: 0.007129427511245012\n",
            " iteration: 9080  loss: 0.00712845753878355\n",
            " iteration: 9081  loss: 0.0071283611468970776\n",
            " iteration: 9082  loss: 0.007128266151994467\n",
            " iteration: 9083  loss: 0.007128030527383089\n",
            " iteration: 9084  loss: 0.007128731347620487\n",
            " iteration: 9085  loss: 0.00712798023596406\n",
            " iteration: 9086  loss: 0.007127831690013409\n",
            " iteration: 9087  loss: 0.007127709686756134\n",
            " iteration: 9088  loss: 0.0071276226080954075\n",
            " iteration: 9089  loss: 0.007127474062144756\n",
            " iteration: 9090  loss: 0.007127446588128805\n",
            " iteration: 9091  loss: 0.007127256598323584\n",
            " iteration: 9092  loss: 0.0071271611377596855\n",
            " iteration: 9093  loss: 0.007127056829631329\n",
            " iteration: 9094  loss: 0.007126918528228998\n",
            " iteration: 9095  loss: 0.007126792334020138\n",
            " iteration: 9096  loss: 0.0071265120059251785\n",
            " iteration: 9097  loss: 0.007126400247216225\n",
            " iteration: 9098  loss: 0.0071263303980231285\n",
            " iteration: 9099  loss: 0.007126210257411003\n",
            " iteration: 9100  loss: 0.007126038894057274\n",
            " iteration: 9101  loss: 0.007126577664166689\n",
            " iteration: 9102  loss: 0.007125971373170614\n",
            " iteration: 9103  loss: 0.007125817239284515\n",
            " iteration: 9104  loss: 0.0071254996582865715\n",
            " iteration: 9105  loss: 0.007125264964997768\n",
            " iteration: 9106  loss: 0.007124745287001133\n",
            " iteration: 9107  loss: 0.007124193012714386\n",
            " iteration: 9108  loss: 0.007135527208447456\n",
            " iteration: 9109  loss: 0.007123982068151236\n",
            " iteration: 9110  loss: 0.007123636547476053\n",
            " iteration: 9111  loss: 0.007123376242816448\n",
            " iteration: 9112  loss: 0.007123238872736692\n",
            " iteration: 9113  loss: 0.0071230619214475155\n",
            " iteration: 9114  loss: 0.007122910115867853\n",
            " iteration: 9115  loss: 0.007122677285224199\n",
            " iteration: 9116  loss: 0.007122338749468327\n",
            " iteration: 9117  loss: 0.007122152019292116\n",
            " iteration: 9118  loss: 0.007122098933905363\n",
            " iteration: 9119  loss: 0.007121816277503967\n",
            " iteration: 9120  loss: 0.007121703587472439\n",
            " iteration: 9121  loss: 0.007121491711586714\n",
            " iteration: 9122  loss: 0.00712131941691041\n",
            " iteration: 9123  loss: 0.007121111266314983\n",
            " iteration: 9124  loss: 0.007120934315025806\n",
            " iteration: 9125  loss: 0.007120756432414055\n",
            " iteration: 9126  loss: 0.007120619993656874\n",
            " iteration: 9127  loss: 0.0071204649284482\n",
            " iteration: 9128  loss: 0.007120356895029545\n",
            " iteration: 9129  loss: 0.007120251655578613\n",
            " iteration: 9130  loss: 0.007120137568563223\n",
            " iteration: 9131  loss: 0.007119974121451378\n",
            " iteration: 9132  loss: 0.0071197571232914925\n",
            " iteration: 9133  loss: 0.007119443267583847\n",
            " iteration: 9134  loss: 0.007119322661310434\n",
            " iteration: 9135  loss: 0.0071191927418112755\n",
            " iteration: 9136  loss: 0.00711915735155344\n",
            " iteration: 9137  loss: 0.0071191065944731236\n",
            " iteration: 9138  loss: 0.007119014393538237\n",
            " iteration: 9139  loss: 0.007118895184248686\n",
            " iteration: 9140  loss: 0.0071187871508300304\n",
            " iteration: 9141  loss: 0.007118683774024248\n",
            " iteration: 9142  loss: 0.007118597626686096\n",
            " iteration: 9143  loss: 0.007118491921573877\n",
            " iteration: 9144  loss: 0.0071183135733008385\n",
            " iteration: 9145  loss: 0.007118206936866045\n",
            " iteration: 9146  loss: 0.007118025328963995\n",
            " iteration: 9147  loss: 0.00711794663220644\n",
            " iteration: 9148  loss: 0.007117816712707281\n",
            " iteration: 9149  loss: 0.007117738481611013\n",
            " iteration: 9150  loss: 0.007117596920579672\n",
            " iteration: 9151  loss: 0.007117634639143944\n",
            " iteration: 9152  loss: 0.007117488421499729\n",
            " iteration: 9153  loss: 0.007117386907339096\n",
            " iteration: 9154  loss: 0.007116979453712702\n",
            " iteration: 9155  loss: 0.007116782944649458\n",
            " iteration: 9156  loss: 0.007116571068763733\n",
            " iteration: 9157  loss: 0.007116443011909723\n",
            " iteration: 9158  loss: 0.007116335444152355\n",
            " iteration: 9159  loss: 0.0071162013337016106\n",
            " iteration: 9160  loss: 0.007116028107702732\n",
            " iteration: 9161  loss: 0.0071158818900585175\n",
            " iteration: 9162  loss: 0.0071157230995595455\n",
            " iteration: 9163  loss: 0.007115665823221207\n",
            " iteration: 9164  loss: 0.007115407846868038\n",
            " iteration: 9165  loss: 0.007115262560546398\n",
            " iteration: 9166  loss: 0.0071151284500956535\n",
            " iteration: 9167  loss: 0.007115022744983435\n",
            " iteration: 9168  loss: 0.0071148923598229885\n",
            " iteration: 9169  loss: 0.0071146683767437935\n",
            " iteration: 9170  loss: 0.007114561274647713\n",
            " iteration: 9171  loss: 0.00711438525468111\n",
            " iteration: 9172  loss: 0.007114537991583347\n",
            " iteration: 9173  loss: 0.007114345207810402\n",
            " iteration: 9174  loss: 0.007114221807569265\n",
            " iteration: 9175  loss: 0.007114096079021692\n",
            " iteration: 9176  loss: 0.007113979198038578\n",
            " iteration: 9177  loss: 0.007113763131201267\n",
            " iteration: 9178  loss: 0.007113556377589703\n",
            " iteration: 9179  loss: 0.007113431580364704\n",
            " iteration: 9180  loss: 0.007113216444849968\n",
            " iteration: 9181  loss: 0.0071130720898509026\n",
            " iteration: 9182  loss: 0.0071128071285784245\n",
            " iteration: 9183  loss: 0.007112377323210239\n",
            " iteration: 9184  loss: 0.007112504914402962\n",
            " iteration: 9185  loss: 0.007112159393727779\n",
            " iteration: 9186  loss: 0.00711186695843935\n",
            " iteration: 9187  loss: 0.007111487910151482\n",
            " iteration: 9188  loss: 0.007111293263733387\n",
            " iteration: 9189  loss: 0.007111423648893833\n",
            " iteration: 9190  loss: 0.007111147977411747\n",
            " iteration: 9191  loss: 0.0071109565906226635\n",
            " iteration: 9192  loss: 0.007110816426575184\n",
            " iteration: 9193  loss: 0.007110718637704849\n",
            " iteration: 9194  loss: 0.007110472768545151\n",
            " iteration: 9195  loss: 0.007112216204404831\n",
            " iteration: 9196  loss: 0.00711038988083601\n",
            " iteration: 9197  loss: 0.007110048551112413\n",
            " iteration: 9198  loss: 0.007109775207936764\n",
            " iteration: 9199  loss: 0.007109636906534433\n",
            " iteration: 9200  loss: 0.007109533529728651\n",
            " iteration: 9201  loss: 0.0071093919686973095\n",
            " iteration: 9202  loss: 0.00710922060534358\n",
            " iteration: 9203  loss: 0.007109124679118395\n",
            " iteration: 9204  loss: 0.007108930964022875\n",
            " iteration: 9205  loss: 0.007108866237103939\n",
            " iteration: 9206  loss: 0.007108782418072224\n",
            " iteration: 9207  loss: 0.007108666934072971\n",
            " iteration: 9208  loss: 0.007108555641025305\n",
            " iteration: 9209  loss: 0.0071085006929934025\n",
            " iteration: 9210  loss: 0.0071083917282521725\n",
            " iteration: 9211  loss: 0.0071082753129303455\n",
            " iteration: 9212  loss: 0.007108232472091913\n",
            " iteration: 9213  loss: 0.0071080890484154224\n",
            " iteration: 9214  loss: 0.007108023855835199\n",
            " iteration: 9215  loss: 0.0071079321205616\n",
            " iteration: 9216  loss: 0.007107786368578672\n",
            " iteration: 9217  loss: 0.007107588462531567\n",
            " iteration: 9218  loss: 0.007107507437467575\n",
            " iteration: 9219  loss: 0.007107362616807222\n",
            " iteration: 9220  loss: 0.007107267621904612\n",
            " iteration: 9221  loss: 0.007107078097760677\n",
            " iteration: 9222  loss: 0.007107368670403957\n",
            " iteration: 9223  loss: 0.007106967736035585\n",
            " iteration: 9224  loss: 0.007106689736247063\n",
            " iteration: 9225  loss: 0.007106423377990723\n",
            " iteration: 9226  loss: 0.007106233853846788\n",
            " iteration: 9227  loss: 0.007106015924364328\n",
            " iteration: 9228  loss: 0.007105894386768341\n",
            " iteration: 9229  loss: 0.007105736527591944\n",
            " iteration: 9230  loss: 0.007105540484189987\n",
            " iteration: 9231  loss: 0.007105425465852022\n",
            " iteration: 9232  loss: 0.007105258293449879\n",
            " iteration: 9233  loss: 0.007105099502950907\n",
            " iteration: 9234  loss: 0.007106543518602848\n",
            " iteration: 9235  loss: 0.007105042226612568\n",
            " iteration: 9236  loss: 0.00710489135235548\n",
            " iteration: 9237  loss: 0.007104700431227684\n",
            " iteration: 9238  loss: 0.0071044922806322575\n",
            " iteration: 9239  loss: 0.007104296237230301\n",
            " iteration: 9240  loss: 0.007104048505425453\n",
            " iteration: 9241  loss: 0.007103800307959318\n",
            " iteration: 9242  loss: 0.007103512063622475\n",
            " iteration: 9243  loss: 0.007103392388671637\n",
            " iteration: 9244  loss: 0.007103204261511564\n",
            " iteration: 9245  loss: 0.007103092968463898\n",
            " iteration: 9246  loss: 0.007103512994945049\n",
            " iteration: 9247  loss: 0.007103038020431995\n",
            " iteration: 9248  loss: 0.0071029639802873135\n",
            " iteration: 9249  loss: 0.007102620787918568\n",
            " iteration: 9250  loss: 0.00710243871435523\n",
            " iteration: 9251  loss: 0.007102372124791145\n",
            " iteration: 9252  loss: 0.007102287840098143\n",
            " iteration: 9253  loss: 0.007102221250534058\n",
            " iteration: 9254  loss: 0.007102089934051037\n",
            " iteration: 9255  loss: 0.007101991213858128\n",
            " iteration: 9256  loss: 0.00710183335468173\n",
            " iteration: 9257  loss: 0.007101661060005426\n",
            " iteration: 9258  loss: 0.007101529277861118\n",
            " iteration: 9259  loss: 0.007101401221007109\n",
            " iteration: 9260  loss: 0.007101505994796753\n",
            " iteration: 9261  loss: 0.007101329509168863\n",
            " iteration: 9262  loss: 0.007101164199411869\n",
            " iteration: 9263  loss: 0.007101025432348251\n",
            " iteration: 9264  loss: 0.0071015250869095325\n",
            " iteration: 9265  loss: 0.007100904360413551\n",
            " iteration: 9266  loss: 0.007100772578269243\n",
            " iteration: 9267  loss: 0.007100366055965424\n",
            " iteration: 9268  loss: 0.0071001420728862286\n",
            " iteration: 9269  loss: 0.007099966984242201\n",
            " iteration: 9270  loss: 0.007099892012774944\n",
            " iteration: 9271  loss: 0.007099697832018137\n",
            " iteration: 9272  loss: 0.00709948455914855\n",
            " iteration: 9273  loss: 0.007099300157278776\n",
            " iteration: 9274  loss: 0.007099042180925608\n",
            " iteration: 9275  loss: 0.007098713424056768\n",
            " iteration: 9276  loss: 0.007098362315446138\n",
            " iteration: 9277  loss: 0.007098116911947727\n",
            " iteration: 9278  loss: 0.007097930647432804\n",
            " iteration: 9279  loss: 0.007097805850207806\n",
            " iteration: 9280  loss: 0.007097661029547453\n",
            " iteration: 9281  loss: 0.00709746778011322\n",
            " iteration: 9282  loss: 0.007097287569195032\n",
            " iteration: 9283  loss: 0.007097108289599419\n",
            " iteration: 9284  loss: 0.007096852175891399\n",
            " iteration: 9285  loss: 0.007096598856151104\n",
            " iteration: 9286  loss: 0.007096407003700733\n",
            " iteration: 9287  loss: 0.007096232380717993\n",
            " iteration: 9288  loss: 0.007095955777913332\n",
            " iteration: 9289  loss: 0.007095718756318092\n",
            " iteration: 9290  loss: 0.00709551852196455\n",
            " iteration: 9291  loss: 0.007095370441675186\n",
            " iteration: 9292  loss: 0.007095273118466139\n",
            " iteration: 9293  loss: 0.00709510687738657\n",
            " iteration: 9294  loss: 0.007094953209161758\n",
            " iteration: 9295  loss: 0.007094753440469503\n",
            " iteration: 9296  loss: 0.007094373926520348\n",
            " iteration: 9297  loss: 0.007093990221619606\n",
            " iteration: 9298  loss: 0.007093698717653751\n",
            " iteration: 9299  loss: 0.007093415129929781\n",
            " iteration: 9300  loss: 0.007093195803463459\n",
            " iteration: 9301  loss: 0.007095729932188988\n",
            " iteration: 9302  loss: 0.007093125022947788\n",
            " iteration: 9303  loss: 0.007092855405062437\n",
            " iteration: 9304  loss: 0.007092619780451059\n",
            " iteration: 9305  loss: 0.007092498708516359\n",
            " iteration: 9306  loss: 0.007092304527759552\n",
            " iteration: 9307  loss: 0.0070920418947935104\n",
            " iteration: 9308  loss: 0.007092529442161322\n",
            " iteration: 9309  loss: 0.007091935724020004\n",
            " iteration: 9310  loss: 0.007091747131198645\n",
            " iteration: 9311  loss: 0.007091440726071596\n",
            " iteration: 9312  loss: 0.007091333623975515\n",
            " iteration: 9313  loss: 0.007090993691235781\n",
            " iteration: 9314  loss: 0.007090680301189423\n",
            " iteration: 9315  loss: 0.007091083563864231\n",
            " iteration: 9316  loss: 0.007090559229254723\n",
            " iteration: 9317  loss: 0.007090296596288681\n",
            " iteration: 9318  loss: 0.007090078666806221\n",
            " iteration: 9319  loss: 0.007090392056852579\n",
            " iteration: 9320  loss: 0.0070899780839681625\n",
            " iteration: 9321  loss: 0.007089692167937756\n",
            " iteration: 9322  loss: 0.0070894379168748856\n",
            " iteration: 9323  loss: 0.007089053280651569\n",
            " iteration: 9324  loss: 0.0070888283662498\n",
            " iteration: 9325  loss: 0.007088451646268368\n",
            " iteration: 9326  loss: 0.007088174112141132\n",
            " iteration: 9327  loss: 0.007087993435561657\n",
            " iteration: 9328  loss: 0.0070878989063203335\n",
            " iteration: 9329  loss: 0.007087803445756435\n",
            " iteration: 9330  loss: 0.007087650243192911\n",
            " iteration: 9331  loss: 0.007087521720677614\n",
            " iteration: 9332  loss: 0.007087551523000002\n",
            " iteration: 9333  loss: 0.007087352219969034\n",
            " iteration: 9334  loss: 0.007087188307195902\n",
            " iteration: 9335  loss: 0.007087005767971277\n",
            " iteration: 9336  loss: 0.007086826954036951\n",
            " iteration: 9337  loss: 0.007086579687893391\n",
            " iteration: 9338  loss: 0.007086261175572872\n",
            " iteration: 9339  loss: 0.007085910066962242\n",
            " iteration: 9340  loss: 0.007085721008479595\n",
            " iteration: 9341  loss: 0.007085506804287434\n",
            " iteration: 9342  loss: 0.007085349876433611\n",
            " iteration: 9343  loss: 0.007085214368999004\n",
            " iteration: 9344  loss: 0.007085035555064678\n",
            " iteration: 9345  loss: 0.007084864191710949\n",
            " iteration: 9346  loss: 0.007085077930241823\n",
            " iteration: 9347  loss: 0.007084773853421211\n",
            " iteration: 9348  loss: 0.007084696553647518\n",
            " iteration: 9349  loss: 0.0070846062153577805\n",
            " iteration: 9350  loss: 0.007084551267325878\n",
            " iteration: 9351  loss: 0.007084420882165432\n",
            " iteration: 9352  loss: 0.007084388751536608\n",
            " iteration: 9353  loss: 0.007084111217409372\n",
            " iteration: 9354  loss: 0.007083965465426445\n",
            " iteration: 9355  loss: 0.007083860691636801\n",
            " iteration: 9356  loss: 0.007083771284669638\n",
            " iteration: 9357  loss: 0.007083613891154528\n",
            " iteration: 9358  loss: 0.007083464879542589\n",
            " iteration: 9359  loss: 0.007083765231072903\n",
            " iteration: 9360  loss: 0.007083396427333355\n",
            " iteration: 9361  loss: 0.007083265110850334\n",
            " iteration: 9362  loss: 0.007083123549818993\n",
            " iteration: 9363  loss: 0.007083057425916195\n",
            " iteration: 9364  loss: 0.007082953583449125\n",
            " iteration: 9365  loss: 0.0070827677845954895\n",
            " iteration: 9366  loss: 0.0070830099284648895\n",
            " iteration: 9367  loss: 0.00708265695720911\n",
            " iteration: 9368  loss: 0.007082419004291296\n",
            " iteration: 9369  loss: 0.0070822834968566895\n",
            " iteration: 9370  loss: 0.007082129362970591\n",
            " iteration: 9371  loss: 0.007082018069922924\n",
            " iteration: 9372  loss: 0.007081847172230482\n",
            " iteration: 9373  loss: 0.00708179222419858\n",
            " iteration: 9374  loss: 0.007081609684973955\n",
            " iteration: 9375  loss: 0.007081444840878248\n",
            " iteration: 9376  loss: 0.0070812879130244255\n",
            " iteration: 9377  loss: 0.007081096991896629\n",
            " iteration: 9378  loss: 0.007080927491188049\n",
            " iteration: 9379  loss: 0.007080685347318649\n",
            " iteration: 9380  loss: 0.007080510724335909\n",
            " iteration: 9381  loss: 0.0070801544934511185\n",
            " iteration: 9382  loss: 0.007080015726387501\n",
            " iteration: 9383  loss: 0.007079788949340582\n",
            " iteration: 9384  loss: 0.007079641334712505\n",
            " iteration: 9385  loss: 0.007079377770423889\n",
            " iteration: 9386  loss: 0.007079046219587326\n",
            " iteration: 9387  loss: 0.007078766357153654\n",
            " iteration: 9388  loss: 0.007078377529978752\n",
            " iteration: 9389  loss: 0.0070781707763671875\n",
            " iteration: 9390  loss: 0.007077917922288179\n",
            " iteration: 9391  loss: 0.007078163325786591\n",
            " iteration: 9392  loss: 0.00707778986543417\n",
            " iteration: 9393  loss: 0.007077556569129229\n",
            " iteration: 9394  loss: 0.007077206391841173\n",
            " iteration: 9395  loss: 0.007077112328261137\n",
            " iteration: 9396  loss: 0.00707694748416543\n",
            " iteration: 9397  loss: 0.007076740264892578\n",
            " iteration: 9398  loss: 0.007076682057231665\n",
            " iteration: 9399  loss: 0.007076304405927658\n",
            " iteration: 9400  loss: 0.0070761083625257015\n",
            " iteration: 9401  loss: 0.007075748406350613\n",
            " iteration: 9402  loss: 0.007075587287545204\n",
            " iteration: 9403  loss: 0.007074845489114523\n",
            " iteration: 9404  loss: 0.007074506022036076\n",
            " iteration: 9405  loss: 0.007074176333844662\n",
            " iteration: 9406  loss: 0.007073982618749142\n",
            " iteration: 9407  loss: 0.007073500193655491\n",
            " iteration: 9408  loss: 0.0070738899521529675\n",
            " iteration: 9409  loss: 0.0070733362808823586\n",
            " iteration: 9410  loss: 0.007073115557432175\n",
            " iteration: 9411  loss: 0.00707300053909421\n",
            " iteration: 9412  loss: 0.0070729306899011135\n",
            " iteration: 9413  loss: 0.007072753272950649\n",
            " iteration: 9414  loss: 0.0070724873803555965\n",
            " iteration: 9415  loss: 0.0070724450051784515\n",
            " iteration: 9416  loss: 0.007072117179632187\n",
            " iteration: 9417  loss: 0.007072019390761852\n",
            " iteration: 9418  loss: 0.0070718638598918915\n",
            " iteration: 9419  loss: 0.0070718927308917046\n",
            " iteration: 9420  loss: 0.007071756292134523\n",
            " iteration: 9421  loss: 0.007071601692587137\n",
            " iteration: 9422  loss: 0.007071532309055328\n",
            " iteration: 9423  loss: 0.0070712389424443245\n",
            " iteration: 9424  loss: 0.00707117049023509\n",
            " iteration: 9425  loss: 0.007070957217365503\n",
            " iteration: 9426  loss: 0.0070707225240767\n",
            " iteration: 9427  loss: 0.007070521358400583\n",
            " iteration: 9428  loss: 0.007070405408740044\n",
            " iteration: 9429  loss: 0.007070290856063366\n",
            " iteration: 9430  loss: 0.007070186082273722\n",
            " iteration: 9431  loss: 0.007070023100823164\n",
            " iteration: 9432  loss: 0.007069919724017382\n",
            " iteration: 9433  loss: 0.007069752085953951\n",
            " iteration: 9434  loss: 0.0070696547627449036\n",
            " iteration: 9435  loss: 0.007069534622132778\n",
            " iteration: 9436  loss: 0.007069303188472986\n",
            " iteration: 9437  loss: 0.007068974431604147\n",
            " iteration: 9438  loss: 0.0070684137754142284\n",
            " iteration: 9439  loss: 0.007068029139190912\n",
            " iteration: 9440  loss: 0.007067861966788769\n",
            " iteration: 9441  loss: 0.007067745551466942\n",
            " iteration: 9442  loss: 0.007067580707371235\n",
            " iteration: 9443  loss: 0.0070673259906470776\n",
            " iteration: 9444  loss: 0.007066940423101187\n",
            " iteration: 9445  loss: 0.007067989557981491\n",
            " iteration: 9446  loss: 0.007066845428198576\n",
            " iteration: 9447  loss: 0.007066568359732628\n",
            " iteration: 9448  loss: 0.007066425401717424\n",
            " iteration: 9449  loss: 0.00706636905670166\n",
            " iteration: 9450  loss: 0.007066294550895691\n",
            " iteration: 9451  loss: 0.0070661879144608974\n",
            " iteration: 9452  loss: 0.007066170684993267\n",
            " iteration: 9453  loss: 0.0070665148086845875\n",
            " iteration: 9454  loss: 0.007066019345074892\n",
            " iteration: 9455  loss: 0.007065854966640472\n",
            " iteration: 9456  loss: 0.007065786048769951\n",
            " iteration: 9457  loss: 0.007065747864544392\n",
            " iteration: 9458  loss: 0.007065712474286556\n",
            " iteration: 9459  loss: 0.007065660320222378\n",
            " iteration: 9460  loss: 0.007065559737384319\n",
            " iteration: 9461  loss: 0.007065455429255962\n",
            " iteration: 9462  loss: 0.007065362762659788\n",
            " iteration: 9463  loss: 0.007065261248499155\n",
            " iteration: 9464  loss: 0.0070651825517416\n",
            " iteration: 9465  loss: 0.007065019104629755\n",
            " iteration: 9466  loss: 0.00706485053524375\n",
            " iteration: 9467  loss: 0.00706459628418088\n",
            " iteration: 9468  loss: 0.007064417935907841\n",
            " iteration: 9469  loss: 0.0070643299259245396\n",
            " iteration: 9470  loss: 0.0070643918588757515\n",
            " iteration: 9471  loss: 0.007064268924295902\n",
            " iteration: 9472  loss: 0.007064150180667639\n",
            " iteration: 9473  loss: 0.007064041681587696\n",
            " iteration: 9474  loss: 0.007063789293169975\n",
            " iteration: 9475  loss: 0.00706368638202548\n",
            " iteration: 9476  loss: 0.007064261939376593\n",
            " iteration: 9477  loss: 0.007063621189445257\n",
            " iteration: 9478  loss: 0.007063515018671751\n",
            " iteration: 9479  loss: 0.007063401397317648\n",
            " iteration: 9480  loss: 0.007063277065753937\n",
            " iteration: 9481  loss: 0.007063149008899927\n",
            " iteration: 9482  loss: 0.007063000928610563\n",
            " iteration: 9483  loss: 0.007062844000756741\n",
            " iteration: 9484  loss: 0.007062703836709261\n",
            " iteration: 9485  loss: 0.00706257252022624\n",
            " iteration: 9486  loss: 0.00706241000443697\n",
            " iteration: 9487  loss: 0.007062166929244995\n",
            " iteration: 9488  loss: 0.0070631178095936775\n",
            " iteration: 9489  loss: 0.007062101271003485\n",
            " iteration: 9490  loss: 0.0070619587786495686\n",
            " iteration: 9491  loss: 0.007061806507408619\n",
            " iteration: 9492  loss: 0.007061707321554422\n",
            " iteration: 9493  loss: 0.007061630953103304\n",
            " iteration: 9494  loss: 0.007061495911329985\n",
            " iteration: 9495  loss: 0.007061345968395472\n",
            " iteration: 9496  loss: 0.0070611415430903435\n",
            " iteration: 9497  loss: 0.007060826290398836\n",
            " iteration: 9498  loss: 0.007060627918690443\n",
            " iteration: 9499  loss: 0.007060476578772068\n",
            " iteration: 9500  loss: 0.007060437463223934\n",
            " iteration: 9501  loss: 0.007060172036290169\n",
            " iteration: 9502  loss: 0.007059948984533548\n",
            " iteration: 9503  loss: 0.007059739902615547\n",
            " iteration: 9504  loss: 0.007059645839035511\n",
            " iteration: 9505  loss: 0.007059554103761911\n",
            " iteration: 9506  loss: 0.007059485651552677\n",
            " iteration: 9507  loss: 0.007059417199343443\n",
            " iteration: 9508  loss: 0.007059263065457344\n",
            " iteration: 9509  loss: 0.007059103809297085\n",
            " iteration: 9510  loss: 0.007062068674713373\n",
            " iteration: 9511  loss: 0.007058949209749699\n",
            " iteration: 9512  loss: 0.0070587084628641605\n",
            " iteration: 9513  loss: 0.007058332674205303\n",
            " iteration: 9514  loss: 0.00705815339460969\n",
            " iteration: 9515  loss: 0.007058000657707453\n",
            " iteration: 9516  loss: 0.007057828828692436\n",
            " iteration: 9517  loss: 0.007057760842144489\n",
            " iteration: 9518  loss: 0.007057609036564827\n",
            " iteration: 9519  loss: 0.007057941984385252\n",
            " iteration: 9520  loss: 0.007057564333081245\n",
            " iteration: 9521  loss: 0.007057475857436657\n",
            " iteration: 9522  loss: 0.0070573072880506516\n",
            " iteration: 9523  loss: 0.007057171314954758\n",
            " iteration: 9524  loss: 0.007056995294988155\n",
            " iteration: 9525  loss: 0.00705721927806735\n",
            " iteration: 9526  loss: 0.007056886330246925\n",
            " iteration: 9527  loss: 0.007056771777570248\n",
            " iteration: 9528  loss: 0.0070566339418292046\n",
            " iteration: 9529  loss: 0.0070565491914749146\n",
            " iteration: 9530  loss: 0.007056370377540588\n",
            " iteration: 9531  loss: 0.007056310772895813\n",
            " iteration: 9532  loss: 0.007056196220219135\n",
            " iteration: 9533  loss: 0.00705606397241354\n",
            " iteration: 9534  loss: 0.0070558395236730576\n",
            " iteration: 9535  loss: 0.0070555186830461025\n",
            " iteration: 9536  loss: 0.0070558227598667145\n",
            " iteration: 9537  loss: 0.0070553915575146675\n",
            " iteration: 9538  loss: 0.007055086549371481\n",
            " iteration: 9539  loss: 0.007054951507598162\n",
            " iteration: 9540  loss: 0.007054772228002548\n",
            " iteration: 9541  loss: 0.0070546348579227924\n",
            " iteration: 9542  loss: 0.007054968737065792\n",
            " iteration: 9543  loss: 0.007054524961858988\n",
            " iteration: 9544  loss: 0.007054292131215334\n",
            " iteration: 9545  loss: 0.007054129149764776\n",
            " iteration: 9546  loss: 0.007053884211927652\n",
            " iteration: 9547  loss: 0.007053588982671499\n",
            " iteration: 9548  loss: 0.007053405046463013\n",
            " iteration: 9549  loss: 0.007053088862448931\n",
            " iteration: 9550  loss: 0.007052967324852943\n",
            " iteration: 9551  loss: 0.007052882574498653\n",
            " iteration: 9552  loss: 0.007052761968225241\n",
            " iteration: 9553  loss: 0.007052331697195768\n",
            " iteration: 9554  loss: 0.0070518674328923225\n",
            " iteration: 9555  loss: 0.007051491644233465\n",
            " iteration: 9556  loss: 0.007051160559058189\n",
            " iteration: 9557  loss: 0.007050747517496347\n",
            " iteration: 9558  loss: 0.00705036660656333\n",
            " iteration: 9559  loss: 0.007049938198179007\n",
            " iteration: 9560  loss: 0.007049565203487873\n",
            " iteration: 9561  loss: 0.007049311883747578\n",
            " iteration: 9562  loss: 0.007049123290926218\n",
            " iteration: 9563  loss: 0.007048896513879299\n",
            " iteration: 9564  loss: 0.007048710249364376\n",
            " iteration: 9565  loss: 0.007048622239381075\n",
            " iteration: 9566  loss: 0.007048488594591618\n",
            " iteration: 9567  loss: 0.007048320025205612\n",
            " iteration: 9568  loss: 0.007048253435641527\n",
            " iteration: 9569  loss: 0.007048171944916248\n",
            " iteration: 9570  loss: 0.007048127707093954\n",
            " iteration: 9571  loss: 0.007047957740724087\n",
            " iteration: 9572  loss: 0.0070477137342095375\n",
            " iteration: 9573  loss: 0.007047134451568127\n",
            " iteration: 9574  loss: 0.007047204300761223\n",
            " iteration: 9575  loss: 0.007046896032989025\n",
            " iteration: 9576  loss: 0.0070466045290231705\n",
            " iteration: 9577  loss: 0.007046261336654425\n",
            " iteration: 9578  loss: 0.00704601313918829\n",
            " iteration: 9579  loss: 0.00704575190320611\n",
            " iteration: 9580  loss: 0.00704537145793438\n",
            " iteration: 9581  loss: 0.007044875994324684\n",
            " iteration: 9582  loss: 0.007044742815196514\n",
            " iteration: 9583  loss: 0.007044666912406683\n",
            " iteration: 9584  loss: 0.007044561672955751\n",
            " iteration: 9585  loss: 0.0070444196462631226\n",
            " iteration: 9586  loss: 0.00704421941190958\n",
            " iteration: 9587  loss: 0.007044077385216951\n",
            " iteration: 9588  loss: 0.007043879013508558\n",
            " iteration: 9589  loss: 0.00704368157312274\n",
            " iteration: 9590  loss: 0.007043472956866026\n",
            " iteration: 9591  loss: 0.007043272722512484\n",
            " iteration: 9592  loss: 0.007043135818094015\n",
            " iteration: 9593  loss: 0.007043017074465752\n",
            " iteration: 9594  loss: 0.007042888086289167\n",
            " iteration: 9595  loss: 0.007042729761451483\n",
            " iteration: 9596  loss: 0.007042495999485254\n",
            " iteration: 9597  loss: 0.007042241748422384\n",
            " iteration: 9598  loss: 0.007042056880891323\n",
            " iteration: 9599  loss: 0.007041802629828453\n",
            " iteration: 9600  loss: 0.007041656877845526\n",
            " iteration: 9601  loss: 0.007041352801024914\n",
            " iteration: 9602  loss: 0.007041198201477528\n",
            " iteration: 9603  loss: 0.00704110087826848\n",
            " iteration: 9604  loss: 0.007040815893560648\n",
            " iteration: 9605  loss: 0.007040699012577534\n",
            " iteration: 9606  loss: 0.007040282711386681\n",
            " iteration: 9607  loss: 0.007039948366582394\n",
            " iteration: 9608  loss: 0.007039507385343313\n",
            " iteration: 9609  loss: 0.007039027754217386\n",
            " iteration: 9610  loss: 0.0070388903841376305\n",
            " iteration: 9611  loss: 0.007038744632154703\n",
            " iteration: 9612  loss: 0.007038578391075134\n",
            " iteration: 9613  loss: 0.0070385197177529335\n",
            " iteration: 9614  loss: 0.007038418669253588\n",
            " iteration: 9615  loss: 0.007038327865302563\n",
            " iteration: 9616  loss: 0.007038984913378954\n",
            " iteration: 9617  loss: 0.007038299459964037\n",
            " iteration: 9618  loss: 0.007038237061351538\n",
            " iteration: 9619  loss: 0.007038190960884094\n",
            " iteration: 9620  loss: 0.0070381080731749535\n",
            " iteration: 9621  loss: 0.0070379795506596565\n",
            " iteration: 9622  loss: 0.007037811912596226\n",
            " iteration: 9623  loss: 0.007037839852273464\n",
            " iteration: 9624  loss: 0.007037711329758167\n",
            " iteration: 9625  loss: 0.007037610746920109\n",
            " iteration: 9626  loss: 0.007037525996565819\n",
            " iteration: 9627  loss: 0.007037370465695858\n",
            " iteration: 9628  loss: 0.0070371381007134914\n",
            " iteration: 9629  loss: 0.007036988623440266\n",
            " iteration: 9630  loss: 0.007036630064249039\n",
            " iteration: 9631  loss: 0.007036459166556597\n",
            " iteration: 9632  loss: 0.007036203984171152\n",
            " iteration: 9633  loss: 0.007036034483462572\n",
            " iteration: 9634  loss: 0.007035689894109964\n",
            " iteration: 9635  loss: 0.007037729024887085\n",
            " iteration: 9636  loss: 0.007035630755126476\n",
            " iteration: 9637  loss: 0.007035433314740658\n",
            " iteration: 9638  loss: 0.0070352316834032536\n",
            " iteration: 9639  loss: 0.007035097107291222\n",
            " iteration: 9640  loss: 0.007034953683614731\n",
            " iteration: 9641  loss: 0.007034812122583389\n",
            " iteration: 9642  loss: 0.007034627720713615\n",
            " iteration: 9643  loss: 0.007034390699118376\n",
            " iteration: 9644  loss: 0.00703419279307127\n",
            " iteration: 9645  loss: 0.00703394552692771\n",
            " iteration: 9646  loss: 0.007033877074718475\n",
            " iteration: 9647  loss: 0.00703347148373723\n",
            " iteration: 9648  loss: 0.007033214904367924\n",
            " iteration: 9649  loss: 0.0070329150184988976\n",
            " iteration: 9650  loss: 0.007032555993646383\n",
            " iteration: 9651  loss: 0.00703219510614872\n",
            " iteration: 9652  loss: 0.00703198928385973\n",
            " iteration: 9653  loss: 0.007031713612377644\n",
            " iteration: 9654  loss: 0.007031516637653112\n",
            " iteration: 9655  loss: 0.007031301036477089\n",
            " iteration: 9656  loss: 0.007030996959656477\n",
            " iteration: 9657  loss: 0.00703082513064146\n",
            " iteration: 9658  loss: 0.0070305271074175835\n",
            " iteration: 9659  loss: 0.007030334789305925\n",
            " iteration: 9660  loss: 0.007030264008790255\n",
            " iteration: 9661  loss: 0.00703004002571106\n",
            " iteration: 9662  loss: 0.007029891014099121\n",
            " iteration: 9663  loss: 0.007029694505035877\n",
            " iteration: 9664  loss: 0.007029490079730749\n",
            " iteration: 9665  loss: 0.007029533386230469\n",
            " iteration: 9666  loss: 0.007029351312667131\n",
            " iteration: 9667  loss: 0.007029117550700903\n",
            " iteration: 9668  loss: 0.007028897292912006\n",
            " iteration: 9669  loss: 0.0070287068374454975\n",
            " iteration: 9670  loss: 0.0070284828543663025\n",
            " iteration: 9671  loss: 0.00702832592651248\n",
            " iteration: 9672  loss: 0.007028242107480764\n",
            " iteration: 9673  loss: 0.007028145715594292\n",
            " iteration: 9674  loss: 0.007028044667094946\n",
            " iteration: 9675  loss: 0.007027904037386179\n",
            " iteration: 9676  loss: 0.007027703337371349\n",
            " iteration: 9677  loss: 0.00702754408121109\n",
            " iteration: 9678  loss: 0.007027867715805769\n",
            " iteration: 9679  loss: 0.0070274160243570805\n",
            " iteration: 9680  loss: 0.007027162704616785\n",
            " iteration: 9681  loss: 0.007027070503681898\n",
            " iteration: 9682  loss: 0.007026941515505314\n",
            " iteration: 9683  loss: 0.0070268576964735985\n",
            " iteration: 9684  loss: 0.007026704493910074\n",
            " iteration: 9685  loss: 0.007026533596217632\n",
            " iteration: 9686  loss: 0.007026382256299257\n",
            " iteration: 9687  loss: 0.007026940118521452\n",
            " iteration: 9688  loss: 0.007026335224509239\n",
            " iteration: 9689  loss: 0.007026258390396833\n",
            " iteration: 9690  loss: 0.007026076316833496\n",
            " iteration: 9691  loss: 0.007027317304164171\n",
            " iteration: 9692  loss: 0.007026034872978926\n",
            " iteration: 9693  loss: 0.0070258560590445995\n",
            " iteration: 9694  loss: 0.007025729864835739\n",
            " iteration: 9695  loss: 0.00702563114464283\n",
            " iteration: 9696  loss: 0.007025493774563074\n",
            " iteration: 9697  loss: 0.007025342434644699\n",
            " iteration: 9698  loss: 0.007025128696113825\n",
            " iteration: 9699  loss: 0.007025029044598341\n",
            " iteration: 9700  loss: 0.007024912629276514\n",
            " iteration: 9701  loss: 0.00702473521232605\n",
            " iteration: 9702  loss: 0.007024555001407862\n",
            " iteration: 9703  loss: 0.007024299819022417\n",
            " iteration: 9704  loss: 0.007023864891380072\n",
            " iteration: 9705  loss: 0.007023551035672426\n",
            " iteration: 9706  loss: 0.007023267913609743\n",
            " iteration: 9707  loss: 0.007023148704320192\n",
            " iteration: 9708  loss: 0.007023123558610678\n",
            " iteration: 9709  loss: 0.007023019716143608\n",
            " iteration: 9710  loss: 0.007022961974143982\n",
            " iteration: 9711  loss: 0.007022866513580084\n",
            " iteration: 9712  loss: 0.007022790145128965\n",
            " iteration: 9713  loss: 0.00702278595417738\n",
            " iteration: 9714  loss: 0.007022746838629246\n",
            " iteration: 9715  loss: 0.007022664416581392\n",
            " iteration: 9716  loss: 0.007022509817034006\n",
            " iteration: 9717  loss: 0.007022415287792683\n",
            " iteration: 9718  loss: 0.00702231889590621\n",
            " iteration: 9719  loss: 0.00702202832326293\n",
            " iteration: 9720  loss: 0.007021869998425245\n",
            " iteration: 9721  loss: 0.007021574769169092\n",
            " iteration: 9722  loss: 0.007021392695605755\n",
            " iteration: 9723  loss: 0.007021190132945776\n",
            " iteration: 9724  loss: 0.007020958233624697\n",
            " iteration: 9725  loss: 0.007020814344286919\n",
            " iteration: 9726  loss: 0.007020688150078058\n",
            " iteration: 9727  loss: 0.007020523771643639\n",
            " iteration: 9728  loss: 0.007020394317805767\n",
            " iteration: 9729  loss: 0.007020199671387672\n",
            " iteration: 9730  loss: 0.007020038552582264\n",
            " iteration: 9731  loss: 0.007019813172519207\n",
            " iteration: 9732  loss: 0.007019590120762587\n",
            " iteration: 9733  loss: 0.007019448094069958\n",
            " iteration: 9734  loss: 0.0070192827843129635\n",
            " iteration: 9735  loss: 0.0070190634578466415\n",
            " iteration: 9736  loss: 0.007019861601293087\n",
            " iteration: 9737  loss: 0.007018981501460075\n",
            " iteration: 9738  loss: 0.0070188008248806\n",
            " iteration: 9739  loss: 0.007018431089818478\n",
            " iteration: 9740  loss: 0.007018295116722584\n",
            " iteration: 9741  loss: 0.007018142379820347\n",
            " iteration: 9742  loss: 0.0070179905742406845\n",
            " iteration: 9743  loss: 0.007018113508820534\n",
            " iteration: 9744  loss: 0.00701788580045104\n",
            " iteration: 9745  loss: 0.0070177605375647545\n",
            " iteration: 9746  loss: 0.007017581257969141\n",
            " iteration: 9747  loss: 0.0070173912681639194\n",
            " iteration: 9748  loss: 0.007018956821411848\n",
            " iteration: 9749  loss: 0.007017308846116066\n",
            " iteration: 9750  loss: 0.007017182186245918\n",
            " iteration: 9751  loss: 0.007017096970230341\n",
            " iteration: 9752  loss: 0.0070169828832149506\n",
            " iteration: 9753  loss: 0.007016824558377266\n",
            " iteration: 9754  loss: 0.007016661111265421\n",
            " iteration: 9755  loss: 0.007016433402895927\n",
            " iteration: 9756  loss: 0.007016643416136503\n",
            " iteration: 9757  loss: 0.007016259245574474\n",
            " iteration: 9758  loss: 0.007016113493591547\n",
            " iteration: 9759  loss: 0.007015829440206289\n",
            " iteration: 9760  loss: 0.007015666458755732\n",
            " iteration: 9761  loss: 0.007015426177531481\n",
            " iteration: 9762  loss: 0.007015489973127842\n",
            " iteration: 9763  loss: 0.0070153274573385715\n",
            " iteration: 9764  loss: 0.007015226874500513\n",
            " iteration: 9765  loss: 0.00701512023806572\n",
            " iteration: 9766  loss: 0.007015036419034004\n",
            " iteration: 9767  loss: 0.007014907896518707\n",
            " iteration: 9768  loss: 0.007015199400484562\n",
            " iteration: 9769  loss: 0.0070148371160030365\n",
            " iteration: 9770  loss: 0.007014775648713112\n",
            " iteration: 9771  loss: 0.007014605216681957\n",
            " iteration: 9772  loss: 0.0070144496858119965\n",
            " iteration: 9773  loss: 0.007014263886958361\n",
            " iteration: 9774  loss: 0.007014090660959482\n",
            " iteration: 9775  loss: 0.0070138960145413876\n",
            " iteration: 9776  loss: 0.007013801019638777\n",
            " iteration: 9777  loss: 0.007013713009655476\n",
            " iteration: 9778  loss: 0.007013613358139992\n",
            " iteration: 9779  loss: 0.007013447117060423\n",
            " iteration: 9780  loss: 0.007013413589447737\n",
            " iteration: 9781  loss: 0.0070133330300450325\n",
            " iteration: 9782  loss: 0.007013289257884026\n",
            " iteration: 9783  loss: 0.007013184949755669\n",
            " iteration: 9784  loss: 0.007013050839304924\n",
            " iteration: 9785  loss: 0.007012947462499142\n",
            " iteration: 9786  loss: 0.00701281800866127\n",
            " iteration: 9787  loss: 0.0070126536302268505\n",
            " iteration: 9788  loss: 0.007012553513050079\n",
            " iteration: 9789  loss: 0.00701273325830698\n",
            " iteration: 9790  loss: 0.007012493442744017\n",
            " iteration: 9791  loss: 0.007012431975454092\n",
            " iteration: 9792  loss: 0.007012337911874056\n",
            " iteration: 9793  loss: 0.007012300193309784\n",
            " iteration: 9794  loss: 0.0070122601464390755\n",
            " iteration: 9795  loss: 0.007012181915342808\n",
            " iteration: 9796  loss: 0.007012155372649431\n",
            " iteration: 9797  loss: 0.007012082729488611\n",
            " iteration: 9798  loss: 0.007012033369392157\n",
            " iteration: 9799  loss: 0.007011952809989452\n",
            " iteration: 9800  loss: 0.0070118592120707035\n",
            " iteration: 9801  loss: 0.007012108340859413\n",
            " iteration: 9802  loss: 0.007011819630861282\n",
            " iteration: 9803  loss: 0.007011714857071638\n",
            " iteration: 9804  loss: 0.007011593785136938\n",
            " iteration: 9805  loss: 0.007011519279330969\n",
            " iteration: 9806  loss: 0.007011422887444496\n",
            " iteration: 9807  loss: 0.007011258974671364\n",
            " iteration: 9808  loss: 0.007013325113803148\n",
            " iteration: 9809  loss: 0.007011221721768379\n",
            " iteration: 9810  loss: 0.007011116947978735\n",
            " iteration: 9811  loss: 0.007011066656559706\n",
            " iteration: 9812  loss: 0.007010975852608681\n",
            " iteration: 9813  loss: 0.0070108817890286446\n",
            " iteration: 9814  loss: 0.007010815665125847\n",
            " iteration: 9815  loss: 0.007010738365352154\n",
            " iteration: 9816  loss: 0.007012214045971632\n",
            " iteration: 9817  loss: 0.007010697852820158\n",
            " iteration: 9818  loss: 0.007010564208030701\n",
            " iteration: 9819  loss: 0.007010410074144602\n",
            " iteration: 9820  loss: 0.007010303437709808\n",
            " iteration: 9821  loss: 0.0070101856254041195\n",
            " iteration: 9822  loss: 0.007009990978986025\n",
            " iteration: 9823  loss: 0.0070098526775836945\n",
            " iteration: 9824  loss: 0.0070097921416163445\n",
            " iteration: 9825  loss: 0.007009750697761774\n",
            " iteration: 9826  loss: 0.007009635213762522\n",
            " iteration: 9827  loss: 0.007009543944150209\n",
            " iteration: 9828  loss: 0.007009399123489857\n",
            " iteration: 9829  loss: 0.007009244989603758\n",
            " iteration: 9830  loss: 0.007009072229266167\n",
            " iteration: 9831  loss: 0.007008855231106281\n",
            " iteration: 9832  loss: 0.007008756510913372\n",
            " iteration: 9833  loss: 0.007008624728769064\n",
            " iteration: 9834  loss: 0.00700853206217289\n",
            " iteration: 9835  loss: 0.0070084030739963055\n",
            " iteration: 9836  loss: 0.007008262909948826\n",
            " iteration: 9837  loss: 0.007008082699030638\n",
            " iteration: 9838  loss: 0.0070078736171126366\n",
            " iteration: 9839  loss: 0.007007685489952564\n",
            " iteration: 9840  loss: 0.007007368374615908\n",
            " iteration: 9841  loss: 0.007007102947682142\n",
            " iteration: 9842  loss: 0.007006870582699776\n",
            " iteration: 9843  loss: 0.0070067113265395164\n",
            " iteration: 9844  loss: 0.00700664846226573\n",
            " iteration: 9845  loss: 0.007006584666669369\n",
            " iteration: 9846  loss: 0.0070065464824438095\n",
            " iteration: 9847  loss: 0.007006491534411907\n",
            " iteration: 9848  loss: 0.007006404455751181\n",
            " iteration: 9849  loss: 0.007006216794252396\n",
            " iteration: 9850  loss: 0.0070067597553133965\n",
            " iteration: 9851  loss: 0.007006129715591669\n",
            " iteration: 9852  loss: 0.007005986757576466\n",
            " iteration: 9853  loss: 0.00700609153136611\n",
            " iteration: 9854  loss: 0.007005935534834862\n",
            " iteration: 9855  loss: 0.007005861960351467\n",
            " iteration: 9856  loss: 0.007005751598626375\n",
            " iteration: 9857  loss: 0.007005593273788691\n",
            " iteration: 9858  loss: 0.007005834951996803\n",
            " iteration: 9859  loss: 0.0070055085234344006\n",
            " iteration: 9860  loss: 0.007005360443145037\n",
            " iteration: 9861  loss: 0.007005192805081606\n",
            " iteration: 9862  loss: 0.00700509175658226\n",
            " iteration: 9863  loss: 0.007004946935921907\n",
            " iteration: 9864  loss: 0.0070048002526164055\n",
            " iteration: 9865  loss: 0.007004734128713608\n",
            " iteration: 9866  loss: 0.007004633545875549\n",
            " iteration: 9867  loss: 0.0070045748725533485\n",
            " iteration: 9868  loss: 0.007004445884376764\n",
            " iteration: 9869  loss: 0.007004308048635721\n",
            " iteration: 9870  loss: 0.00700419582426548\n",
            " iteration: 9871  loss: 0.007003978826105595\n",
            " iteration: 9872  loss: 0.00700380140915513\n",
            " iteration: 9873  loss: 0.00700358347967267\n",
            " iteration: 9874  loss: 0.007003381382673979\n",
            " iteration: 9875  loss: 0.007003281265497208\n",
            " iteration: 9876  loss: 0.007003063801676035\n",
            " iteration: 9877  loss: 0.007003158796578646\n",
            " iteration: 9878  loss: 0.007002975791692734\n",
            " iteration: 9879  loss: 0.007002776023000479\n",
            " iteration: 9880  loss: 0.007002576254308224\n",
            " iteration: 9881  loss: 0.007002436555922031\n",
            " iteration: 9882  loss: 0.007002281956374645\n",
            " iteration: 9883  loss: 0.007002264726907015\n",
            " iteration: 9884  loss: 0.007002155762165785\n",
            " iteration: 9885  loss: 0.007001989055424929\n",
            " iteration: 9886  loss: 0.007001806516200304\n",
            " iteration: 9887  loss: 0.007001665886491537\n",
            " iteration: 9888  loss: 0.007001485209912062\n",
            " iteration: 9889  loss: 0.00700171384960413\n",
            " iteration: 9890  loss: 0.0070014153607189655\n",
            " iteration: 9891  loss: 0.0070013245567679405\n",
            " iteration: 9892  loss: 0.0070012654177844524\n",
            " iteration: 9893  loss: 0.007001196965575218\n",
            " iteration: 9894  loss: 0.007001092191785574\n",
            " iteration: 9895  loss: 0.007000922691076994\n",
            " iteration: 9896  loss: 0.007000945042818785\n",
            " iteration: 9897  loss: 0.007000776939094067\n",
            " iteration: 9898  loss: 0.007000645622611046\n",
            " iteration: 9899  loss: 0.007000486366450787\n",
            " iteration: 9900  loss: 0.007000362500548363\n",
            " iteration: 9901  loss: 0.007000202313065529\n",
            " iteration: 9902  loss: 0.0070004952140152454\n",
            " iteration: 9903  loss: 0.007000128272920847\n",
            " iteration: 9904  loss: 0.0070000020787119865\n",
            " iteration: 9905  loss: 0.006999914068728685\n",
            " iteration: 9906  loss: 0.006999830715358257\n",
            " iteration: 9907  loss: 0.006999622564762831\n",
            " iteration: 9908  loss: 0.006999440491199493\n",
            " iteration: 9909  loss: 0.006999334320425987\n",
            " iteration: 9910  loss: 0.0069992695935070515\n",
            " iteration: 9911  loss: 0.006999217439442873\n",
            " iteration: 9912  loss: 0.006999081466346979\n",
            " iteration: 9913  loss: 0.006998794618993998\n",
            " iteration: 9914  loss: 0.0069987899623811245\n",
            " iteration: 9915  loss: 0.006998665165156126\n",
            " iteration: 9916  loss: 0.006998512893915176\n",
            " iteration: 9917  loss: 0.0069983950816094875\n",
            " iteration: 9918  loss: 0.006998256780207157\n",
            " iteration: 9919  loss: 0.006998174358159304\n",
            " iteration: 9920  loss: 0.006998084951192141\n",
            " iteration: 9921  loss: 0.006997924298048019\n",
            " iteration: 9922  loss: 0.006997816730290651\n",
            " iteration: 9923  loss: 0.006997504737228155\n",
            " iteration: 9924  loss: 0.006997228134423494\n",
            " iteration: 9925  loss: 0.0069970628246665\n",
            " iteration: 9926  loss: 0.006997003220021725\n",
            " iteration: 9927  loss: 0.006996945943683386\n",
            " iteration: 9928  loss: 0.006996792275458574\n",
            " iteration: 9929  loss: 0.006996664218604565\n",
            " iteration: 9930  loss: 0.0069964430294930935\n",
            " iteration: 9931  loss: 0.006996287032961845\n",
            " iteration: 9932  loss: 0.006996209733188152\n",
            " iteration: 9933  loss: 0.006995825096964836\n",
            " iteration: 9934  loss: 0.00699612544849515\n",
            " iteration: 9935  loss: 0.00699567561969161\n",
            " iteration: 9936  loss: 0.006995304021984339\n",
            " iteration: 9937  loss: 0.006995141040533781\n",
            " iteration: 9938  loss: 0.006995016243308783\n",
            " iteration: 9939  loss: 0.006994870491325855\n",
            " iteration: 9940  loss: 0.0069947512820363045\n",
            " iteration: 9941  loss: 0.006994397845119238\n",
            " iteration: 9942  loss: 0.006994238588958979\n",
            " iteration: 9943  loss: 0.006994118448346853\n",
            " iteration: 9944  loss: 0.0069939629174768925\n",
            " iteration: 9945  loss: 0.0069936770014464855\n",
            " iteration: 9946  loss: 0.006993344519287348\n",
            " iteration: 9947  loss: 0.006993237417191267\n",
            " iteration: 9948  loss: 0.006992809474468231\n",
            " iteration: 9949  loss: 0.006992689333856106\n",
            " iteration: 9950  loss: 0.006992510054260492\n",
            " iteration: 9951  loss: 0.0069922576658427715\n",
            " iteration: 9952  loss: 0.006991780363023281\n",
            " iteration: 9953  loss: 0.006992774084210396\n",
            " iteration: 9954  loss: 0.006991641130298376\n",
            " iteration: 9955  loss: 0.006991347763687372\n",
            " iteration: 9956  loss: 0.006991144735366106\n",
            " iteration: 9957  loss: 0.006991032510995865\n",
            " iteration: 9958  loss: 0.006990756839513779\n",
            " iteration: 9959  loss: 0.006990573834627867\n",
            " iteration: 9960  loss: 0.006990183610469103\n",
            " iteration: 9961  loss: 0.006989943794906139\n",
            " iteration: 9962  loss: 0.006989757996052504\n",
            " iteration: 9963  loss: 0.006989653687924147\n",
            " iteration: 9964  loss: 0.006989461835473776\n",
            " iteration: 9965  loss: 0.006989319808781147\n",
            " iteration: 9966  loss: 0.006988978944718838\n",
            " iteration: 9967  loss: 0.006988572888076305\n",
            " iteration: 9968  loss: 0.006988204549998045\n",
            " iteration: 9969  loss: 0.006988054607063532\n",
            " iteration: 9970  loss: 0.006987788248807192\n",
            " iteration: 9971  loss: 0.006987603846937418\n",
            " iteration: 9972  loss: 0.006987474858760834\n",
            " iteration: 9973  loss: 0.006987109314650297\n",
            " iteration: 9974  loss: 0.006986977066844702\n",
            " iteration: 9975  loss: 0.006986685562878847\n",
            " iteration: 9976  loss: 0.006986403837800026\n",
            " iteration: 9977  loss: 0.006986135616898537\n",
            " iteration: 9978  loss: 0.006987665314227343\n",
            " iteration: 9979  loss: 0.006985989399254322\n",
            " iteration: 9980  loss: 0.006985808722674847\n",
            " iteration: 9981  loss: 0.006985611282289028\n",
            " iteration: 9982  loss: 0.006985404994338751\n",
            " iteration: 9983  loss: 0.006985193118453026\n",
            " iteration: 9984  loss: 0.006984738167375326\n",
            " iteration: 9985  loss: 0.0069847144186496735\n",
            " iteration: 9986  loss: 0.006984459236264229\n",
            " iteration: 9987  loss: 0.006984377279877663\n",
            " iteration: 9988  loss: 0.006984200328588486\n",
            " iteration: 9989  loss: 0.006987148895859718\n",
            " iteration: 9990  loss: 0.006984149105846882\n",
            " iteration: 9991  loss: 0.006983987055718899\n",
            " iteration: 9992  loss: 0.006983818951994181\n",
            " iteration: 9993  loss: 0.0069835856556892395\n",
            " iteration: 9994  loss: 0.006983316037803888\n",
            " iteration: 9995  loss: 0.006983104627579451\n",
            " iteration: 9996  loss: 0.006982868537306786\n",
            " iteration: 9997  loss: 0.006982744671404362\n",
            " iteration: 9998  loss: 0.006982491351664066\n",
            " iteration: 9999  loss: 0.006982170976698399\n",
            " iteration: 10000  loss: 0.006981947924941778\n",
            " iteration: 10001  loss: 0.006981599610298872\n",
            " iteration: 10002  loss: 0.0069810617715120316\n",
            " iteration: 10003  loss: 0.006981128826737404\n",
            " iteration: 10004  loss: 0.006980956066399813\n",
            " iteration: 10005  loss: 0.006980828940868378\n",
            " iteration: 10006  loss: 0.006980761885643005\n",
            " iteration: 10007  loss: 0.0069807181134819984\n",
            " iteration: 10008  loss: 0.006980609614402056\n",
            " iteration: 10009  loss: 0.006980417761951685\n",
            " iteration: 10010  loss: 0.0069802855141460896\n",
            " iteration: 10011  loss: 0.006980034988373518\n",
            " iteration: 10012  loss: 0.006979899015277624\n",
            " iteration: 10013  loss: 0.00697976304218173\n",
            " iteration: 10014  loss: 0.0069796680472791195\n",
            " iteration: 10015  loss: 0.006979488302022219\n",
            " iteration: 10016  loss: 0.006981407292187214\n",
            " iteration: 10017  loss: 0.006979441735893488\n",
            " iteration: 10018  loss: 0.006979328580200672\n",
            " iteration: 10019  loss: 0.006979266181588173\n",
            " iteration: 10020  loss: 0.006979195401072502\n",
            " iteration: 10021  loss: 0.006979105528444052\n",
            " iteration: 10022  loss: 0.006981384940445423\n",
            " iteration: 10023  loss: 0.006979025900363922\n",
            " iteration: 10024  loss: 0.006978871766477823\n",
            " iteration: 10025  loss: 0.006978638004511595\n",
            " iteration: 10026  loss: 0.006978531368076801\n",
            " iteration: 10027  loss: 0.006978732533752918\n",
            " iteration: 10028  loss: 0.006978396326303482\n",
            " iteration: 10029  loss: 0.006978113669902086\n",
            " iteration: 10030  loss: 0.0069779385812580585\n",
            " iteration: 10031  loss: 0.006977855693548918\n",
            " iteration: 10032  loss: 0.006977748591452837\n",
            " iteration: 10033  loss: 0.0069778612814843655\n",
            " iteration: 10034  loss: 0.006977663841098547\n",
            " iteration: 10035  loss: 0.006977537646889687\n",
            " iteration: 10036  loss: 0.006977366749197245\n",
            " iteration: 10037  loss: 0.0069772969000041485\n",
            " iteration: 10038  loss: 0.006977239158004522\n",
            " iteration: 10039  loss: 0.006977088283747435\n",
            " iteration: 10040  loss: 0.006977010052651167\n",
            " iteration: 10041  loss: 0.0069768428802490234\n",
            " iteration: 10042  loss: 0.006976716220378876\n",
            " iteration: 10043  loss: 0.006981466896831989\n",
            " iteration: 10044  loss: 0.006976664997637272\n",
            " iteration: 10045  loss: 0.006976552307605743\n",
            " iteration: 10046  loss: 0.006976403295993805\n",
            " iteration: 10047  loss: 0.006976151838898659\n",
            " iteration: 10048  loss: 0.006975999101996422\n",
            " iteration: 10049  loss: 0.006975796539336443\n",
            " iteration: 10050  loss: 0.006975619122385979\n",
            " iteration: 10051  loss: 0.006975480820983648\n",
            " iteration: 10052  loss: 0.006975176744163036\n",
            " iteration: 10053  loss: 0.006975788157433271\n",
            " iteration: 10054  loss: 0.006975085940212011\n",
            " iteration: 10055  loss: 0.006974943447858095\n",
            " iteration: 10056  loss: 0.006974758580327034\n",
            " iteration: 10057  loss: 0.006974582094699144\n",
            " iteration: 10058  loss: 0.0069759199395775795\n",
            " iteration: 10059  loss: 0.006974474061280489\n",
            " iteration: 10060  loss: 0.006974298506975174\n",
            " iteration: 10061  loss: 0.006973789073526859\n",
            " iteration: 10062  loss: 0.00697349663823843\n",
            " iteration: 10063  loss: 0.006973299663513899\n",
            " iteration: 10064  loss: 0.006973118055611849\n",
            " iteration: 10065  loss: 0.006972990930080414\n",
            " iteration: 10066  loss: 0.006972851697355509\n",
            " iteration: 10067  loss: 0.006972756702452898\n",
            " iteration: 10068  loss: 0.00697254529222846\n",
            " iteration: 10069  loss: 0.006972358096390963\n",
            " iteration: 10070  loss: 0.006972081493586302\n",
            " iteration: 10071  loss: 0.006972847040742636\n",
            " iteration: 10072  loss: 0.006972017232328653\n",
            " iteration: 10073  loss: 0.006971893832087517\n",
            " iteration: 10074  loss: 0.0069718072190880775\n",
            " iteration: 10075  loss: 0.006971720140427351\n",
            " iteration: 10076  loss: 0.006971622351557016\n",
            " iteration: 10077  loss: 0.0069715166464447975\n",
            " iteration: 10078  loss: 0.0069713592529296875\n",
            " iteration: 10079  loss: 0.006971241440623999\n",
            " iteration: 10080  loss: 0.006971173454076052\n",
            " iteration: 10081  loss: 0.006971108261495829\n",
            " iteration: 10082  loss: 0.006971009075641632\n",
            " iteration: 10083  loss: 0.006970888935029507\n",
            " iteration: 10084  loss: 0.006971241440623999\n",
            " iteration: 10085  loss: 0.006970839109271765\n",
            " iteration: 10086  loss: 0.006970721296966076\n",
            " iteration: 10087  loss: 0.006970538292080164\n",
            " iteration: 10088  loss: 0.006970205344259739\n",
            " iteration: 10089  loss: 0.0069699399173259735\n",
            " iteration: 10090  loss: 0.006969831883907318\n",
            " iteration: 10091  loss: 0.00696971220895648\n",
            " iteration: 10092  loss: 0.006969935726374388\n",
            " iteration: 10093  loss: 0.0069696721620857716\n",
            " iteration: 10094  loss: 0.00696957390755415\n",
            " iteration: 10095  loss: 0.006969497539103031\n",
            " iteration: 10096  loss: 0.0069694509729743\n",
            " iteration: 10097  loss: 0.006969347596168518\n",
            " iteration: 10098  loss: 0.006969282403588295\n",
            " iteration: 10099  loss: 0.006969111505895853\n",
            " iteration: 10100  loss: 0.006968934554606676\n",
            " iteration: 10101  loss: 0.006968644447624683\n",
            " iteration: 10102  loss: 0.006968841888010502\n",
            " iteration: 10103  loss: 0.006968466565012932\n",
            " iteration: 10104  loss: 0.006968324072659016\n",
            " iteration: 10105  loss: 0.006968182045966387\n",
            " iteration: 10106  loss: 0.006968081463128328\n",
            " iteration: 10107  loss: 0.006967843044549227\n",
            " iteration: 10108  loss: 0.006967941764742136\n",
            " iteration: 10109  loss: 0.006967742927372456\n",
            " iteration: 10110  loss: 0.006967631634324789\n",
            " iteration: 10111  loss: 0.006967501249164343\n",
            " iteration: 10112  loss: 0.006967388559132814\n",
            " iteration: 10113  loss: 0.00696743605658412\n",
            " iteration: 10114  loss: 0.006967301946133375\n",
            " iteration: 10115  loss: 0.006967153400182724\n",
            " iteration: 10116  loss: 0.006967001128941774\n",
            " iteration: 10117  loss: 0.0069672418758273125\n",
            " iteration: 10118  loss: 0.006966947112232447\n",
            " iteration: 10119  loss: 0.006966826971620321\n",
            " iteration: 10120  loss: 0.006966696586459875\n",
            " iteration: 10121  loss: 0.006966542452573776\n",
            " iteration: 10122  loss: 0.00696636363863945\n",
            " iteration: 10123  loss: 0.006966055370867252\n",
            " iteration: 10124  loss: 0.0069662644527852535\n",
            " iteration: 10125  loss: 0.006965948734432459\n",
            " iteration: 10126  loss: 0.00696582393720746\n",
            " iteration: 10127  loss: 0.006965726613998413\n",
            " iteration: 10128  loss: 0.006965657230466604\n",
            " iteration: 10129  loss: 0.00696545047685504\n",
            " iteration: 10130  loss: 0.00696530332788825\n",
            " iteration: 10131  loss: 0.006965113338083029\n",
            " iteration: 10132  loss: 0.00696494011208415\n",
            " iteration: 10133  loss: 0.0069647724740207195\n",
            " iteration: 10134  loss: 0.0069645121693611145\n",
            " iteration: 10135  loss: 0.006964374799281359\n",
            " iteration: 10136  loss: 0.006964264437556267\n",
            " iteration: 10137  loss: 0.006964126601815224\n",
            " iteration: 10138  loss: 0.006964016705751419\n",
            " iteration: 10139  loss: 0.006963918451219797\n",
            " iteration: 10140  loss: 0.00696386257186532\n",
            " iteration: 10141  loss: 0.006963803432881832\n",
            " iteration: 10142  loss: 0.006963723339140415\n",
            " iteration: 10143  loss: 0.0069635771214962006\n",
            " iteration: 10144  loss: 0.006963574327528477\n",
            " iteration: 10145  loss: 0.0069635300897061825\n",
            " iteration: 10146  loss: 0.006963439751416445\n",
            " iteration: 10147  loss: 0.006963357329368591\n",
            " iteration: 10148  loss: 0.006963287014514208\n",
            " iteration: 10149  loss: 0.006963124964386225\n",
            " iteration: 10150  loss: 0.006963088642805815\n",
            " iteration: 10151  loss: 0.006962994579225779\n",
            " iteration: 10152  loss: 0.006962963845580816\n",
            " iteration: 10153  loss: 0.006962893530726433\n",
            " iteration: 10154  loss: 0.0069627948105335236\n",
            " iteration: 10155  loss: 0.006962601561099291\n",
            " iteration: 10156  loss: 0.006962667219340801\n",
            " iteration: 10157  loss: 0.006962521933019161\n",
            " iteration: 10158  loss: 0.0069624450989067554\n",
            " iteration: 10159  loss: 0.00696238037198782\n",
            " iteration: 10160  loss: 0.0069623407907783985\n",
            " iteration: 10161  loss: 0.0069622136652469635\n",
            " iteration: 10162  loss: 0.006962453946471214\n",
            " iteration: 10163  loss: 0.006962169427424669\n",
            " iteration: 10164  loss: 0.006962029729038477\n",
            " iteration: 10165  loss: 0.006961837876588106\n",
            " iteration: 10166  loss: 0.006961634382605553\n",
            " iteration: 10167  loss: 0.006961670704185963\n",
            " iteration: 10168  loss: 0.006961568724364042\n",
            " iteration: 10169  loss: 0.0069614555686712265\n",
            " iteration: 10170  loss: 0.006961355451494455\n",
            " iteration: 10171  loss: 0.006961263716220856\n",
            " iteration: 10172  loss: 0.00696113845333457\n",
            " iteration: 10173  loss: 0.006961372680962086\n",
            " iteration: 10174  loss: 0.00696105882525444\n",
            " iteration: 10175  loss: 0.006960962899029255\n",
            " iteration: 10176  loss: 0.006960883736610413\n",
            " iteration: 10177  loss: 0.00696085998788476\n",
            " iteration: 10178  loss: 0.006960810627788305\n",
            " iteration: 10179  loss: 0.006960656028240919\n",
            " iteration: 10180  loss: 0.006960497237741947\n",
            " iteration: 10181  loss: 0.0069603025913238525\n",
            " iteration: 10182  loss: 0.006960343103855848\n",
            " iteration: 10183  loss: 0.006960218772292137\n",
            " iteration: 10184  loss: 0.006960170343518257\n",
            " iteration: 10185  loss: 0.00696001248434186\n",
            " iteration: 10186  loss: 0.0069599151611328125\n",
            " iteration: 10187  loss: 0.006959849502891302\n",
            " iteration: 10188  loss: 0.0069595817476511\n",
            " iteration: 10189  loss: 0.00695950398221612\n",
            " iteration: 10190  loss: 0.00695932749658823\n",
            " iteration: 10191  loss: 0.006959087681025267\n",
            " iteration: 10192  loss: 0.0069588529877364635\n",
            " iteration: 10193  loss: 0.006959269754588604\n",
            " iteration: 10194  loss: 0.0069587272591888905\n",
            " iteration: 10195  loss: 0.006958513054996729\n",
            " iteration: 10196  loss: 0.0069583384320139885\n",
            " iteration: 10197  loss: 0.006958208978176117\n",
            " iteration: 10198  loss: 0.006958214100450277\n",
            " iteration: 10199  loss: 0.006958098616451025\n",
            " iteration: 10200  loss: 0.0069579314440488815\n",
            " iteration: 10201  loss: 0.006957659497857094\n",
            " iteration: 10202  loss: 0.006957482546567917\n",
            " iteration: 10203  loss: 0.006957218982279301\n",
            " iteration: 10204  loss: 0.0069577391259372234\n",
            " iteration: 10205  loss: 0.00695711700245738\n",
            " iteration: 10206  loss: 0.006956865545362234\n",
            " iteration: 10207  loss: 0.006956595461815596\n",
            " iteration: 10208  loss: 0.0069563561119139194\n",
            " iteration: 10209  loss: 0.006956140045076609\n",
            " iteration: 10210  loss: 0.0069563486613333225\n",
            " iteration: 10211  loss: 0.006956089753657579\n",
            " iteration: 10212  loss: 0.006955930031836033\n",
            " iteration: 10213  loss: 0.006955836899578571\n",
            " iteration: 10214  loss: 0.006955604534596205\n",
            " iteration: 10215  loss: 0.0069553726352751255\n",
            " iteration: 10216  loss: 0.006956340745091438\n",
            " iteration: 10217  loss: 0.006955321412533522\n",
            " iteration: 10218  loss: 0.006955102551728487\n",
            " iteration: 10219  loss: 0.006954897195100784\n",
            " iteration: 10220  loss: 0.0069547249004244804\n",
            " iteration: 10221  loss: 0.006954560987651348\n",
            " iteration: 10222  loss: 0.006954587064683437\n",
            " iteration: 10223  loss: 0.0069544208236038685\n",
            " iteration: 10224  loss: 0.006954224314540625\n",
            " iteration: 10225  loss: 0.006954035721719265\n",
            " iteration: 10226  loss: 0.006953902076929808\n",
            " iteration: 10227  loss: 0.0069536068476736546\n",
            " iteration: 10228  loss: 0.00695369066670537\n",
            " iteration: 10229  loss: 0.006953524425625801\n",
            " iteration: 10230  loss: 0.006953401491045952\n",
            " iteration: 10231  loss: 0.006953208241611719\n",
            " iteration: 10232  loss: 0.00695309042930603\n",
            " iteration: 10233  loss: 0.006953148171305656\n",
            " iteration: 10234  loss: 0.006952981930226088\n",
            " iteration: 10235  loss: 0.00695288460701704\n",
            " iteration: 10236  loss: 0.006952861789613962\n",
            " iteration: 10237  loss: 0.006952745374292135\n",
            " iteration: 10238  loss: 0.006952708121389151\n",
            " iteration: 10239  loss: 0.0069526503793895245\n",
            " iteration: 10240  loss: 0.0069526201114058495\n",
            " iteration: 10241  loss: 0.006952566560357809\n",
            " iteration: 10242  loss: 0.006952520925551653\n",
            " iteration: 10243  loss: 0.006952440366148949\n",
            " iteration: 10244  loss: 0.006952340714633465\n",
            " iteration: 10245  loss: 0.006952164229005575\n",
            " iteration: 10246  loss: 0.006952240597456694\n",
            " iteration: 10247  loss: 0.006951913703233004\n",
            " iteration: 10248  loss: 0.006951605901122093\n",
            " iteration: 10249  loss: 0.006951263640075922\n",
            " iteration: 10250  loss: 0.006953759118914604\n",
            " iteration: 10251  loss: 0.006951187737286091\n",
            " iteration: 10252  loss: 0.006951112765818834\n",
            " iteration: 10253  loss: 0.006950809620320797\n",
            " iteration: 10254  loss: 0.00695071741938591\n",
            " iteration: 10255  loss: 0.006950538605451584\n",
            " iteration: 10256  loss: 0.006950333248823881\n",
            " iteration: 10257  loss: 0.006950104143470526\n",
            " iteration: 10258  loss: 0.006949880626052618\n",
            " iteration: 10259  loss: 0.006949683651328087\n",
            " iteration: 10260  loss: 0.0069497860968112946\n",
            " iteration: 10261  loss: 0.006949553266167641\n",
            " iteration: 10262  loss: 0.006949443370103836\n",
            " iteration: 10263  loss: 0.006949229631572962\n",
            " iteration: 10264  loss: 0.006949051283299923\n",
            " iteration: 10265  loss: 0.006948642432689667\n",
            " iteration: 10266  loss: 0.006949290633201599\n",
            " iteration: 10267  loss: 0.006948494352400303\n",
            " iteration: 10268  loss: 0.0069482214748859406\n",
            " iteration: 10269  loss: 0.0069478037767112255\n",
            " iteration: 10270  loss: 0.0069478838704526424\n",
            " iteration: 10271  loss: 0.006947614252567291\n",
            " iteration: 10272  loss: 0.006947243120521307\n",
            " iteration: 10273  loss: 0.0069470577873289585\n",
            " iteration: 10274  loss: 0.006946578621864319\n",
            " iteration: 10275  loss: 0.006946259178221226\n",
            " iteration: 10276  loss: 0.006946126464754343\n",
            " iteration: 10277  loss: 0.0069457245990633965\n",
            " iteration: 10278  loss: 0.006945479195564985\n",
            " iteration: 10279  loss: 0.0069451723247766495\n",
            " iteration: 10280  loss: 0.006944952066987753\n",
            " iteration: 10281  loss: 0.006944756489247084\n",
            " iteration: 10282  loss: 0.0069446442648768425\n",
            " iteration: 10283  loss: 0.006944540422409773\n",
            " iteration: 10284  loss: 0.006944432854652405\n",
            " iteration: 10285  loss: 0.006944271270185709\n",
            " iteration: 10286  loss: 0.0069441162049770355\n",
            " iteration: 10287  loss: 0.006943955086171627\n",
            " iteration: 10288  loss: 0.00694379024207592\n",
            " iteration: 10289  loss: 0.00694353599101305\n",
            " iteration: 10290  loss: 0.0069433050230145454\n",
            " iteration: 10291  loss: 0.006943067070096731\n",
            " iteration: 10292  loss: 0.0069428966380655766\n",
            " iteration: 10293  loss: 0.006942818872630596\n",
            " iteration: 10294  loss: 0.006942519452422857\n",
            " iteration: 10295  loss: 0.006942348554730415\n",
            " iteration: 10296  loss: 0.006942141335457563\n",
            " iteration: 10297  loss: 0.006941984407603741\n",
            " iteration: 10298  loss: 0.006941793020814657\n",
            " iteration: 10299  loss: 0.006941575091332197\n",
            " iteration: 10300  loss: 0.006941377185285091\n",
            " iteration: 10301  loss: 0.0069410353899002075\n",
            " iteration: 10302  loss: 0.006940890569239855\n",
            " iteration: 10303  loss: 0.00694063026458025\n",
            " iteration: 10304  loss: 0.0069404710084199905\n",
            " iteration: 10305  loss: 0.006940254010260105\n",
            " iteration: 10306  loss: 0.0069399187341332436\n",
            " iteration: 10307  loss: 0.006939675193279982\n",
            " iteration: 10308  loss: 0.006939594633877277\n",
            " iteration: 10309  loss: 0.006939289625734091\n",
            " iteration: 10310  loss: 0.006939122453331947\n",
            " iteration: 10311  loss: 0.00693892315030098\n",
            " iteration: 10312  loss: 0.006938626989722252\n",
            " iteration: 10313  loss: 0.006938252132385969\n",
            " iteration: 10314  loss: 0.0069380393251776695\n",
            " iteration: 10315  loss: 0.006937707308679819\n",
            " iteration: 10316  loss: 0.006937464699149132\n",
            " iteration: 10317  loss: 0.006937252823263407\n",
            " iteration: 10318  loss: 0.006937013939023018\n",
            " iteration: 10319  loss: 0.006937079597264528\n",
            " iteration: 10320  loss: 0.006936913821846247\n",
            " iteration: 10321  loss: 0.006936706602573395\n",
            " iteration: 10322  loss: 0.006936542224138975\n",
            " iteration: 10323  loss: 0.006936393678188324\n",
            " iteration: 10324  loss: 0.006936230231076479\n",
            " iteration: 10325  loss: 0.006936720106750727\n",
            " iteration: 10326  loss: 0.006936166901141405\n",
            " iteration: 10327  loss: 0.006936072371900082\n",
            " iteration: 10328  loss: 0.00693601556122303\n",
            " iteration: 10329  loss: 0.0069359466433525085\n",
            " iteration: 10330  loss: 0.00693576131016016\n",
            " iteration: 10331  loss: 0.006935478653758764\n",
            " iteration: 10332  loss: 0.006937059573829174\n",
            " iteration: 10333  loss: 0.006935409735888243\n",
            " iteration: 10334  loss: 0.0069351401180028915\n",
            " iteration: 10335  loss: 0.006935030687600374\n",
            " iteration: 10336  loss: 0.0069349221885204315\n",
            " iteration: 10337  loss: 0.006934725679457188\n",
            " iteration: 10338  loss: 0.006934456527233124\n",
            " iteration: 10339  loss: 0.006934104487299919\n",
            " iteration: 10340  loss: 0.006933852564543486\n",
            " iteration: 10341  loss: 0.006933641619980335\n",
            " iteration: 10342  loss: 0.00693353870883584\n",
            " iteration: 10343  loss: 0.0069333938881754875\n",
            " iteration: 10344  loss: 0.006933238822966814\n",
            " iteration: 10345  loss: 0.006933031138032675\n",
            " iteration: 10346  loss: 0.00693277595564723\n",
            " iteration: 10347  loss: 0.006932342424988747\n",
            " iteration: 10348  loss: 0.006932123564183712\n",
            " iteration: 10349  loss: 0.006931948009878397\n",
            " iteration: 10350  loss: 0.0069318669848144054\n",
            " iteration: 10351  loss: 0.006931670475751162\n",
            " iteration: 10352  loss: 0.00693149957805872\n",
            " iteration: 10353  loss: 0.0069313389249145985\n",
            " iteration: 10354  loss: 0.006931175012141466\n",
            " iteration: 10355  loss: 0.006930857431143522\n",
            " iteration: 10356  loss: 0.006930442992597818\n",
            " iteration: 10357  loss: 0.006931389216333628\n",
            " iteration: 10358  loss: 0.006930209696292877\n",
            " iteration: 10359  loss: 0.006929934024810791\n",
            " iteration: 10360  loss: 0.0069297077134251595\n",
            " iteration: 10361  loss: 0.006929514929652214\n",
            " iteration: 10362  loss: 0.006928842980414629\n",
            " iteration: 10363  loss: 0.006928103975951672\n",
            " iteration: 10364  loss: 0.006927425507456064\n",
            " iteration: 10365  loss: 0.00692681735381484\n",
            " iteration: 10366  loss: 0.0069262925535440445\n",
            " iteration: 10367  loss: 0.006925898138433695\n",
            " iteration: 10368  loss: 0.006925499066710472\n",
            " iteration: 10369  loss: 0.0069249412044882774\n",
            " iteration: 10370  loss: 0.006924637593328953\n",
            " iteration: 10371  loss: 0.006924129091203213\n",
            " iteration: 10372  loss: 0.006923734210431576\n",
            " iteration: 10373  loss: 0.006923262495547533\n",
            " iteration: 10374  loss: 0.006922738626599312\n",
            " iteration: 10375  loss: 0.006922276224941015\n",
            " iteration: 10376  loss: 0.006921944674104452\n",
            " iteration: 10377  loss: 0.0069216471165418625\n",
            " iteration: 10378  loss: 0.006921473890542984\n",
            " iteration: 10379  loss: 0.006921268068253994\n",
            " iteration: 10380  loss: 0.006922530010342598\n",
            " iteration: 10381  loss: 0.006921196356415749\n",
            " iteration: 10382  loss: 0.006921021267771721\n",
            " iteration: 10383  loss: 0.006920557003468275\n",
            " iteration: 10384  loss: 0.006919486913830042\n",
            " iteration: 10385  loss: 0.006918738596141338\n",
            " iteration: 10386  loss: 0.00691866222769022\n",
            " iteration: 10387  loss: 0.006918213330209255\n",
            " iteration: 10388  loss: 0.0069174086675047874\n",
            " iteration: 10389  loss: 0.006916698534041643\n",
            " iteration: 10390  loss: 0.0069157760590314865\n",
            " iteration: 10391  loss: 0.006915138568729162\n",
            " iteration: 10392  loss: 0.006915250327438116\n",
            " iteration: 10393  loss: 0.006914967205375433\n",
            " iteration: 10394  loss: 0.0069146971218287945\n",
            " iteration: 10395  loss: 0.006914311088621616\n",
            " iteration: 10396  loss: 0.006914019118994474\n",
            " iteration: 10397  loss: 0.006916211452335119\n",
            " iteration: 10398  loss: 0.006913784891366959\n",
            " iteration: 10399  loss: 0.006913578137755394\n",
            " iteration: 10400  loss: 0.006913379300385714\n",
            " iteration: 10401  loss: 0.006913200486451387\n",
            " iteration: 10402  loss: 0.006912755314260721\n",
            " iteration: 10403  loss: 0.0069123441353440285\n",
            " iteration: 10404  loss: 0.006911789532750845\n",
            " iteration: 10405  loss: 0.006911496166139841\n",
            " iteration: 10406  loss: 0.00691133551299572\n",
            " iteration: 10407  loss: 0.006911005824804306\n",
            " iteration: 10408  loss: 0.006910700350999832\n",
            " iteration: 10409  loss: 0.006910264026373625\n",
            " iteration: 10410  loss: 0.006909895688295364\n",
            " iteration: 10411  loss: 0.006909589748829603\n",
            " iteration: 10412  loss: 0.0069093466736376286\n",
            " iteration: 10413  loss: 0.006909048184752464\n",
            " iteration: 10414  loss: 0.006908804178237915\n",
            " iteration: 10415  loss: 0.0069084917195141315\n",
            " iteration: 10416  loss: 0.00690820999443531\n",
            " iteration: 10417  loss: 0.00690754409879446\n",
            " iteration: 10418  loss: 0.006907220929861069\n",
            " iteration: 10419  loss: 0.00690695783123374\n",
            " iteration: 10420  loss: 0.006906652823090553\n",
            " iteration: 10421  loss: 0.006906505674123764\n",
            " iteration: 10422  loss: 0.006906226277351379\n",
            " iteration: 10423  loss: 0.0069059147499501705\n",
            " iteration: 10424  loss: 0.006905898451805115\n",
            " iteration: 10425  loss: 0.006905632559210062\n",
            " iteration: 10426  loss: 0.0069053033366799355\n",
            " iteration: 10427  loss: 0.006904729176312685\n",
            " iteration: 10428  loss: 0.006904577370733023\n",
            " iteration: 10429  loss: 0.00690419552847743\n",
            " iteration: 10430  loss: 0.006903885398060083\n",
            " iteration: 10431  loss: 0.006903577595949173\n",
            " iteration: 10432  loss: 0.006905462127178907\n",
            " iteration: 10433  loss: 0.006903487257659435\n",
            " iteration: 10434  loss: 0.006903276313096285\n",
            " iteration: 10435  loss: 0.006903042085468769\n",
            " iteration: 10436  loss: 0.006902906112372875\n",
            " iteration: 10437  loss: 0.006902909372001886\n",
            " iteration: 10438  loss: 0.006902833003550768\n",
            " iteration: 10439  loss: 0.0069027068093419075\n",
            " iteration: 10440  loss: 0.0069025540724396706\n",
            " iteration: 10441  loss: 0.00690244697034359\n",
            " iteration: 10442  loss: 0.006902292370796204\n",
            " iteration: 10443  loss: 0.006901980843394995\n",
            " iteration: 10444  loss: 0.006901219021528959\n",
            " iteration: 10445  loss: 0.006901170127093792\n",
            " iteration: 10446  loss: 0.006900768261402845\n",
            " iteration: 10447  loss: 0.006900170352309942\n",
            " iteration: 10448  loss: 0.006899798754602671\n",
            " iteration: 10449  loss: 0.0068996199406683445\n",
            " iteration: 10450  loss: 0.006899502594023943\n",
            " iteration: 10451  loss: 0.006899371277540922\n",
            " iteration: 10452  loss: 0.006899252533912659\n",
            " iteration: 10453  loss: 0.006899172440171242\n",
            " iteration: 10454  loss: 0.006898876279592514\n",
            " iteration: 10455  loss: 0.006898826919496059\n",
            " iteration: 10456  loss: 0.0068986802361905575\n",
            " iteration: 10457  loss: 0.0068983458913862705\n",
            " iteration: 10458  loss: 0.006898018065840006\n",
            " iteration: 10459  loss: 0.006897614803165197\n",
            " iteration: 10460  loss: 0.00689736707136035\n",
            " iteration: 10461  loss: 0.0068973819725215435\n",
            " iteration: 10462  loss: 0.006897277664393187\n",
            " iteration: 10463  loss: 0.00689708162099123\n",
            " iteration: 10464  loss: 0.006896995007991791\n",
            " iteration: 10465  loss: 0.006896874401718378\n",
            " iteration: 10466  loss: 0.0068967328406870365\n",
            " iteration: 10467  loss: 0.0068965754471719265\n",
            " iteration: 10468  loss: 0.006896522827446461\n",
            " iteration: 10469  loss: 0.006896385923027992\n",
            " iteration: 10470  loss: 0.0068963514640927315\n",
            " iteration: 10471  loss: 0.006896240171045065\n",
            " iteration: 10472  loss: 0.006896059960126877\n",
            " iteration: 10473  loss: 0.006895736791193485\n",
            " iteration: 10474  loss: 0.0068953693844377995\n",
            " iteration: 10475  loss: 0.006895054131746292\n",
            " iteration: 10476  loss: 0.006895153317600489\n",
            " iteration: 10477  loss: 0.006894931197166443\n",
            " iteration: 10478  loss: 0.006894800812005997\n",
            " iteration: 10479  loss: 0.006894627120345831\n",
            " iteration: 10480  loss: 0.006894600111991167\n",
            " iteration: 10481  loss: 0.006894424092024565\n",
            " iteration: 10482  loss: 0.0068943011574447155\n",
            " iteration: 10483  loss: 0.00689434539526701\n",
            " iteration: 10484  loss: 0.006894230842590332\n",
            " iteration: 10485  loss: 0.0068940515629947186\n",
            " iteration: 10486  loss: 0.006893734447658062\n",
            " iteration: 10487  loss: 0.006893231999129057\n",
            " iteration: 10488  loss: 0.006892820354551077\n",
            " iteration: 10489  loss: 0.006892683915793896\n",
            " iteration: 10490  loss: 0.006892618723213673\n",
            " iteration: 10491  loss: 0.006892324425280094\n",
            " iteration: 10492  loss: 0.006892118602991104\n",
            " iteration: 10493  loss: 0.006891908124089241\n",
            " iteration: 10494  loss: 0.0068916622549295425\n",
            " iteration: 10495  loss: 0.006891285069286823\n",
            " iteration: 10496  loss: 0.00689113000407815\n",
            " iteration: 10497  loss: 0.0068907965905964375\n",
            " iteration: 10498  loss: 0.006890591699630022\n",
            " iteration: 10499  loss: 0.006890398450195789\n",
            " iteration: 10500  loss: 0.006890234537422657\n",
            " iteration: 10501  loss: 0.006889983080327511\n",
            " iteration: 10502  loss: 0.0068898675963282585\n",
            " iteration: 10503  loss: 0.006889606360346079\n",
            " iteration: 10504  loss: 0.006889378186315298\n",
            " iteration: 10505  loss: 0.00688912533223629\n",
            " iteration: 10506  loss: 0.006888878531754017\n",
            " iteration: 10507  loss: 0.006888673175126314\n",
            " iteration: 10508  loss: 0.006888426840305328\n",
            " iteration: 10509  loss: 0.006888308096677065\n",
            " iteration: 10510  loss: 0.006888134870678186\n",
            " iteration: 10511  loss: 0.00688806502148509\n",
            " iteration: 10512  loss: 0.006887893658131361\n",
            " iteration: 10513  loss: 0.00688768969848752\n",
            " iteration: 10514  loss: 0.006887360475957394\n",
            " iteration: 10515  loss: 0.006886978633701801\n",
            " iteration: 10516  loss: 0.006886736489832401\n",
            " iteration: 10517  loss: 0.006886531598865986\n",
            " iteration: 10518  loss: 0.006886227987706661\n",
            " iteration: 10519  loss: 0.006886049173772335\n",
            " iteration: 10520  loss: 0.006885748356580734\n",
            " iteration: 10521  loss: 0.006885554175823927\n",
            " iteration: 10522  loss: 0.006885353475809097\n",
            " iteration: 10523  loss: 0.006885165814310312\n",
            " iteration: 10524  loss: 0.006885001435875893\n",
            " iteration: 10525  loss: 0.006884779781103134\n",
            " iteration: 10526  loss: 0.006884593516588211\n",
            " iteration: 10527  loss: 0.006884181406348944\n",
            " iteration: 10528  loss: 0.00688367011025548\n",
            " iteration: 10529  loss: 0.006883327849209309\n",
            " iteration: 10530  loss: 0.0068831732496619225\n",
            " iteration: 10531  loss: 0.006883041933178902\n",
            " iteration: 10532  loss: 0.006882845424115658\n",
            " iteration: 10533  loss: 0.006882526446133852\n",
            " iteration: 10534  loss: 0.0068821981549263\n",
            " iteration: 10535  loss: 0.006882307585328817\n",
            " iteration: 10536  loss: 0.006882071495056152\n",
            " iteration: 10537  loss: 0.00688191968947649\n",
            " iteration: 10538  loss: 0.006881748326122761\n",
            " iteration: 10539  loss: 0.006881581153720617\n",
            " iteration: 10540  loss: 0.006881428882479668\n",
            " iteration: 10541  loss: 0.006881298031657934\n",
            " iteration: 10542  loss: 0.006881155073642731\n",
            " iteration: 10543  loss: 0.006880934815853834\n",
            " iteration: 10544  loss: 0.006880821194499731\n",
            " iteration: 10545  loss: 0.00688058091327548\n",
            " iteration: 10546  loss: 0.006880467291921377\n",
            " iteration: 10547  loss: 0.006880391389131546\n",
            " iteration: 10548  loss: 0.006880336906760931\n",
            " iteration: 10549  loss: 0.0068801878951489925\n",
            " iteration: 10550  loss: 0.006880329456180334\n",
            " iteration: 10551  loss: 0.006880079861730337\n",
            " iteration: 10552  loss: 0.006879983004182577\n",
            " iteration: 10553  loss: 0.006879852153360844\n",
            " iteration: 10554  loss: 0.006879777181893587\n",
            " iteration: 10555  loss: 0.006879670079797506\n",
            " iteration: 10556  loss: 0.006879463791847229\n",
            " iteration: 10557  loss: 0.006879272870719433\n",
            " iteration: 10558  loss: 0.00687912292778492\n",
            " iteration: 10559  loss: 0.006878896150738001\n",
            " iteration: 10560  loss: 0.006878552958369255\n",
            " iteration: 10561  loss: 0.006882076151669025\n",
            " iteration: 10562  loss: 0.006878489162772894\n",
            " iteration: 10563  loss: 0.006878288928419352\n",
            " iteration: 10564  loss: 0.006878078915178776\n",
            " iteration: 10565  loss: 0.006877862848341465\n",
            " iteration: 10566  loss: 0.006877759005874395\n",
            " iteration: 10567  loss: 0.006877558305859566\n",
            " iteration: 10568  loss: 0.006877406965941191\n",
            " iteration: 10569  loss: 0.006877177860587835\n",
            " iteration: 10570  loss: 0.00687693664804101\n",
            " iteration: 10571  loss: 0.006876652128994465\n",
            " iteration: 10572  loss: 0.006876461207866669\n",
            " iteration: 10573  loss: 0.00687631918117404\n",
            " iteration: 10574  loss: 0.00687604583799839\n",
            " iteration: 10575  loss: 0.006875849794596434\n",
            " iteration: 10576  loss: 0.006875599268823862\n",
            " iteration: 10577  loss: 0.006875479593873024\n",
            " iteration: 10578  loss: 0.006875405088067055\n",
            " iteration: 10579  loss: 0.00687525887042284\n",
            " iteration: 10580  loss: 0.006875737104564905\n",
            " iteration: 10581  loss: 0.006875209044665098\n",
            " iteration: 10582  loss: 0.006875113118439913\n",
            " iteration: 10583  loss: 0.006874985061585903\n",
            " iteration: 10584  loss: 0.00687486631795764\n",
            " iteration: 10585  loss: 0.006874683313071728\n",
            " iteration: 10586  loss: 0.006874565500766039\n",
            " iteration: 10587  loss: 0.006874422077089548\n",
            " iteration: 10588  loss: 0.006874374579638243\n",
            " iteration: 10589  loss: 0.006874293088912964\n",
            " iteration: 10590  loss: 0.006874242331832647\n",
            " iteration: 10591  loss: 0.006874116137623787\n",
            " iteration: 10592  loss: 0.006874722428619862\n",
            " iteration: 10593  loss: 0.006874041631817818\n",
            " iteration: 10594  loss: 0.006873916368931532\n",
            " iteration: 10595  loss: 0.006873766425997019\n",
            " iteration: 10596  loss: 0.006873682141304016\n",
            " iteration: 10597  loss: 0.006873490288853645\n",
            " iteration: 10598  loss: 0.00687324907630682\n",
            " iteration: 10599  loss: 0.006873498670756817\n",
            " iteration: 10600  loss: 0.006873150821775198\n",
            " iteration: 10601  loss: 0.006872921250760555\n",
            " iteration: 10602  loss: 0.006872828118503094\n",
            " iteration: 10603  loss: 0.006872725673019886\n",
            " iteration: 10604  loss: 0.006872592959553003\n",
            " iteration: 10605  loss: 0.00687242578715086\n",
            " iteration: 10606  loss: 0.006872290279716253\n",
            " iteration: 10607  loss: 0.006872199010103941\n",
            " iteration: 10608  loss: 0.006872064899653196\n",
            " iteration: 10609  loss: 0.006871920078992844\n",
            " iteration: 10610  loss: 0.0068718260154128075\n",
            " iteration: 10611  loss: 0.0068716490641236305\n",
            " iteration: 10612  loss: 0.006871498189866543\n",
            " iteration: 10613  loss: 0.0068713887594640255\n",
            " iteration: 10614  loss: 0.006871300283819437\n",
            " iteration: 10615  loss: 0.006871132645756006\n",
            " iteration: 10616  loss: 0.006870826706290245\n",
            " iteration: 10617  loss: 0.006872047204524279\n",
            " iteration: 10618  loss: 0.006870705634355545\n",
            " iteration: 10619  loss: 0.006870490964502096\n",
            " iteration: 10620  loss: 0.006870368029922247\n",
            " iteration: 10621  loss: 0.006870301440358162\n",
            " iteration: 10622  loss: 0.00687017384916544\n",
            " iteration: 10623  loss: 0.006869911216199398\n",
            " iteration: 10624  loss: 0.006869886536151171\n",
            " iteration: 10625  loss: 0.006869719363749027\n",
            " iteration: 10626  loss: 0.006869324948638678\n",
            " iteration: 10627  loss: 0.006869073957204819\n",
            " iteration: 10628  loss: 0.006868862081319094\n",
            " iteration: 10629  loss: 0.0068687014281749725\n",
            " iteration: 10630  loss: 0.006868581287562847\n",
            " iteration: 10631  loss: 0.006868373602628708\n",
            " iteration: 10632  loss: 0.006868196185678244\n",
            " iteration: 10633  loss: 0.006867954507470131\n",
            " iteration: 10634  loss: 0.006870149169117212\n",
            " iteration: 10635  loss: 0.006867862772196531\n",
            " iteration: 10636  loss: 0.006867725867778063\n",
            " iteration: 10637  loss: 0.006867596879601479\n",
            " iteration: 10638  loss: 0.006867506541311741\n",
            " iteration: 10639  loss: 0.006867267191410065\n",
            " iteration: 10640  loss: 0.0068671382032334805\n",
            " iteration: 10641  loss: 0.006866761017590761\n",
            " iteration: 10642  loss: 0.00686656404286623\n",
            " iteration: 10643  loss: 0.006866414565593004\n",
            " iteration: 10644  loss: 0.00686628557741642\n",
            " iteration: 10645  loss: 0.006866369862109423\n",
            " iteration: 10646  loss: 0.006866170559078455\n",
            " iteration: 10647  loss: 0.0068659596145153046\n",
            " iteration: 10648  loss: 0.0068656885996460915\n",
            " iteration: 10649  loss: 0.006865490227937698\n",
            " iteration: 10650  loss: 0.006865449715405703\n",
            " iteration: 10651  loss: 0.006865276489406824\n",
            " iteration: 10652  loss: 0.006865225732326508\n",
            " iteration: 10653  loss: 0.00686505576595664\n",
            " iteration: 10654  loss: 0.006864890456199646\n",
            " iteration: 10655  loss: 0.006864555645734072\n",
            " iteration: 10656  loss: 0.006867983378469944\n",
            " iteration: 10657  loss: 0.006864470429718494\n",
            " iteration: 10658  loss: 0.006864271592348814\n",
            " iteration: 10659  loss: 0.006864102091640234\n",
            " iteration: 10660  loss: 0.006863937713205814\n",
            " iteration: 10661  loss: 0.006863809190690517\n",
            " iteration: 10662  loss: 0.006863643880933523\n",
            " iteration: 10663  loss: 0.006863465066999197\n",
            " iteration: 10664  loss: 0.006863297428935766\n",
            " iteration: 10665  loss: 0.006866995710879564\n",
            " iteration: 10666  loss: 0.006863248534500599\n",
            " iteration: 10667  loss: 0.006863123271614313\n",
            " iteration: 10668  loss: 0.006862960755825043\n",
            " iteration: 10669  loss: 0.006862707436084747\n",
            " iteration: 10670  loss: 0.006862711161375046\n",
            " iteration: 10671  loss: 0.006862575188279152\n",
            " iteration: 10672  loss: 0.0068622855469584465\n",
            " iteration: 10673  loss: 0.006862010806798935\n",
            " iteration: 10674  loss: 0.0068618194200098515\n",
            " iteration: 10675  loss: 0.006861718837171793\n",
            " iteration: 10676  loss: 0.0068632690235972404\n",
            " iteration: 10677  loss: 0.0068616983480751514\n",
            " iteration: 10678  loss: 0.006861588452011347\n",
            " iteration: 10679  loss: 0.006861522328108549\n",
            " iteration: 10680  loss: 0.006861438509076834\n",
            " iteration: 10681  loss: 0.006861361209303141\n",
            " iteration: 10682  loss: 0.00686115026473999\n",
            " iteration: 10683  loss: 0.006861070636659861\n",
            " iteration: 10684  loss: 0.006860728375613689\n",
            " iteration: 10685  loss: 0.0068605318665504456\n",
            " iteration: 10686  loss: 0.006860177498310804\n",
            " iteration: 10687  loss: 0.006859925109893084\n",
            " iteration: 10688  loss: 0.006859688553959131\n",
            " iteration: 10689  loss: 0.006859494373202324\n",
            " iteration: 10690  loss: 0.006859354674816132\n",
            " iteration: 10691  loss: 0.00685920799151063\n",
            " iteration: 10692  loss: 0.006859589368104935\n",
            " iteration: 10693  loss: 0.006859146989881992\n",
            " iteration: 10694  loss: 0.006858989596366882\n",
            " iteration: 10695  loss: 0.00685887411236763\n",
            " iteration: 10696  loss: 0.0068587432615458965\n",
            " iteration: 10697  loss: 0.006858579348772764\n",
            " iteration: 10698  loss: 0.006858342327177525\n",
            " iteration: 10699  loss: 0.006858193781226873\n",
            " iteration: 10700  loss: 0.0068578715436160564\n",
            " iteration: 10701  loss: 0.006857696454972029\n",
            " iteration: 10702  loss: 0.006857490167021751\n",
            " iteration: 10703  loss: 0.006857424043118954\n",
            " iteration: 10704  loss: 0.006857296451926231\n",
            " iteration: 10705  loss: 0.006857159547507763\n",
            " iteration: 10706  loss: 0.006857017520815134\n",
            " iteration: 10707  loss: 0.006856872700154781\n",
            " iteration: 10708  loss: 0.006856723688542843\n",
            " iteration: 10709  loss: 0.006859324872493744\n",
            " iteration: 10710  loss: 0.006856678985059261\n",
            " iteration: 10711  loss: 0.006856422871351242\n",
            " iteration: 10712  loss: 0.00685617933049798\n",
            " iteration: 10713  loss: 0.006855991668999195\n",
            " iteration: 10714  loss: 0.006855865009129047\n",
            " iteration: 10715  loss: 0.006855667103081942\n",
            " iteration: 10716  loss: 0.006855506915599108\n",
            " iteration: 10717  loss: 0.00685525219887495\n",
            " iteration: 10718  loss: 0.0068550677970051765\n",
            " iteration: 10719  loss: 0.006854959763586521\n",
            " iteration: 10720  loss: 0.006854788865894079\n",
            " iteration: 10721  loss: 0.006854644510895014\n",
            " iteration: 10722  loss: 0.006854597013443708\n",
            " iteration: 10723  loss: 0.00685448432341218\n",
            " iteration: 10724  loss: 0.006854441948235035\n",
            " iteration: 10725  loss: 0.0068542202934622765\n",
            " iteration: 10726  loss: 0.006854045670479536\n",
            " iteration: 10727  loss: 0.006854476407170296\n",
            " iteration: 10728  loss: 0.006853955332189798\n",
            " iteration: 10729  loss: 0.006853803992271423\n",
            " iteration: 10730  loss: 0.006853519473224878\n",
            " iteration: 10731  loss: 0.0068533215671777725\n",
            " iteration: 10732  loss: 0.006854373496025801\n",
            " iteration: 10733  loss: 0.006853261962532997\n",
            " iteration: 10734  loss: 0.006852999329566956\n",
            " iteration: 10735  loss: 0.006852768361568451\n",
            " iteration: 10736  loss: 0.006852456368505955\n",
            " iteration: 10737  loss: 0.006852122023701668\n",
            " iteration: 10738  loss: 0.006851730868220329\n",
            " iteration: 10739  loss: 0.0068514179438352585\n",
            " iteration: 10740  loss: 0.006851173005998135\n",
            " iteration: 10741  loss: 0.006850924342870712\n",
            " iteration: 10742  loss: 0.006850656121969223\n",
            " iteration: 10743  loss: 0.006850382313132286\n",
            " iteration: 10744  loss: 0.006850159261375666\n",
            " iteration: 10745  loss: 0.0068500470370054245\n",
            " iteration: 10746  loss: 0.006849843543022871\n",
            " iteration: 10747  loss: 0.006849696859717369\n",
            " iteration: 10748  loss: 0.0068495930172502995\n",
            " iteration: 10749  loss: 0.006849457509815693\n",
            " iteration: 10750  loss: 0.006849465426057577\n",
            " iteration: 10751  loss: 0.006849384400993586\n",
            " iteration: 10752  loss: 0.006849246099591255\n",
            " iteration: 10753  loss: 0.006849046330899\n",
            " iteration: 10754  loss: 0.006848893128335476\n",
            " iteration: 10755  loss: 0.006848711986094713\n",
            " iteration: 10756  loss: 0.006849062629044056\n",
            " iteration: 10757  loss: 0.0068486412055790424\n",
            " iteration: 10758  loss: 0.006848463788628578\n",
            " iteration: 10759  loss: 0.006848239339888096\n",
            " iteration: 10760  loss: 0.006848035845905542\n",
            " iteration: 10761  loss: 0.006847988814115524\n",
            " iteration: 10762  loss: 0.0068478272296488285\n",
            " iteration: 10763  loss: 0.006847732700407505\n",
            " iteration: 10764  loss: 0.00684764189645648\n",
            " iteration: 10765  loss: 0.006847509182989597\n",
            " iteration: 10766  loss: 0.006847363896667957\n",
            " iteration: 10767  loss: 0.0068472218699753284\n",
            " iteration: 10768  loss: 0.006847112439572811\n",
            " iteration: 10769  loss: 0.006846958305686712\n",
            " iteration: 10770  loss: 0.006846849340945482\n",
            " iteration: 10771  loss: 0.006846621632575989\n",
            " iteration: 10772  loss: 0.006846510339528322\n",
            " iteration: 10773  loss: 0.00684632221236825\n",
            " iteration: 10774  loss: 0.006846167147159576\n",
            " iteration: 10775  loss: 0.006845763884484768\n",
            " iteration: 10776  loss: 0.006845592986792326\n",
            " iteration: 10777  loss: 0.006845362018793821\n",
            " iteration: 10778  loss: 0.006845174357295036\n",
            " iteration: 10779  loss: 0.006844998337328434\n",
            " iteration: 10780  loss: 0.006844945717602968\n",
            " iteration: 10781  loss: 0.006844782270491123\n",
            " iteration: 10782  loss: 0.006844726856797934\n",
            " iteration: 10783  loss: 0.006844584830105305\n",
            " iteration: 10784  loss: 0.006844483781605959\n",
            " iteration: 10785  loss: 0.006844255607575178\n",
            " iteration: 10786  loss: 0.0068443105556070805\n",
            " iteration: 10787  loss: 0.006844180170446634\n",
            " iteration: 10788  loss: 0.006844007410109043\n",
            " iteration: 10789  loss: 0.006843888200819492\n",
            " iteration: 10790  loss: 0.0068436977453529835\n",
            " iteration: 10791  loss: 0.006843447685241699\n",
            " iteration: 10792  loss: 0.006843096576631069\n",
            " iteration: 10793  loss: 0.006843140814453363\n",
            " iteration: 10794  loss: 0.006842730566859245\n",
            " iteration: 10795  loss: 0.006842460483312607\n",
            " iteration: 10796  loss: 0.006842196919023991\n",
            " iteration: 10797  loss: 0.006841985043138266\n",
            " iteration: 10798  loss: 0.00684175593778491\n",
            " iteration: 10799  loss: 0.006841450929641724\n",
            " iteration: 10800  loss: 0.0068412115797400475\n",
            " iteration: 10801  loss: 0.006840972695499659\n",
            " iteration: 10802  loss: 0.006840926129370928\n",
            " iteration: 10803  loss: 0.006840614601969719\n",
            " iteration: 10804  loss: 0.006840497720986605\n",
            " iteration: 10805  loss: 0.006840337999165058\n",
            " iteration: 10806  loss: 0.00684013357385993\n",
            " iteration: 10807  loss: 0.006839979439973831\n",
            " iteration: 10808  loss: 0.006839701905846596\n",
            " iteration: 10809  loss: 0.006839554291218519\n",
            " iteration: 10810  loss: 0.006839366629719734\n",
            " iteration: 10811  loss: 0.006839078851044178\n",
            " iteration: 10812  loss: 0.0068401917815208435\n",
            " iteration: 10813  loss: 0.006838974077254534\n",
            " iteration: 10814  loss: 0.006838594563305378\n",
            " iteration: 10815  loss: 0.0068382928147912025\n",
            " iteration: 10816  loss: 0.0068380520679056644\n",
            " iteration: 10817  loss: 0.006837873719632626\n",
            " iteration: 10818  loss: 0.0068377661518752575\n",
            " iteration: 10819  loss: 0.006837379187345505\n",
            " iteration: 10820  loss: 0.0068371775560081005\n",
            " iteration: 10821  loss: 0.006836801767349243\n",
            " iteration: 10822  loss: 0.00683647021651268\n",
            " iteration: 10823  loss: 0.006836202461272478\n",
            " iteration: 10824  loss: 0.00683589605614543\n",
            " iteration: 10825  loss: 0.006835754029452801\n",
            " iteration: 10826  loss: 0.00683552585542202\n",
            " iteration: 10827  loss: 0.006835378240793943\n",
            " iteration: 10828  loss: 0.006835213862359524\n",
            " iteration: 10829  loss: 0.0068350546061992645\n",
            " iteration: 10830  loss: 0.006834821775555611\n",
            " iteration: 10831  loss: 0.006834594998508692\n",
            " iteration: 10832  loss: 0.0068344734609127045\n",
            " iteration: 10833  loss: 0.006835241802036762\n",
            " iteration: 10834  loss: 0.006834398023784161\n",
            " iteration: 10835  loss: 0.006834318861365318\n",
            " iteration: 10836  loss: 0.006834113504737616\n",
            " iteration: 10837  loss: 0.006834077648818493\n",
            " iteration: 10838  loss: 0.006833899300545454\n",
            " iteration: 10839  loss: 0.00683384295552969\n",
            " iteration: 10840  loss: 0.006833744700998068\n",
            " iteration: 10841  loss: 0.006833492778241634\n",
            " iteration: 10842  loss: 0.006835822481662035\n",
            " iteration: 10843  loss: 0.006833429913967848\n",
            " iteration: 10844  loss: 0.006833203136920929\n",
            " iteration: 10845  loss: 0.006833069957792759\n",
            " iteration: 10846  loss: 0.006832883693277836\n",
            " iteration: 10847  loss: 0.006832629442214966\n",
            " iteration: 10848  loss: 0.0068322475999593735\n",
            " iteration: 10849  loss: 0.0068325018510222435\n",
            " iteration: 10850  loss: 0.006832003593444824\n",
            " iteration: 10851  loss: 0.006831597536802292\n",
            " iteration: 10852  loss: 0.00683144386857748\n",
            " iteration: 10853  loss: 0.006831101141870022\n",
            " iteration: 10854  loss: 0.006831004284322262\n",
            " iteration: 10855  loss: 0.006830911617726088\n",
            " iteration: 10856  loss: 0.006830819882452488\n",
            " iteration: 10857  loss: 0.006830658297985792\n",
            " iteration: 10858  loss: 0.0068304152227938175\n",
            " iteration: 10859  loss: 0.0068302820436656475\n",
            " iteration: 10860  loss: 0.006829969584941864\n",
            " iteration: 10861  loss: 0.006829836871474981\n",
            " iteration: 10862  loss: 0.006829606369137764\n",
            " iteration: 10863  loss: 0.006829318590462208\n",
            " iteration: 10864  loss: 0.006829093210399151\n",
            " iteration: 10865  loss: 0.006828947924077511\n",
            " iteration: 10866  loss: 0.00682872487232089\n",
            " iteration: 10867  loss: 0.006828425917774439\n",
            " iteration: 10868  loss: 0.006828316021710634\n",
            " iteration: 10869  loss: 0.006827938836067915\n",
            " iteration: 10870  loss: 0.006827694363892078\n",
            " iteration: 10871  loss: 0.006827399134635925\n",
            " iteration: 10872  loss: 0.00682716304436326\n",
            " iteration: 10873  loss: 0.006826789118349552\n",
            " iteration: 10874  loss: 0.006826748605817556\n",
            " iteration: 10875  loss: 0.006826430093497038\n",
            " iteration: 10876  loss: 0.006826302502304316\n",
            " iteration: 10877  loss: 0.006825962569564581\n",
            " iteration: 10878  loss: 0.006825623102486134\n",
            " iteration: 10879  loss: 0.006826427765190601\n",
            " iteration: 10880  loss: 0.006825352553278208\n",
            " iteration: 10881  loss: 0.006824970711022615\n",
            " iteration: 10882  loss: 0.006824571173638105\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9013, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPgcIVok1Lwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRwXTT_ia8c0",
        "outputId": "024ca9b9-422c-4156-c142-df4e3d49cbea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 1]), torch.Size([10000, 1]), torch.Size([10000, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "t_test = torch.linspace(2, 2,1).requires_grad_(True)\n",
        "x_test = torch.linspace(x_min+1e-3,x_max-1e-3,total_points).requires_grad_(True)\n",
        "y_test = torch.linspace(y_min+1e-3,y_max-1e-3,total_points).requires_grad_(True)\n",
        "\n",
        "x_test, y_test, t_test = torch.meshgrid(x_test, y_test, t_test)\n",
        "\n",
        "x_test = torch.reshape(x_test, (-1, 1)).to(device)\n",
        "y_test = torch.reshape(y_test, (-1, 1)).to(device)\n",
        "t_test = torch.reshape(t_test, (-1, 1)).to(device)\n",
        "\n",
        "x_test.shape, y_test.shape, t_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcWXOk7yerkT"
      },
      "outputs": [],
      "source": [
        "u_test, v_test = PINN.inference(x_test, y_test, t_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "WDDRD6pgdzte",
        "outputId": "c29562a5-0e38-4d68-915b-11949d5b176f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7ec8b04b6d70>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.kdeplot(x=x_test.cpu().detach().numpy().squeeze(), y=y_test.cpu().detach().numpy().squeeze(),\n",
        "            )\n",
        "plt.imshow(u_test.cpu().detach().numpy().reshape(100,100), cmap='jet')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "DBel3wqRey6L",
        "outputId": "eb70538b-05cf-4384-85eb-02b71cee8b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-5b61c16e017d>:1: FutureWarning: \n",
            "\n",
            "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
            "This will become an error in seaborn v0.14.0; please update your code.\n",
            "\n",
            "  sns.kdeplot(x=x_test.cpu().detach().numpy().squeeze(), y=y_test.cpu().detach().numpy().squeeze(),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7ec8b1b1f880>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.kdeplot(x=x_test.cpu().detach().numpy().squeeze(), y=y_test.cpu().detach().numpy().squeeze(),\n",
        "            cmap='hot', shade=True)\n",
        "plt.imshow(v_test.cpu().detach().numpy().reshape(100,100), cmap='hot', alpha=0.5,)\n",
        "plt.colorbar()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}